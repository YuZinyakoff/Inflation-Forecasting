{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8277,
     "status": "ok",
     "timestamp": 1745837646803,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "wsia_YErlLPu",
    "outputId": "f3f18a61-cc5e-45ab-90f0-3f7dd216b60d"
   },
   "outputs": [],
   "source": [
    "# --- Базовые библиотеки ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import collections # Для dm_test\n",
    "from itertools import combinations # Для dm_test\n",
    "\n",
    "# --- Метрики и Утилиты ---\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Оставим оба на всякий случай\n",
    "from scipy.stats import t # Для dm_test\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests # Для DM-теста\n",
    "\n",
    "# --- Модели Временных Рядов ---\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA as StatsmodelsARIMA\n",
    "\n",
    "# --- Модели Машинного Обучения ---\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- (Опционально) Нейронные Сети ---\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# --- Настройки Визуализации и Предупреждений ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Стиль графиков\n",
    "warnings.filterwarnings(\"ignore\") # Игнорировать предупреждения (можно закомментировать для отладки)\n",
    "\n",
    "print(\"Библиотеки импортированы.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745841359221,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "sx5R9oEXoMbh",
    "outputId": "86885504-0435-4bfd-fdd0-ebdfbb37de6f"
   },
   "outputs": [],
   "source": [
    "# --- Глобальные Настройки ---\n",
    "RANDOM_SEED = 42 # Для воспроизводимости\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# tf.random.set_seed(RANDOM_SEED) # Если используем TensorFlow\n",
    "\n",
    "# --- Параметры Данных ---\n",
    "FILE_PATH = 'CPI.xlsx' \n",
    "WEIGHTS_FILE_PATH = 'Weights.csv'\n",
    "CPI_MONTHLY_SHEET = '44 компоненты' \n",
    "WEIGHTS_SHEET = 'Веса'\n",
    "DATE_COLUMN = 'Дата'\n",
    "CPI_TOTAL_COLUMN = 'ИПЦ'\n",
    "COMPONENTS_TO_MODEL = ['Мясопродукты', 'Рыбопродукты', 'Масло и жиры',\n",
    "       'Молоко и молочная продукция', 'Сыр', 'Яйца', 'Сахар',\n",
    "       'Кондитерские изделия', 'Чай, кофе',\n",
    "       'Хлеб и хлебобулочные изделия', 'Макаронные и крупяные изделия',\n",
    "       'Плодоовощная продукция, включая картофель', 'Алкогольные напитки',\n",
    "       'Общественное питание', 'Одежда и белье', 'Меха и меховые изделия',\n",
    "       'Трикотажные изделия',\n",
    "       'Обувь кожаная, текстильная и комбинированная',\n",
    "       'Моющие и чистящие средства', 'Парфюмерно-косметические товары',\n",
    "       'Галантерея', 'Табачные изделия', 'Мебель',\n",
    "       'Электротовары и другие бытовые приборы', 'Печатные издания',\n",
    "       'Телерадиотовары', 'Персональные компьютеры', 'Средства связи',\n",
    "       'Строительные материалы', 'Легковые автомобили',\n",
    "       'Инструменты и оборудование', 'Нефтепродукты',\n",
    "       'Медицинские товары', 'Бытовые услуги',\n",
    "       'Услуги пассажирского транспорта', 'Услуги связи',\n",
    "       'Жилищно-коммунальные услуги',\n",
    "       'Услуги гостиниц и прочих мест проживания',\n",
    "       'Услуги в системе образования', 'Услуги организаций культуры',\n",
    "       'Услуги в сфере зарубежного туризма', 'Экскурсионные услуги',\n",
    "       'Санаторно-оздоровительные услуги', 'Медицинские услуги']\n",
    "\n",
    "# --- Параметры Времени и Оценки ---\n",
    "TEST_MONTHS = 24\n",
    "TRAIN_DO = '2023-02-01' # Дата начала тестового периода\n",
    "HORIZONS_TO_EVALUATE = [1, 2, 3, 6, 12, 18, 24] # Горизонты для расчета RMSE\n",
    "PLOT_START_DATE = '2016-01-01' # Начальная дата для графиков прогнозов\n",
    "\n",
    "# --- Параметры Кросс-Валидации и Feature Engineering ---\n",
    "N_CV_SPLITS = 3 # Количество сплитов для GridSearchCV при обучении моделей компонент\n",
    "N_OOF_SPLITS = 5 # Количество сплитов для генерации OOF прогнозов\n",
    "LAGS_TO_CREATE = [1, 2, 3, 6, 12] # Лаги для признаков ML\n",
    "ROLLING_WINDOWS = [3, 6, 12] # Окна для скользящих статистик ML\n",
    "# Порядок признаков (важно для функций)\n",
    "FEATURE_ORDER = [\n",
    "    't-1', 't-2', 't-3', 't-6', 't-12',\n",
    "    't_mean_lag3', 't_std_lag3',\n",
    "    't_mean_lag6', 't_std_lag6',\n",
    "    't_mean_lag12', 't_std_lag12',\n",
    "    'month_sin', 'month_cos'\n",
    "]\n",
    "\n",
    "# --- Параметры Критерия Выбора Лучшей Модели Компоненты ---\n",
    "HORIZONS_FOR_WEIGHTING = [1, 2, 3, 6, 12, 18, 24] # Горизонты для взвешенного RMSE\n",
    "weights_raw = pd.Series({h: 1/np.sqrt(h) for h in HORIZONS_FOR_WEIGHTING})\n",
    "WEIGHTS_NORMALIZED_RMSE = weights_raw / weights_raw.sum()\n",
    "print(\"Веса для выбора лучшей модели компоненты:\")\n",
    "print(WEIGHTS_NORMALIZED_RMSE.round(4))\n",
    "\n",
    "# --- Инициализация Глобальных Структур для Результатов ---\n",
    "# Общие результаты агрегации\n",
    "aggregation_rmse = {} # Словарь: {имя_агрег_модели: общий_rmse}\n",
    "all_horizon_rmse = pd.DataFrame(columns=['model', 'horizon', 'rmse']) # RMSE по горизонтам для агрег. моделей\n",
    "\n",
    "# Результаты по компонентам\n",
    "component_best_models = {} # Словарь: {компонента: имя_лучшей_модели}\n",
    "df_best_component_forecasts = None # DataFrame: [ИндексТеста x Компоненты] - прогнозы ЛУЧШИХ моделей на тесте\n",
    "df_all_component_rmses = None # DataFrame: [Компонента, Горизонт, Модель] - RMSE всех моделей для всех компонент\n",
    "\n",
    "# Промежуточные прогнозы компонент\n",
    "df_forecasts_arima_comp = None # DataFrame: [ИндексТеста x Компоненты] - прогнозы ARIMA на тесте\n",
    "df_oof_comp_arima_clean = None # DataFrame: [ИндексТрейна x Компоненты] - OOF прогнозы ARIMA (чистые)\n",
    "df_oof_best_comp_clean = None # DataFrame: [ИндексТрейна x Компоненты] - OOF прогнозы ЛУЧШИХ моделей (чистые)\n",
    "\n",
    "# Финальные прогнозы для графиков и DM-тестов\n",
    "final_forecasts = {} # Словарь: {имя_финальной_модели: np.array прогноза на тест}\n",
    "\n",
    "print(\"\\nНастройки и структуры результатов инициализированы (с CV для компонент).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1745840453558,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "IVSwUoXbCSUs",
    "outputId": "397f1ac3-88e3-4f49-ac1e-791e6cb77abc"
   },
   "outputs": [],
   "source": [
    "# df_monthly = pd.read_excel(FILE_PATH,\n",
    "#                                engine='openpyxl',\n",
    "#                                sheet_name=CPI_MONTHLY_SHEET)\n",
    "# df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745837717831,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "kMSdwH_eCWnq",
    "outputId": "eb444b21-92ba-48dc-c2b3-66ae255bdbed"
   },
   "outputs": [],
   "source": [
    "# df_monthly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1745838052723,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "CjPmOLMWDYJc",
    "outputId": "c497f78c-2eb4-4caf-90cb-c1c07803be42"
   },
   "outputs": [],
   "source": [
    "# df_weights_raw = pd.read_csv(WEIGHTS_FILE_PATH)\n",
    "# df_weights_raw[df_weights_raw['Уровень'] == '44 компоненты']['Компонента'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAbtHSakDeuV"
   },
   "outputs": [],
   "source": [
    "# df_weights_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1745841394975,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "yrHuTMHrKw2M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1745841446371,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "quQR4GY-or-f",
    "outputId": "6fcf56d3-f801-4622-9118-eb9774040014"
   },
   "outputs": [],
   "source": [
    "print(\"--- Раздел 1: Загрузка и Подготовка Данных ---\")\n",
    "print(\"\\nЯчейка 3: Загрузка ежемесячных данных (ИПЦ + Компоненты)\")\n",
    "\n",
    "try:\n",
    "    # Загружаем лист с ежемесячными данными\n",
    "    df_monthly = pd.read_excel(FILE_PATH,\n",
    "                               engine='openpyxl',\n",
    "                               sheet_name=CPI_MONTHLY_SHEET) # Используем имя листа из настроек\n",
    "\n",
    "    # Преобразование колонки с датой\n",
    "    df_monthly[DATE_COLUMN] = pd.to_datetime(df_monthly[DATE_COLUMN])\n",
    "    df_monthly = df_monthly.set_index(DATE_COLUMN)\n",
    "\n",
    "    # Выбираем только нужные колонки: общий ИПЦ и заданные компоненты\n",
    "    columns_to_keep = [CPI_TOTAL_COLUMN] + COMPONENTS_TO_MODEL\n",
    "    df_monthly = df_monthly[columns_to_keep]\n",
    "\n",
    "    # # Удаление строк с пропусками, если есть\n",
    "    # initial_rows = len(df_monthly)\n",
    "    # df_monthly = df_monthly.dropna()\n",
    "    # final_rows = len(df_monthly)\n",
    "    # if initial_rows > final_rows:\n",
    "    #     print(f\"Удалено {initial_rows - final_rows} строк с NaN.\")\n",
    "\n",
    "    print(f\"\\nЕжемесячные данные загружены и обработаны.\")\n",
    "    print(f\"Период данных: {df_monthly.index.min().strftime('%Y-%m-%d')} - {df_monthly.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Размер DataFrame: {df_monthly.shape}\")\n",
    "    print(\"\\nПервые 5 строк:\")\n",
    "    print(df_monthly.head())\n",
    "    print(\"\\nПоследние 5 строк:\")\n",
    "    print(df_monthly.tail())\n",
    "    print(\"\\nИнформация о данных:\")\n",
    "    df_monthly.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ОШИБКА: Файл не найден по пути: {FILE_PATH}\")\n",
    "    # Выход или дальнейшие действия в зависимости от логики ноутбука\n",
    "    raise # Поднимаем ошибку, чтобы остановить выполнение, если файл критичен\n",
    "except KeyError as e:\n",
    "    print(f\"ОШИБКА: Не найдена колонка {e} на листе '{CPI_MONTHLY_SHEET}'. Проверьте настройки CPI_TOTAL_COLUMN и COMPONENTS_TO_MODEL.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ОШИБКА при загрузке или обработке листа '{CPI_MONTHLY_SHEET}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1745841453883,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "77GiawW7QXUi",
    "outputId": "d71b8473-4eb9-4e58-91fe-91fdfb37dcc0"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 3а: Анализ и Заполнение Пропусков в Компонентах\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_monthly: DataFrame с ИПЦ и всеми компонентами (после Ячейки 3)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Компоненты с пропусками для анализа\n",
    "cols_to_impute = ['Персональные компьютеры', 'Средства связи']\n",
    "col_to_exclude = 'Инструменты и оборудование' # Компонента для исключения\n",
    "\n",
    "# --- 1. Исключение компоненты ---\n",
    "if col_to_exclude in df_monthly.columns:\n",
    "    df_monthly = df_monthly.drop(columns=[col_to_exclude])\n",
    "    print(f\"Компонента '{col_to_exclude}' исключена из DataFrame.\")\n",
    "    # Удаляем ее из глобального списка компонент, если он уже определен\n",
    "    if 'COMPONENTS_TO_MODEL' in globals() and isinstance(COMPONENTS_TO_MODEL, list):\n",
    "         if col_to_exclude in COMPONENTS_TO_MODEL:\n",
    "             COMPONENTS_TO_MODEL.remove(col_to_exclude)\n",
    "             print(f\"'{col_to_exclude}' удалена из списка COMPONENTS_TO_MODEL.\")\n",
    "else:\n",
    "    print(f\"Компонента '{col_to_exclude}' не найдена в df_monthly.\")\n",
    "\n",
    "\n",
    "# --- 2. Анализ и Визуализация рядов ДО заполнения ---\n",
    "print(\"\\nАнализ рядов с пропусками ДО заполнения:\")\n",
    "missing_data_info = df_monthly[cols_to_impute].isnull().sum()\n",
    "print(\"Количество пропусков:\")\n",
    "print(missing_data_info)\n",
    "\n",
    "# Строим графики\n",
    "plt.figure(figsize=(14, 5 * len(cols_to_impute)))\n",
    "for i, col in enumerate(cols_to_impute):\n",
    "    if col in df_monthly.columns:\n",
    "        plt.subplot(len(cols_to_impute), 1, i + 1)\n",
    "        plt.plot(df_monthly.index, df_monthly[col], label=f'{col} (Original)', marker='.', linestyle='-')\n",
    "        plt.title(f'Компонента: {col} (До заполнения)')\n",
    "        plt.ylabel('Значение')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    else:\n",
    "         print(f\"Колонка '{col}' не найдена для визуализации.\")\n",
    "plt.xlabel('Дата')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Заполнение пропусков линейной интерполяцией ---\n",
    "print(\"\\nЗаполнение пропусков линейной интерполяцией...\")\n",
    "df_monthly_imputed = df_monthly.copy() # Создаем копию для изменений\n",
    "\n",
    "for col in cols_to_impute:\n",
    "    if col in df_monthly_imputed.columns:\n",
    "        initial_nan = df_monthly_imputed[col].isnull().sum()\n",
    "        if initial_nan > 0:\n",
    "            # limit_direction='both' заполняет NaN в начале и в конце\n",
    "            df_monthly_imputed[col] = df_monthly_imputed[col].interpolate(method='linear', limit_direction='both')\n",
    "            filled_nan = initial_nan - df_monthly_imputed[col].isnull().sum()\n",
    "            print(f\"  Для '{col}': Заполнено {filled_nan} из {initial_nan} пропусков.\")\n",
    "        else:\n",
    "            print(f\"  Для '{col}': Пропусков не найдено.\")\n",
    "    else:\n",
    "        print(f\"  Колонка '{col}' не найдена для заполнения.\")\n",
    "\n",
    "# Перезаписываем исходный DataFrame или используем новый\n",
    "df_monthly = df_monthly_imputed # Перезаписываем df_monthly\n",
    "print(\"\\nПроверка NaN ПОСЛЕ интерполяции:\")\n",
    "print(df_monthly[cols_to_impute].isnull().sum())\n",
    "\n",
    "# --- 4. Визуализация ПОСЛЕ интерполяции ---\n",
    "print(\"\\nВизуализация рядов ПОСЛЕ заполнения:\")\n",
    "plt.figure(figsize=(14, 5 * len(cols_to_impute)))\n",
    "for i, col in enumerate(cols_to_impute):\n",
    "     if col in df_monthly.columns:\n",
    "        plt.subplot(len(cols_to_impute), 1, i + 1)\n",
    "        # Отметим интерполированные значения другим цветом/маркером\n",
    "        original_data_mask = df_monthly_imputed[col].notna() & df_monthly[col].isna() # Где были NaN\n",
    "        interpolated_values = df_monthly[col].copy()\n",
    "        interpolated_values[~original_data_mask] = np.nan # Оставляем только интерполированные\n",
    "\n",
    "        plt.plot(df_monthly.index, df_monthly[col], label=f'{col} (Заполненный)', marker='.', linestyle='-', zorder=1)\n",
    "        #plt.scatter(interpolated_values.index, interpolated_values, color='red', s=15, label='Интерполяция', zorder=2) # Отметим красным\n",
    "        plt.title(f'Компонента: {col} (После заполнения)')\n",
    "        plt.ylabel('Значение')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "     else:\n",
    "          print(f\"Колонка '{col}' не найдена для визуализации.\")\n",
    "\n",
    "plt.xlabel('Дата')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Ячейка 3а завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1745841518191,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "SoBM1UinQ2XC",
    "outputId": "373562f2-f74d-4a3a-dcf8-574be90974f2"
   },
   "outputs": [],
   "source": [
    "df_monthly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1745843793720,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "ARTNuSjCpJpW",
    "outputId": "fb99f37e-119e-45e9-9443-c0d7e3968a9c"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 4: Загрузка и Обработка Данных по Годовым Весам\")\n",
    "\n",
    "try:\n",
    "    # Загружаем лист с весами\n",
    "    df_weights_raw = pd.read_csv(WEIGHTS_FILE_PATH) # Используем имя листа из настроек\n",
    "\n",
    "    print(\"Структура исходного файла весов (первые 5 строк):\")\n",
    "    print(df_weights_raw.head())\n",
    "\n",
    "    # --- Адаптация под структуру файла весов ---\n",
    "    # Предполагаем структуру: 'Уровень', 'Компонента', Год1, Год2, ...\n",
    "    # Отбираем строки, относящиеся к 3 компонентам\n",
    "    # !!! Убедись, что значение '3 компоненты' точное !!!\n",
    "    df_weights_filtered = df_weights_raw[df_weights_raw['Уровень'] == '44 компоненты'].copy()\n",
    "\n",
    "    if df_weights_filtered.empty:\n",
    "        raise ValueError(f\"Не найдены строки с 'Уровень' == '6 компонент' на листе '{WEIGHTS_SHEET}'.\")\n",
    "\n",
    "    # Убираем ненужную колонку \"Уровень\" и делаем компоненты индексом\n",
    "    df_weights_processed = df_weights_filtered.drop(columns=['Уровень']).set_index('Компонента')\n",
    "\n",
    "    # Транспонируем, чтобы годы стали индексом\n",
    "    weights_df_transposed = df_weights_processed.T\n",
    "\n",
    "    # Преобразуем индекс (годы) в числовой формат и проверяем\n",
    "    weights_df_transposed.index = pd.to_numeric(weights_df_transposed.index, errors='coerce')\n",
    "    weights_df_transposed = weights_df_transposed.dropna(axis=0) # Удаляем строки, если год не распознался\n",
    "    weights_df_transposed.index = weights_df_transposed.index.astype(int)\n",
    "    weights_df_transposed.index.name = 'Год'\n",
    "\n",
    "    # Убираем лишние пробелы в названиях колонок (на всякий случай)\n",
    "    weights_df_transposed.columns = weights_df_transposed.columns.str.strip()\n",
    "\n",
    "    # Выбираем и УПОРЯДОЧИВАЕМ колонки согласно COMPONENTS_TO_MODEL\n",
    "    weights_df_pivot = weights_df_transposed[COMPONENTS_TO_MODEL]\n",
    "\n",
    "    print(\"\\nДанные по весам успешно обработаны (Год x Компонента):\")\n",
    "    print(weights_df_pivot.head())\n",
    "    print(weights_df_pivot.tail())\n",
    "\n",
    "    # Нормализуем веса (делим на 100, чтобы сумма стала 1)\n",
    "    weights_norm = weights_df_pivot.apply(lambda x: x / 100.0, axis=1)\n",
    "    print(\"\\nНормализованные веса (доли):\")\n",
    "    print(weights_norm.head())\n",
    "    sum_weights_modelled = weights_norm.sum(axis=1)\n",
    "    print(\"\\nСуммарный вес моделируемых компонент по годам (первые 5 лет):\")\n",
    "    print(sum_weights_modelled.head())\n",
    "    print(f\"\\nСредний суммарный вес за весь период: {sum_weights_modelled.mean():.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ОШИБКА: Файл не найден по пути: {FILE_PATH}\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "     print(f\"ОШИБКА: Не найдена колонка {e} при обработке весов. Проверьте названия 'Уровень', 'Компонента' и годов на листе '{WEIGHTS_SHEET}'.\")\n",
    "     raise\n",
    "except ValueError as e:\n",
    "     print(f\"ОШИБКА: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    print(f\"ОШИБКА при загрузке или обработке листа весов '{WEIGHTS_SHEET}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745841525951,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "vd0NByS1prcQ",
    "outputId": "26e150e7-018d-43d6-c17e-80abd806db4c"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 5: Разделение Данных на Train/Test\")\n",
    "\n",
    "# --- Агрегированный ряд ---\n",
    "try:\n",
    "    cpi_agg_series = df_monthly[CPI_TOTAL_COLUMN]\n",
    "\n",
    "    # Определяем дату среза из настроек\n",
    "    train_do_dt = pd.to_datetime(TRAIN_DO)\n",
    "\n",
    "    # Разделяем\n",
    "    train_values = cpi_agg_series[cpi_agg_series.index < train_do_dt]\n",
    "    test_values = cpi_agg_series[cpi_agg_series.index >= train_do_dt][:24]\n",
    "\n",
    "    # Проверяем длину тестовой выборки\n",
    "    if len(test_values) != TEST_MONTHS:\n",
    "        print(f\"ВНИМАНИЕ: Длина тестовой выборки агрегата ({len(test_values)}) не равна {TEST_MONTHS}.\")\n",
    "        # Можно добавить логику корректировки или остановки, если это критично\n",
    "        # Например, взять последние TEST_MONTHS точек:\n",
    "        # test_values = cpi_agg_series.iloc[-TEST_MONTHS:]\n",
    "        # train_values = cpi_agg_series.iloc[:-TEST_MONTHS]\n",
    "        # print(f\"Скорректировано: Длина теста = {len(test_values)}, Трейна = {len(train_values)}\")\n",
    "        # Пока оставим как есть, но выведем предупреждение.\n",
    "\n",
    "    print(\"\\nАгрегированный ряд:\")\n",
    "    print(f\"  Размер обучающей выборки (train_values): {len(train_values)} (до {train_values.index.max().strftime('%Y-%m-%d')})\")\n",
    "    print(f\"  Размер тестовой выборки (test_values): {len(test_values)} (с {test_values.index.min().strftime('%Y-%m-%d')})\")\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"ОШИБКА: Колонка общего ИПЦ '{CPI_TOTAL_COLUMN}' не найдена.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "     print(f\"ОШИБКА при разделении агрегированного ряда: {e}\")\n",
    "     raise\n",
    "\n",
    "# --- Компонентные ряды ---\n",
    "try:\n",
    "    cpi_components_data = df_monthly[COMPONENTS_TO_MODEL]\n",
    "\n",
    "    # Разделяем по той же дате\n",
    "    train_comp_data = cpi_components_data[cpi_components_data.index < train_do_dt]\n",
    "    test_comp_data = cpi_components_data[cpi_components_data.index >= train_do_dt][:24]\n",
    "\n",
    "    # Проверяем длину теста для компонент\n",
    "    if len(test_comp_data) != TEST_MONTHS:\n",
    "         print(f\"ВНИМАНИЕ: Длина тестовой выборки компонент ({len(test_comp_data)}) не равна {TEST_MONTHS}.\")\n",
    "         # Аналогично, можно скорректировать или оставить с предупреждением.\n",
    "\n",
    "    print(\"\\nКомпонентные ряды:\")\n",
    "    print(f\"  Размер обучающей выборки (train_comp_data): {train_comp_data.shape}\")\n",
    "    print(f\"  Размер тестовой выборки (test_comp_data): {test_comp_data.shape}\")\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"ОШИБКА: Не все колонки компонент {COMPONENTS_TO_MODEL} найдены.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "     print(f\"ОШИБКА при разделении компонентных рядов: {e}\")\n",
    "     raise\n",
    "\n",
    "print(\"\\n--- Раздел 1 завершен ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18081,
     "status": "ok",
     "timestamp": 1745841550384,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "C2WPMZSSpwsd",
    "outputId": "0e1cf538-3dc4-488a-e63a-2f0b69ae6a4c"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Раздел 2: Базовая Модель - ARIMA на Агрегате ---\")\n",
    "print(\"\\nЯчейка 6: Обучение и Оценка Baseline ARIMA\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_values: pd.Series - Обучающая выборка агрегированного ИПЦ\n",
    "# test_values: pd.Series - Тестовая выборка агрегированного ИПЦ\n",
    "# TEST_MONTHS: int - Количество месяцев в тесте\n",
    "# HORIZONS_TO_EVALUATE: list - Горизонты для оценки RMSE\n",
    "# aggregation_rmse: dict - Словарь для общих RMSE\n",
    "# all_horizon_rmse: pd.DataFrame - DataFrame для RMSE по горизонтам\n",
    "# final_forecasts: dict - Словарь для финальных прогнозов\n",
    "# PLOT_START_DATE: str - Дата для начала графика\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "model_name_baseline = \"Baseline ARIMA\"\n",
    "\n",
    "# 1. Подбор и обучение модели auto_arima\n",
    "print(f\"\\nПодбор и обучение {model_name_baseline}...\")\n",
    "try:\n",
    "    # Используем параметры, которые хорошо себя показали ранее\n",
    "    baseline_arima_model = auto_arima(train_values,\n",
    "                                      start_p=1, start_q=1, max_p=5, max_q=5, # Ограничим немного поиск\n",
    "                                      m=1, seasonal=False, # Без сезонности для месячных приростов\n",
    "                                      d=0,\n",
    "                                      test='adf',       # Тест для d\n",
    "                                      trace=False,      # Не выводить шаги подбора\n",
    "                                      error_action='ignore',\n",
    "                                      suppress_warnings=True,\n",
    "                                      stepwise=True,    # Ускоренный подбор\n",
    "                                      information_criterion='aic', # Критерий выбора\n",
    "                                      n_jobs=-1) # Используем все ядра CPU\n",
    "\n",
    "    print(f\"Выбрана модель: {baseline_arima_model.order} {baseline_arima_model.seasonal_order}\")\n",
    "\n",
    "    # 2. Генерация рекурсивного прогноза\n",
    "    print(f\"Генерация рекурсивного прогноза на {TEST_MONTHS} шагов...\")\n",
    "    forecast_baseline_arima = baseline_arima_model.predict(n_periods=TEST_MONTHS)\n",
    "    # Сохраняем прогноз в общий словарь\n",
    "    final_forecasts[model_name_baseline] = forecast_baseline_arima[:TEST_MONTHS]\n",
    "    print(\"Прогноз сохранен.\")\n",
    "\n",
    "    # 3. Оценка RMSE\n",
    "    # Убедимся, что длина прогноза совпадает с тестом\n",
    "    eval_len = min(len(forecast_baseline_arima), len(test_values))\n",
    "    if eval_len < TEST_MONTHS:\n",
    "        print(f\"ВНИМАНИЕ: Длина прогноза ({eval_len}) меньше длины теста ({len(test_values)}). Оценка по {eval_len} точкам.\")\n",
    "    y_test_eval = test_values[:eval_len]\n",
    "    forecast_eval = forecast_baseline_arima[:eval_len]\n",
    "\n",
    "    overall_rmse_baseline = np.sqrt(mean_squared_error(y_test_eval, forecast_eval))\n",
    "    print(f\"\\nОбщий RMSE на тесте для {model_name_baseline}: {overall_rmse_baseline:.4f}\")\n",
    "\n",
    "    # Добавляем/Обновляем общий RMSE\n",
    "    aggregation_rmse[model_name_baseline] = overall_rmse_baseline\n",
    "\n",
    "    # Расчет и добавление RMSE по горизонтам\n",
    "    print(\"RMSE по горизонтам:\")\n",
    "    # Удаляем старые записи для этой модели, если есть\n",
    "    all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_baseline]\n",
    "    horizon_results_list_baseline = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(y_test_eval[:h], forecast_eval[:h]))\n",
    "            print(f\"  1-{h} мес.: {rmse_h:.4f}\")\n",
    "            horizon_results_list_baseline.append({'model': model_name_baseline, 'horizon': h, 'rmse': rmse_h})\n",
    "        else:\n",
    "             print(f\"  1-{h} мес.: Недостаточно данных для расчета.\")\n",
    "\n",
    "    if horizon_results_list_baseline:\n",
    "        df_horizon_rmse_baseline = pd.DataFrame(horizon_results_list_baseline)\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon_rmse_baseline], ignore_index=True)\n",
    "    print(\"Результаты RMSE по горизонтам добавлены/обновлены.\")\n",
    "\n",
    "    # 4. Графики (опционально, но полезно)\n",
    "    # График прогноза\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    history_to_plot = train_values[train_values.index >= PLOT_START_DATE]\n",
    "    plt.plot(history_to_plot.index, history_to_plot, label='Обучающая выборка (часть)', alpha=0.7)\n",
    "    plt.plot(test_values.index, test_values, label='Тестовая выборка (реальные)', color='orange', linewidth=2)\n",
    "    forecast_baseline_series = pd.Series(forecast_eval, index=test_values.index[:eval_len])\n",
    "    plt.plot(forecast_baseline_series.index, forecast_baseline_series, label=f'Прогноз {model_name_baseline}', color='blue', linestyle='--')\n",
    "    plt.title(f'Прогноз ИПЦ с помощью {model_name_baseline}')\n",
    "    plt.xlabel('Дата'); plt.ylabel('ИПЦ (месячный рост)'); plt.legend(); plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # График RMSE vs Горизонт\n",
    "    if horizon_results_list_baseline:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df_horizon_rmse_baseline['horizon'], df_horizon_rmse_baseline['rmse'], marker='o', linestyle='-')\n",
    "        plt.title(f'Зависимость RMSE от горизонта прогноза для {model_name_baseline}')\n",
    "        plt.xlabel('Горизонт прогноза (месяцы)'); plt.ylabel('RMSE')\n",
    "        plt.xticks(HORIZONS_TO_EVALUATE); plt.grid(True); plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ОШИБКА при обучении или прогнозировании {model_name_baseline}: {e}\")\n",
    "    aggregation_rmse[model_name_baseline] = np.nan # Записываем NaN в случае ошибки\n",
    "    final_forecasts[model_name_baseline] = np.full(TEST_MONTHS, np.nan)\n",
    "\n",
    "print(f\"\\n--- Раздел 2 ({model_name_baseline}) завершен ---\")\n",
    "print(\"\\nТекущая таблица общих RMSE:\")\n",
    "print(pd.DataFrame(aggregation_rmse.items(), columns=['model', 'rmse']).sort_values(by='rmse'))\n",
    "print(\"\\nТекущая таблица RMSE по горизонтам:\")\n",
    "print(all_horizon_rmse.pivot(index='horizon', columns='model', values='rmse').round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1745841551038,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "1ss3SvtYqq3a",
    "outputId": "9334b3a9-b516-425c-b74c-9b64470ffacd"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "print(\"\\nЯчейка 5а: Анализ Стационарности Компонент (ADF Тест)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data: DataFrame с обучающими выборками компонент\n",
    "# COMPONENTS_TO_MODEL: Список имен компонент\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "significance_level_adf = 0.05 # Уровень значимости для теста\n",
    "adf_results = {} # Словарь для хранения результатов\n",
    "\n",
    "print(f\"Уровень значимости для ADF теста: {significance_level_adf}\")\n",
    "\n",
    "for component in COMPONENTS_TO_MODEL:\n",
    "    print(f\"\\n--- Тест для компоненты: {component} ---\")\n",
    "    try:\n",
    "        # Берем обучающую выборку компоненты\n",
    "        component_series = train_comp_data[component].dropna()\n",
    "\n",
    "        if component_series.empty:\n",
    "            print(\"  Ряд пуст после удаления NaN. Пропуск теста.\")\n",
    "            adf_results[component] = {'p_value': np.nan, 'is_stationary': None, 'error': 'Empty series'}\n",
    "            continue\n",
    "        if component_series.nunique() <= 1:\n",
    "             print(\"  Ряд содержит константу. Пропуск теста ADF (неприменим).\")\n",
    "             # Считаем такой ряд стационарным (d=0) для простоты\n",
    "             adf_results[component] = {'p_value': 0.0, 'is_stationary': True, 'error': 'Constant series'}\n",
    "             continue\n",
    "\n",
    "\n",
    "        # Проводим тест ADF\n",
    "        adf_test = adfuller(component_series, autolag='AIC')\n",
    "\n",
    "        adf_stat = adf_test[0]\n",
    "        p_value = adf_test[1]\n",
    "        crit_values = adf_test[4]\n",
    "\n",
    "        print(f\"  ADF Statistic: {adf_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        # print(\"  Критические значения:\")\n",
    "        # for key, value in crit_values.items():\n",
    "        #     print(f\"    {key}: {value:.4f}\")\n",
    "\n",
    "        # Интерпретация\n",
    "        if p_value <= significance_level_adf:\n",
    "            print(f\"  Вывод: Ряд '{component}' СТАЦИОНАРЕН (p <= {significance_level_adf}). Рекомендуется d=0.\")\n",
    "            adf_results[component] = {'p_value': p_value, 'is_stationary': True}\n",
    "        else:\n",
    "            print(f\"  Вывод: Ряд '{component}' НЕ СТАЦИОНАРЕН (p > {significance_level_adf}). auto_arima определит d.\")\n",
    "            adf_results[component] = {'p_value': p_value, 'is_stationary': False}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ОШИБКА при выполнении ADF теста для {component}: {e}\")\n",
    "        adf_results[component] = {'p_value': np.nan, 'is_stationary': None, 'error': str(e)}\n",
    "\n",
    "print(\"\\n--- Анализ стационарности компонент завершен ---\")\n",
    "# Можно вывести итоговый словарь adf_results, если нужно\n",
    "# print(\"\\nРезультаты ADF тестов:\")\n",
    "# print(adf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7173,
     "status": "ok",
     "timestamp": 1745841888939,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "zKRXZp6SSNtE",
    "outputId": "3015c92b-bf5a-417a-c82b-f5aa66e64dda"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import matplotlib.pyplot as plt # Добавляем импорт для графиков\n",
    "\n",
    "print(\"\\nЯчейка 5а: Анализ Стационарности Компонент (ADF Тест с Поправкой Холма)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data: DataFrame с обучающими выборками компонент\n",
    "# COMPONENTS_TO_MODEL_FINAL: Финальный список имен компонент (из Ячейки 3а)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "significance_level_adf = 0.05 # Уровень значимости для ОДНОГО теста\n",
    "alpha_fwer = 0.05 # Уровень значимости для КОНТРОЛЯ FWER\n",
    "adf_results = {}\n",
    "p_values_list = []\n",
    "components_tested = [] # Список компонент, для которых тест был проведен\n",
    "\n",
    "print(f\"Уровень значимости для контроля FWER (Метод Холма): {alpha_fwer}\")\n",
    "\n",
    "# --- Шаг 1: Проведение ADF тестов и сбор p-value ---\n",
    "print(\"\\n--- Проведение ADF тестов для компонент ---\")\n",
    "for component in COMPONENTS_TO_MODEL:\n",
    "    # print(f\"\\n--- Тест для компоненты: {component} ---\") # Убрал для краткости вывода\n",
    "    try:\n",
    "        component_series = train_comp_data[component].dropna()\n",
    "\n",
    "        if component_series.empty:\n",
    "            # print(\"  Ряд пуст. Пропуск.\")\n",
    "            adf_results[component] = {'p_value': np.nan, 'adf_stat': np.nan, 'is_stationary_holm': None, 'notes': 'Empty series'}\n",
    "            continue\n",
    "        if component_series.nunique() <= 1:\n",
    "            # print(\"  Ряд константа. Считаем стационарным (p=0).\")\n",
    "            adf_results[component] = {'p_value': 0.0, 'adf_stat': np.nan, 'is_stationary_holm': True, 'notes': 'Constant series'}\n",
    "            # Добавляем в списки для Холма\n",
    "            p_values_list.append(0.0)\n",
    "            components_tested.append(component)\n",
    "            continue\n",
    "\n",
    "        adf_test = adfuller(component_series, autolag='AIC')\n",
    "        adf_stat = adf_test[0]\n",
    "        p_value = adf_test[1]\n",
    "\n",
    "        adf_results[component] = {'p_value': p_value, 'adf_stat': adf_stat} # Сохраняем исходный p-value\n",
    "        # Добавляем в списки для Холма\n",
    "        p_values_list.append(p_value)\n",
    "        components_tested.append(component)\n",
    "        # print(f\"  p-value (исходный): {p_value:.4f}\") # Убрал для краткости\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ОШИБКА при ADF тесте для {component}: {e}\")\n",
    "        adf_results[component] = {'p_value': np.nan, 'adf_stat': np.nan, 'is_stationary_holm': None, 'notes': str(e)}\n",
    "\n",
    "# --- Шаг 2: Применение метода Холма ---\n",
    "print(f\"\\n--- Применение поправки Холма (alpha={alpha_fwer}) ---\")\n",
    "if not p_values_list:\n",
    "    print(\"Нет валидных p-value для применения поправки.\")\n",
    "else:\n",
    "    try:\n",
    "        reject_holm, pvals_corrected_holm, _, _ = multipletests(p_values_list,\n",
    "                                                                 alpha=alpha_fwer,\n",
    "                                                                 method='holm')\n",
    "\n",
    "        # Обновляем результаты в словаре adf_results\n",
    "        non_stationary_components_holm = []\n",
    "        for i, component in enumerate(components_tested):\n",
    "            is_stationary = reject_holm[i] # True если гипотеза о нестационарности отвергнута\n",
    "            adf_results[component]['p_value_holm'] = pvals_corrected_holm[i]\n",
    "            adf_results[component]['is_stationary_holm'] = is_stationary\n",
    "            if not is_stationary:\n",
    "                 non_stationary_components_holm.append(component)\n",
    "\n",
    "        print(f\"Поправка Холма применена к {len(p_values_list)} тестам.\")\n",
    "        if non_stationary_components_holm:\n",
    "            print(\"\\nКомпоненты, показавшие НЕстационарность ПОСЛЕ поправки Холма:\")\n",
    "            for comp in non_stationary_components_holm:\n",
    "                 p_orig = adf_results[comp]['p_value']\n",
    "                 p_holm = adf_results[comp]['p_value_holm']\n",
    "                 print(f\"  - {comp} (p_orig={p_orig:.4f}, p_holm={p_holm:.4f})\")\n",
    "        else:\n",
    "            print(\"\\nВсе протестированные компоненты стационарны после поправки Холма.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ОШИБКА при применении multipletests (Holm): {e}\")\n",
    "        # Помечаем все как неопределенные\n",
    "        for comp in components_tested: adf_results[comp]['is_stationary_holm'] = None\n",
    "\n",
    "\n",
    "# --- Шаг 3: Визуализация НЕстационарных рядов (если есть) ---\n",
    "if 'non_stationary_components_holm' in locals() and non_stationary_components_holm:\n",
    "    print(\"\\n--- Визуализация рядов, определенных как НЕстационарные ---\")\n",
    "    n_plots = len(non_stationary_components_holm)\n",
    "    n_cols_plot = 2 # По 2 графика в ряд\n",
    "    n_rows_plot = int(np.ceil(n_plots / n_cols_plot))\n",
    "\n",
    "    plt.figure(figsize=(14, 5 * n_rows_plot))\n",
    "    for i, component in enumerate(non_stationary_components_holm):\n",
    "        ax = plt.subplot(n_rows_plot, n_cols_plot, i + 1)\n",
    "        try:\n",
    "            component_series = train_comp_data[component].dropna()\n",
    "            plt.plot(component_series.index, component_series, marker='.', linestyle='-', markersize=3)\n",
    "            plt.title(f\"{component} (p_holm={adf_results[component]['p_value_holm']:.4f})\")\n",
    "            plt.ylabel(\"Значение\")\n",
    "            plt.grid(True)\n",
    "        except Exception as plot_e:\n",
    "             print(f\"Ошибка при построении графика для {component}: {plot_e}\")\n",
    "             ax.set_title(f\"{component} - Ошибка графика\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "     print(\"\\nВизуализация нестационарных рядов не требуется.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Анализ стационарности компонент завершен ---\")\n",
    "# Словарь adf_results теперь содержит 'is_stationary_holm' (True/False/None)\n",
    "# Его можно использовать для определения параметра 'd' в auto_arima, если нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225408,
     "status": "ok",
     "timestamp": 1745843299861,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "tcyYm2wrquSP",
    "outputId": "aeb02ec2-4291-4756-e728-52797ae0e133"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Раздел 3: Bottom-up с ARIMA Компонентами (Простой) ---\")\n",
    "print(\"\\nЯчейка 7: Обучение auto_arima для Каждой Компоненты (с выбором d)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data: DataFrame с обучающими выборками компонент\n",
    "# test_comp_data: DataFrame с тестовыми выборками компонент\n",
    "# COMPONENTS_TO_MODEL_FINAL: Финальный список имен компонент (из Ячейки 3а)\n",
    "# TEST_MONTHS: int\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Список компонент, для которых ADF тест был НЕ отвергнут (или показал явный тренд)\n",
    "# Обнови этот список на основе твоего последнего вывода Ячейки 5а\n",
    "non_stationary_candidates = [\n",
    "    'Алкогольные напитки', 'Общественное питание', 'Меха и меховые изделия',\n",
    "    'Табачные изделия', 'Печатные издания', 'Бытовые услуги',\n",
    "    'Услуги в системе образования', 'Услуги организаций культуры',\n",
    "    'Санаторно-оздоровительные услуги', 'Медицинские услуги'\n",
    "]\n",
    "\n",
    "df_forecasts_arima_comp_list = []\n",
    "arima_orders_dict = {} # Словарь для хранения ИТОГОВЫХ порядков\n",
    "\n",
    "print(f\"Обучение ARIMA и генерация прогнозов для {len(COMPONENTS_TO_MODEL)} компонент...\")\n",
    "\n",
    "for component in COMPONENTS_TO_MODEL:\n",
    "    print(f\"\\n--- Компонента: {component} ---\")\n",
    "    d_param = None # По умолчанию позволяем auto_arima решать\n",
    "\n",
    "    # Определяем параметр d на основе результатов ADF и нашего анализа\n",
    "    if component not in non_stationary_candidates:\n",
    "        # Если компонент прошел тест на стационарность (или мы решили, что он стационарен)\n",
    "        d_param = 0\n",
    "        print(f\"  Устанавливаем d=0 (стационарный по тесту/анализу)\")\n",
    "    else:\n",
    "        # Для нестационарных или спорных - позволяем auto_arima выбрать d\n",
    "        print(f\"  Позволяем auto_arima определить d (потенциально нестационарный)\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        y_train_comp = train_comp_data[component].dropna()\n",
    "        y_test_comp = test_comp_data[component]\n",
    "\n",
    "        if y_train_comp.empty:\n",
    "            print(\"  Обучающий ряд пуст. Пропуск.\")\n",
    "            forecast = np.full(TEST_MONTHS, np.nan)\n",
    "            order = None\n",
    "        elif y_train_comp.nunique() <= 1:\n",
    "             print(\"  Обучающий ряд содержит константу. Прогноз = константа.\")\n",
    "             forecast = np.full(TEST_MONTHS, y_train_comp.iloc[0])\n",
    "             order = (0,0,0) # Условно\n",
    "        else:\n",
    "            # Используем auto_arima с определенным d_param (0 или None)\n",
    "            arima_comp_model = auto_arima(y_train_comp,\n",
    "                                          start_p=1, start_q=1, max_p=5, max_q=5,\n",
    "                                          m=1, seasonal=False,\n",
    "                                          d=d_param, # <--- ИСПОЛЬЗУЕМ d=0 или d=None\n",
    "                                          test='adf', # Тест все равно запустится, если d=None\n",
    "                                          trace=False, error_action='ignore', suppress_warnings=True,\n",
    "                                          stepwise=True, information_criterion='aic', n_jobs=-1)\n",
    "\n",
    "            order = arima_comp_model.order # Получаем ИТОГОВЫЙ порядок (p,d,q)\n",
    "            print(f\"  Выбрана модель: {order} {arima_comp_model.seasonal_order}\")\n",
    "\n",
    "            # Генерация прогноза\n",
    "            forecast = arima_comp_model.predict(n_periods=TEST_MONTHS)\n",
    "\n",
    "        # Сохраняем итоговый порядок\n",
    "        arima_orders_dict[component] = order\n",
    "        # Сохраняем прогноз\n",
    "        forecast_series = pd.Series(forecast, index=y_test_comp.index[:len(forecast)], name=component)\n",
    "        df_forecasts_arima_comp_list.append(forecast_series)\n",
    "        print(f\"  Прогноз для {component} сгенерирован.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ОШИБКА при обработке {component}: {e}\")\n",
    "        forecast_series = pd.Series(np.nan, index=test_comp_data.index[:TEST_MONTHS], name=component)\n",
    "        df_forecasts_arima_comp_list.append(forecast_series)\n",
    "        arima_orders_dict[component] = None\n",
    "\n",
    "# Объединяем прогнозы в DataFrame\n",
    "df_forecasts_arima_comp = pd.concat(df_forecasts_arima_comp_list, axis=1)\n",
    "\n",
    "print(\"\\n--- Прогнозы ARIMA для компонент готовы ---\")\n",
    "print(\"Итоговые порядки ARIMA (p,d,q):\")\n",
    "# Выведем порядки в более читаемом виде\n",
    "for comp, order in arima_orders_dict.items():\n",
    "    print(f\"  - {comp}: {order}\")\n",
    "# print(arima_orders_dict) # Можно вывести и весь словарь\n",
    "print(\"\\nПервые 5 строк прогнозов:\")\n",
    "print(df_forecasts_arima_comp.head())\n",
    "print(\"\\nПроверка на NaN в прогнозах:\")\n",
    "print(df_forecasts_arima_comp.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Ячейка 7 (для Раздела 3) завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1745843806458,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "ucnj1D1EtO_3",
    "outputId": "bb45d806-d31b-4767-ce32-21b7600392ba"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 8: Агрегация ARIMA Прогнозов Простыми Весами (с Учетом 'Прочего')\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_forecasts_arima_comp: DataFrame [ИндексТеста x 44 Компоненты] - прогнозы ARIMA\n",
    "# weights_norm: DataFrame [Год x 44 Компоненты] - нормализованные веса (сумма < 1)\n",
    "# sum_weights_modelled: pd.Series [Год] - Суммарный вес 44 компонент по годам (из Ячейки 4)\n",
    "# train_values: Series - Обуч. выборка агрегата\n",
    "# test_values: Series - Тест. выборка агрегата\n",
    "# COMPONENTS_TO_MODEL_FINAL: list - Финальный список 44 компонент\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# aggregation_rmse, all_horizon_rmse, final_forecasts\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Проверка df_forecasts_arima_comp\n",
    "if 'df_forecasts_arima_comp' not in locals() or df_forecasts_arima_comp is None:\n",
    "    print(\"ОШИБКА: DataFrame 'df_forecasts_arima_comp' не найден. Запустите Ячейку 7.\")\n",
    "    raise NameError(\"df_forecasts_arima_comp не определен\")\n",
    "elif df_forecasts_arima_comp.isnull().any().any():\n",
    "    print(\"ВНИМАНИЕ: Обнаружены NaN в прогнозах компонент ARIMA. Агрегация может быть некорректна.\")\n",
    "    # Можно добавить обработку NaN здесь, если требуется\n",
    "\n",
    "# --- Метод 1: Веса последнего года обучения (с масштабированием) ---\n",
    "model_name_arima_lastw = \"BottomUp-ARIMA-LastW\"\n",
    "print(f\"\\n--- {model_name_arima_lastw} ---\")\n",
    "try:\n",
    "    last_train_date_agg = train_values.index.max()\n",
    "    last_train_year_agg = last_train_date_agg.year\n",
    "\n",
    "    if last_train_year_agg in weights_norm.index and last_train_year_agg in sum_weights_modelled.index:\n",
    "        # Веса ТОЛЬКО для 44 компонент\n",
    "        last_train_weights_44 = weights_norm.loc[last_train_year_agg, COMPONENTS_TO_MODEL]\n",
    "        # Сумма весов ТОЛЬКО для 44 компонент в этом году\n",
    "        sum_weights_44_last = sum_weights_modelled.loc[last_train_year_agg]\n",
    "\n",
    "        print(f\"Используются веса за {last_train_year_agg} год (сумма = {sum_weights_44_last:.4f})\")\n",
    "\n",
    "        # Шаг А: Взвешенная сумма прогнозов 44 компонент\n",
    "        forecast_44_weighted = (df_forecasts_arima_comp[COMPONENTS_TO_MODEL] * last_train_weights_44).sum(axis=1)\n",
    "\n",
    "        # Шаг Б: Масштабирование на суммарный вес\n",
    "        if sum_weights_44_last > 1e-6: # Проверка деления на ноль\n",
    "             final_forecast_scaled = forecast_44_weighted / sum_weights_44_last\n",
    "        else:\n",
    "             print(\"ОШИБКА: Сумма весов равна нулю, масштабирование невозможно.\")\n",
    "             final_forecast_scaled = pd.Series(np.nan, index=forecast_44_weighted.index)\n",
    "\n",
    "        final_forecasts[model_name_arima_lastw] = final_forecast_scaled.values\n",
    "\n",
    "        # Оценка\n",
    "        eval_len = min(len(final_forecast_scaled), len(test_values))\n",
    "        # Проверим наличие NaN в финальном прогнозе перед оценкой\n",
    "        if pd.isna(final_forecast_scaled[:eval_len]).any():\n",
    "             print(\"ПРЕДУПРЕЖДЕНИЕ: NaN в итоговом прогнозе после масштабирования.\")\n",
    "             overall_rmse = np.nan\n",
    "        else:\n",
    "             overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], final_forecast_scaled[:eval_len]))\n",
    "        print(f\"Общий RMSE: {overall_rmse:.4f}\")\n",
    "        aggregation_rmse[model_name_arima_lastw] = overall_rmse\n",
    "\n",
    "        # ... (код расчета RMSE по горизонтам, аналогично предыдущему, используя final_forecast_scaled) ...\n",
    "        print(\"RMSE по горизонтам:\")\n",
    "        horizon_results_list = []\n",
    "        for h in HORIZONS_TO_EVALUATE:\n",
    "            if h <= eval_len and not pd.isna(final_forecast_scaled[:h]).any():\n",
    "                rmse_h = np.sqrt(mean_squared_error(test_values[:h], final_forecast_scaled[:h]))\n",
    "                # print(f\"  1-{h} мес.: {rmse_h:.4f}\")\n",
    "                horizon_results_list.append({'model': model_name_arima_lastw, 'horizon': h, 'rmse': rmse_h})\n",
    "        if horizon_results_list:\n",
    "             df_horizon = pd.DataFrame(horizon_results_list)\n",
    "             all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_arima_lastw]\n",
    "             all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "        print(\"RMSE по горизонтам рассчитаны.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"ОШИБКА: Веса или сумма весов за {last_train_year_agg} год не найдены!\")\n",
    "        aggregation_rmse[model_name_arima_lastw] = np.nan\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при расчете {model_name_arima_lastw}: {e}\")\n",
    "    aggregation_rmse[model_name_arima_lastw] = np.nan\n",
    "\n",
    "\n",
    "# --- Метод 2: Средние веса (с масштабированием) ---\n",
    "model_name_arima_avgw = \"BottomUp-ARIMA-AvgW\"\n",
    "print(f\"\\n--- {model_name_arima_avgw} ---\")\n",
    "try:\n",
    "    # Средние веса ТОЛЬКО для 44 компонент\n",
    "    avg_weights_44 = weights_norm.mean()[COMPONENTS_TO_MODEL]\n",
    "    # Средняя сумма весов ТОЛЬКО для 44 компонент\n",
    "    avg_sum_weights_44 = sum_weights_modelled.mean()\n",
    "\n",
    "    print(f\"Используемые средние веса (средняя сумма = {avg_sum_weights_44:.4f})\")\n",
    "\n",
    "    # Шаг А: Взвешенная сумма прогнозов 44 компонент\n",
    "    forecast_44_weighted_avg = (df_forecasts_arima_comp[COMPONENTS_TO_MODEL] * avg_weights_44).sum(axis=1)\n",
    "\n",
    "    # Шаг Б: Масштабирование на средний суммарный вес\n",
    "    if avg_sum_weights_44 > 1e-6:\n",
    "        final_forecast_scaled_avg = forecast_44_weighted_avg / avg_sum_weights_44\n",
    "    else:\n",
    "        print(\"ОШИБКА: Средняя сумма весов равна нулю.\")\n",
    "        final_forecast_scaled_avg = pd.Series(np.nan, index=forecast_44_weighted_avg.index)\n",
    "\n",
    "    final_forecasts[model_name_arima_avgw] = final_forecast_scaled_avg.values\n",
    "\n",
    "    # Оценка\n",
    "    eval_len = min(len(final_forecast_scaled_avg), len(test_values))\n",
    "    if pd.isna(final_forecast_scaled_avg[:eval_len]).any():\n",
    "         print(\"ПРЕДУПРЕЖДЕНИЕ: NaN в итоговом прогнозе после масштабирования.\")\n",
    "         overall_rmse = np.nan\n",
    "    else:\n",
    "        overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], final_forecast_scaled_avg[:eval_len]))\n",
    "    print(f\"Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name_arima_avgw] = overall_rmse\n",
    "\n",
    "    # ... (код расчета RMSE по горизонтам, используя final_forecast_scaled_avg) ...\n",
    "    print(\"RMSE по горизонтам:\")\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len and not pd.isna(final_forecast_scaled_avg[:h]).any():\n",
    "           rmse_h = np.sqrt(mean_squared_error(test_values[:h], final_forecast_scaled_avg[:h]))\n",
    "           # print(f\"  1-{h} мес.: {rmse_h:.4f}\")\n",
    "           horizon_results_list.append({'model': model_name_arima_avgw, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_arima_avgw]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "    print(\"RMSE по горизонтам рассчитаны.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при расчете {model_name_arima_avgw}: {e}\")\n",
    "    aggregation_rmse[model_name_arima_avgw] = np.nan\n",
    "\n",
    "# --- Вывод обновленных таблиц ---\n",
    "# ... (код для вывода таблиц RMSE как в предыдущих ячейках) ...\n",
    "print(\"\\n--- Обновленное сравнение RMSE (Общий RMSE) ---\")\n",
    "# ... (код вывода отсортированного aggregation_rmse) ...\n",
    "if 'aggregation_rmse' in locals() and aggregation_rmse:\n",
    "    valid_keys = [k for k in aggregation_rmse if pd.notna(aggregation_rmse[k])]\n",
    "    if valid_keys:\n",
    "        sorted_rmse = {k: aggregation_rmse[k] for k in sorted(valid_keys, key=aggregation_rmse.get)}\n",
    "        for model, rmse in sorted_rmse.items(): print(f\"  {model}: {rmse:.4f}\")\n",
    "    else: print(\"Нет валидных RMSE для сортировки.\")\n",
    "else: print(\"Словарь aggregation_rmse пуст.\")\n",
    "\n",
    "print(\"\\nОбновленная таблица RMSE по горизонтам:\")\n",
    "# ... (код вывода сводной таблицы all_horizon_rmse) ...\n",
    "if 'all_horizon_rmse' in locals() and not all_horizon_rmse.empty:\n",
    "    try:\n",
    "        pivot_rmse_final = all_horizon_rmse.pivot(index='horizon', columns='model', values='rmse')\n",
    "        if 'sorted_rmse' in locals():\n",
    "             ordered_columns_keys = list(sorted_rmse.keys())\n",
    "             ordered_columns_final = [col for col in ordered_columns_keys if col in pivot_rmse_final.columns]\n",
    "             remaining_cols_final = [col for col in pivot_rmse_final.columns if col not in ordered_columns_final]\n",
    "             pivot_rmse_final = pivot_rmse_final[ordered_columns_final + remaining_cols_final]\n",
    "        print(pivot_rmse_final.round(4))\n",
    "    except Exception as e: print(f\"Не удалось создать сводную таблицу: {e}\")\n",
    "else: print(\"Нет данных.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Ячейка 8 (с масштабированием) завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21833,
     "status": "ok",
     "timestamp": 1745844848332,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "vVS06JSjvC1x",
    "outputId": "d178476b-8a95-44f2-8ce6-a893701fd7d9"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 9: Генерация OOF-прогнозов ARIMA Компонент\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data: DataFrame с обучающими выборками компонент\n",
    "# components_to_model: Список имен компонент\n",
    "# arima_orders_dict: Словарь с порядками ARIMA для компонент (из Ячейки 7)\n",
    "# N_OOF_SPLITS: int - Количество сплитов для OOF\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "components_to_model = COMPONENTS_TO_MODEL\n",
    "\n",
    "# --- Инициализация DataFrame для OOF прогнозов ARIMA ---\n",
    "df_oof_comp_arima = pd.DataFrame(index=train_comp_data.index,\n",
    "                                columns=components_to_model,\n",
    "                                dtype=float)\n",
    "\n",
    "print(f\"Генерация OOF-прогнозов ARIMA для {len(components_to_model)} компонент...\")\n",
    "print(f\"Используется TimeSeriesSplit с {N_OOF_SPLITS} фолдами.\")\n",
    "\n",
    "# --- Цикл по компонентам ---\n",
    "for i, component_name in enumerate(components_to_model):\n",
    "    print(f\"\\n({i+1}/{len(components_to_model)}) OOF для компоненты: {component_name}\")\n",
    "\n",
    "    component_series_train = train_comp_data[component_name].rename('t').dropna()\n",
    "    component_order = arima_orders_dict.get(component_name)\n",
    "\n",
    "    if component_order is None:\n",
    "        print(f\"  ОШИБКА: Порядок ARIMA для {component_name} не определен. Пропуск OOF.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Используется порядок: {component_order}\")\n",
    "\n",
    "    # --- Подготовка к OOF ---\n",
    "    tscv_oof_arima = TimeSeriesSplit(n_splits=N_OOF_SPLITS)\n",
    "    oof_predictions_component = pd.Series(index=component_series_train.index, dtype=float)\n",
    "\n",
    "    split_count = 0\n",
    "    for train_idx, val_idx in tscv_oof_arima.split(component_series_train):\n",
    "        split_count += 1\n",
    "        train_data_split = component_series_train.iloc[train_idx]\n",
    "        val_indices = component_series_train.index[val_idx]\n",
    "        n_steps_val = len(val_idx)\n",
    "\n",
    "        # Проверка на достаточный размер обучающей выборки фолда\n",
    "        min_obs_req = sum(component_order) + 5 # Примерная оценка\n",
    "        if len(train_data_split) < min_obs_req:\n",
    "             print(f\"    Фолд {split_count}/{N_OOF_SPLITS}: Пропуск (мало данных: {len(train_data_split)} < {min_obs_req})\")\n",
    "             continue\n",
    "        else:\n",
    "             #print(f\"    Фолд {split_count}/{N_OOF_SPLITS}: Обучение на {len(train_data_split)}, прогноз на {n_steps_val}\")\n",
    "             pass # Убрал вывод для краткости\n",
    "\n",
    "        # --- Обучение и прогноз ARIMA на фолде ---\n",
    "        try:\n",
    "            # Используем statsmodels ARIMA с ФИКСИРОВАННЫМ порядком\n",
    "            model_oof = StatsmodelsARIMA(train_data_split, order=component_order,\n",
    "                                         enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "            forecast_val = model_oof.forecast(steps=n_steps_val).values\n",
    "\n",
    "            # Сохраняем прогноз фолда\n",
    "            oof_predictions_component.loc[val_indices] = forecast_val[:len(val_indices)]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ОШИБКА на фолде {split_count} для {component_name}: {e}\")\n",
    "\n",
    "    # Сохраняем OOF-прогнозы для компоненты\n",
    "    df_oof_comp_arima[component_name] = oof_predictions_component\n",
    "    print(f\"  OOF-прогнозы ARIMA для {component_name} сгенерированы.\")\n",
    "\n",
    "print(\"\\n--- Генерация OOF-прогнозов ARIMA завершена ---\")\n",
    "\n",
    "# --- Постобработка и проверка OOF ---\n",
    "print(\"\\nКоличество НЕ-NaN OOF-прогнозов ARIMA по компонентам:\")\n",
    "print(df_oof_comp_arima.notna().sum())\n",
    "\n",
    "df_oof_comp_arima_clean = df_oof_comp_arima.dropna()\n",
    "\n",
    "if df_oof_comp_arima_clean.empty:\n",
    "     print(\"\\nОШИБКА: После удаления NaN не осталось OOF-прогнозов ARIMA.\")\n",
    "     # Дальнейший код обучения мета-модели выполнять не стоит\n",
    "else:\n",
    "    # Выравниваем таргет для МЕТА-модели (используем train_values из Раздела 1)\n",
    "    train_values_aligned_meta_arima = train_values.loc[df_oof_comp_arima_clean.index]\n",
    "\n",
    "    print(f\"\\nРазмер OOF DataFrame ARIMA после удаления NaN: {df_oof_comp_arima_clean.shape}\")\n",
    "    print(f\"Размер выровненного таргета для мета-модели: {train_values_aligned_meta_arima.shape}\")\n",
    "    print(\"\\nПервые 5 строк OOF-прогнозов ARIMA:\")\n",
    "    print(df_oof_comp_arima_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1745845008059,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "hJ0NH6kjwMno",
    "outputId": "617abecd-af8f-4c4e-e6e7-9f2d675edf35"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 7: Определение Вспомогательных Функций\")\n",
    "\n",
    "# --- 1. Функция для Feature Engineering (для мета-моделей) ---\n",
    "def create_meta_features(df_input, component_names, lags, windows, feature_order_meta):\n",
    "    \"\"\"\n",
    "    Создает признаки для мета-модели на основе прогнозов компонент.\n",
    "    Использует глобальные LAGS_TO_CREATE, ROLLING_WINDOWS.\n",
    "\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): DataFrame с прогнозами компонент (индекс=Дата).\n",
    "        component_names (list): Список названий компонент (колонок в df_input).\n",
    "        lags (list): Список создаваемых лагов.\n",
    "        windows (list): Список окон для скользящих статистик.\n",
    "        feature_order_meta (list): Ожидаемый порядок выходных признаков.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame с оригинальными прогнозами и новыми признаками.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    # Используем индекс исходного df для нового DataFrame признаков\n",
    "    feature_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 1. Добавляем оригинальные прогнозы как базовые фичи\n",
    "    for comp in component_names:\n",
    "        # Добавляем префикс _pred для ясности, что это прогнозы\n",
    "        feature_df[f'{comp}_pred'] = df[comp]\n",
    "\n",
    "    # 2. Лаги\n",
    "    for lag in lags:\n",
    "        for comp in component_names:\n",
    "            feature_df[f'{comp}_pred_lag{lag}'] = df[comp].shift(lag)\n",
    "\n",
    "    # 3. Скользящие статистики\n",
    "    for window in windows:\n",
    "        for comp in component_names:\n",
    "            feature_df[f'{comp}_pred_roll_mean{window}'] = df[comp].rolling(window=window, min_periods=1).mean()\n",
    "            feature_df[f'{comp}_pred_roll_std{window}'] = df[comp].rolling(window=window, min_periods=1).std()\n",
    "\n",
    "    # 4. Разности (с лагом 1)\n",
    "    for comp in component_names:\n",
    "        feature_df[f'{comp}_pred_diff1'] = df[comp].diff(1)\n",
    "\n",
    "    # 5. Календарные признаки\n",
    "    try:\n",
    "        # Проверяем, что индекс - DatetimeIndex\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            feature_df['month_meta_sin'] = np.sin(2 * np.pi * df.index.month / 12)\n",
    "            feature_df['month_meta_cos'] = np.cos(2 * np.pi * df.index.month / 12)\n",
    "        else:\n",
    "            print(\"Предупреждение: Индекс не является DatetimeIndex, календарные признаки не добавлены.\")\n",
    "            feature_df['month_meta_sin'] = np.nan\n",
    "            feature_df['month_meta_cos'] = np.nan\n",
    "    except AttributeError:\n",
    "         print(\"Предупреждение: Не удалось извлечь месяц из индекса, календарные признаки не добавлены.\")\n",
    "         feature_df['month_meta_sin'] = np.nan\n",
    "         feature_df['month_meta_cos'] = np.nan\n",
    "\n",
    "\n",
    "    # Переупорядочиваем колонки согласно feature_order_meta\n",
    "    # Сначала создадим список всех колонок, которые есть в feature_df\n",
    "    available_columns = feature_df.columns.tolist()\n",
    "    # Создадим итоговый список колонок, беря их из feature_order_meta, если они есть\n",
    "    final_columns_order = [col for col in feature_order_meta if col in available_columns]\n",
    "    # Проверим, все ли колонки из feature_order_meta нашлись\n",
    "    if len(final_columns_order) != len(feature_order_meta):\n",
    "        missing_cols = set(feature_order_meta) - set(available_columns)\n",
    "        print(f\"Предупреждение: Не все ожидаемые признаки найдены/созданы! Отсутствуют: {missing_cols}\")\n",
    "        # Можно либо добавить недостающие колонки с NaN, либо продолжить с тем, что есть\n",
    "        # Пока продолжим с тем, что есть\n",
    "    feature_df = feature_df[final_columns_order]\n",
    "\n",
    "    #print(f\"Создано/отобрано признаков для мета-модели: {feature_df.shape[1]}\")\n",
    "    return feature_df\n",
    "\n",
    "# Определяем ОЖИДАЕМЫЙ порядок признаков для МЕТА-моделей\n",
    "# (На основе того, что генерирует функция create_meta_features)\n",
    "# Важно, чтобы он был консистентным везде, где используется эта функция!\n",
    "META_FEATURE_ORDER = []\n",
    "for comp in COMPONENTS_TO_MODEL: META_FEATURE_ORDER.append(f'{comp}_pred')\n",
    "for lag in LAGS_TO_CREATE:\n",
    "    for comp in COMPONENTS_TO_MODEL: META_FEATURE_ORDER.append(f'{comp}_pred_lag{lag}')\n",
    "for window in ROLLING_WINDOWS:\n",
    "    for comp in COMPONENTS_TO_MODEL: META_FEATURE_ORDER.append(f'{comp}_pred_roll_mean{window}')\n",
    "    for comp in COMPONENTS_TO_MODEL: META_FEATURE_ORDER.append(f'{comp}_pred_roll_std{window}')\n",
    "for comp in COMPONENTS_TO_MODEL: META_FEATURE_ORDER.append(f'{comp}_pred_diff1')\n",
    "META_FEATURE_ORDER.extend(['month_meta_sin', 'month_meta_cos'])\n",
    "\n",
    "\n",
    "# --- 2. Функция для Feature Engineering (для моделей компонент) ---\n",
    "def calculate_features_for_step(history_series: pd.Series, feature_order_comp: list):\n",
    "    \"\"\"\n",
    "    Рассчитывает набор признаков для прогноза СЛЕДУЮЩЕГО шага компоненты.\n",
    "    Использует глобальные LAGS_TO_CREATE, ROLLING_WINDOWS.\n",
    "\n",
    "    Args:\n",
    "        history_series (pd.Series): Временной ряд значений компоненты (y) с DatetimeIndex.\n",
    "                                     Имя серии должно быть 't'.\n",
    "        feature_order_comp (list): Ожидаемый порядок признаков для моделей компонент.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Строка признаков для прогнозирования y(t),\n",
    "                   индексированная именами признаков, или None.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    n = len(history_series)\n",
    "    if n == 0: return None\n",
    "    try:\n",
    "        next_index = history_series.index[-1] + pd.DateOffset(months=1)\n",
    "    except (TypeError, IndexError): return None # Ошибка индекса\n",
    "\n",
    "    # Лаги\n",
    "    for lag in LAGS_TO_CREATE:\n",
    "        features[f't-{lag}'] = history_series.iloc[-lag] if n >= lag else np.nan\n",
    "\n",
    "    # Скользящие статистики\n",
    "    for window in ROLLING_WINDOWS:\n",
    "        if n >= window:\n",
    "            window_data = history_series.iloc[-window:]\n",
    "            features[f't_mean_lag{window}'] = window_data.mean()\n",
    "            features[f't_std_lag{window}'] = window_data.std()\n",
    "        else:\n",
    "            features[f't_mean_lag{window}'] = np.nan\n",
    "            features[f't_std_lag{window}'] = np.nan\n",
    "\n",
    "    # Календарные признаки\n",
    "    try:\n",
    "        next_month_num = next_index.month\n",
    "        features['month_sin'] = np.sin(2 * np.pi * next_month_num / 12)\n",
    "        features['month_cos'] = np.cos(2 * np.pi * next_month_num / 12)\n",
    "    except AttributeError: # На случай, если индекс не DatetimeIndex\n",
    "        features['month_sin'] = np.nan\n",
    "        features['month_cos'] = np.nan\n",
    "\n",
    "    features_series = pd.Series(features)\n",
    "    if features_series.isnull().any(): return None\n",
    "    try:\n",
    "        return features_series[feature_order_comp] # Используем порядок для компонент\n",
    "    except KeyError as e:\n",
    "        print(f\"Ошибка calculate_features_for_step: Несовпадение имен признаков! {e}\")\n",
    "        return None\n",
    "\n",
    "# Определяем ОЖИДАЕМЫЙ порядок признаков для моделей КОМПОНЕНТ\n",
    "# (На основе того, что генерирует функция calculate_features_for_step)\n",
    "COMP_FEATURE_ORDER = FEATURE_ORDER # Используем настройку из Ячейки 2\n",
    "\n",
    "# --- 3. Функция для рекурсивного прогноза (обновленная) ---\n",
    "def recursive_predict(model, initial_history_series: pd.Series, n_steps: int,\n",
    "                      feature_calculator, feature_order_func: list, scaler=None): # Добавили feature_order_func\n",
    "    \"\"\"\n",
    "    Генерирует рекурсивный многошаговый прогноз.\n",
    "\n",
    "    Args:\n",
    "        model: Обученная ML модель (с методом predict).\n",
    "        initial_history_series (pd.Series): Исходная история y до начала прогноза. Имя должно быть 't'.\n",
    "        n_steps (int): Количество шагов прогноза.\n",
    "        feature_calculator (function): Функция, рассчитывающая признаки для шага.\n",
    "        feature_order_func (list): Порядок колонок признаков, ожидаемый функцией калькулятора и моделью.\n",
    "        scaler (object, optional): Scaler для масштабирования признаков (если нужен).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Массив с прогнозами на n_steps шагов.\n",
    "    \"\"\"\n",
    "    current_history = initial_history_series.copy()\n",
    "    predictions = []\n",
    "    #print(f\"Запуск рекурсивного прогноза для {model.__class__.__name__} на {n_steps} шагов...\")\n",
    "    for i in range(n_steps):\n",
    "        # Рассчитываем признаки, передавая правильный порядок\n",
    "        features_for_step = feature_calculator(current_history, feature_order_func)\n",
    "\n",
    "        if features_for_step is None:\n",
    "            print(f\"  Предупреждение: Не удалось рассчитать признаки на шаге {i+1} для {model.__class__.__name__}. Заполняем NaN.\")\n",
    "            # Заполняем оставшиеся прогнозы NaN\n",
    "            predictions.extend([np.nan] * (n_steps - i))\n",
    "            break\n",
    "\n",
    "        # Подготовка признаков для модели\n",
    "        features_df = features_for_step.to_frame().T\n",
    "        if scaler:\n",
    "             # Масштабируем ПЕРЕД подачей в модель\n",
    "             try:\n",
    "                 features_scaled = scaler.transform(features_df)\n",
    "                 model_input = features_scaled\n",
    "             except Exception as scale_e:\n",
    "                 print(f\"  Ошибка масштабирования на шаге {i+1}: {scale_e}\")\n",
    "                 predictions.extend([np.nan] * (n_steps - i))\n",
    "                 break\n",
    "        else:\n",
    "             model_input = features_df\n",
    "\n",
    "        # Прогноз на 1 шаг\n",
    "        try:\n",
    "             next_pred = model.predict(model_input)[0]\n",
    "        except Exception as pred_e:\n",
    "             print(f\"  Ошибка предсказания модели {model.__class__.__name__} на шаге {i+1}: {pred_e}\")\n",
    "             predictions.extend([np.nan] * (n_steps - i))\n",
    "             break\n",
    "\n",
    "        # Сохраняем прогноз\n",
    "        predictions.append(next_pred)\n",
    "\n",
    "        # Обновляем историю\n",
    "        try:\n",
    "            next_index = current_history.index[-1] + pd.DateOffset(months=1)\n",
    "            current_history = pd.concat([current_history, pd.Series([next_pred], index=[next_index], name='t')])\n",
    "        except (TypeError, IndexError, ValueError) as hist_e:\n",
    "             print(f\"  Ошибка обновления истории на шаге {i+1}: {hist_e}\")\n",
    "             # Продолжаем без обновления истории, если это возможно, но прогнозы могут ухудшиться\n",
    "             # Или прерываем:\n",
    "             # predictions.extend([np.nan] * (n_steps - i))\n",
    "             # break\n",
    "             pass # Пока просто продолжаем\n",
    "\n",
    "    #print(f\"Рекурсивный прогноз для {model.__class__.__name__} завершен.\")\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# --- 4. Функции для обучения и прогноза моделей компонент (обновленные) ---\n",
    "\n",
    "def train_predict_arima(y_train_comp, n_steps):\n",
    "    \"\"\"Обучает auto_arima (с d=0) и делает рекурсивный прогноз.\"\"\"\n",
    "    print(f\"    Обучение ARIMA для ряда длиной {len(y_train_comp)}...\")\n",
    "    try:\n",
    "        model = auto_arima(y_train_comp, start_p=1, start_q=1, max_p=3, max_q=3,\n",
    "                           m=1, seasonal=False, d=0, test='adf', # Используем d=0\n",
    "                           trace=False, error_action='ignore', suppress_warnings=True,\n",
    "                           stepwise=True, information_criterion='aic', n_jobs=1)\n",
    "        order = model.order\n",
    "        print(f\"    ARIMA обучена, порядок: {order}\")\n",
    "        # Используем statsmodels для рекурсивного прогноза (стабильнее)\n",
    "        history_arima = list(y_train_comp)\n",
    "        arima_forecast_rec = []\n",
    "        for _ in range(n_steps):\n",
    "            model_rec = StatsmodelsARIMA(history_arima, order=order,\n",
    "                                        enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "            next_pred = model_rec.forecast(steps=1)[0]\n",
    "            arima_forecast_rec.append(next_pred)\n",
    "            history_arima.append(next_pred)\n",
    "        print(\"    Рекурсивный прогноз ARIMA выполнен.\")\n",
    "        return np.array(arima_forecast_rec), order\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка ARIMA: {e}\")\n",
    "        return np.full(n_steps, np.nan), None\n",
    "\n",
    "def train_predict_ml(model_class, model_params_grid, y_train_comp, n_steps,\n",
    "                     feature_calculator, feature_order_ml, tscv_ml):\n",
    "    \"\"\"Подбирает гиперпараметры, обучает ML модель (XGB, RF) и делает рекурсивный прогноз.\"\"\"\n",
    "    model_name = model_class.__name__.replace('Regressor','')\n",
    "    print(f\"    Подготовка данных и GridSearchCV для {model_name}...\")\n",
    "    try:\n",
    "        # Подготовка признаков для обучения и CV\n",
    "        data_ml_comp = pd.DataFrame({'t': y_train_comp})\n",
    "        # ... (код генерации признаков data_ml_comp как в предыдущей версии) ...\n",
    "        for lag in LAGS_TO_CREATE: data_ml_comp[f't-{lag}'] = data_ml_comp['t'].shift(lag)\n",
    "        target_series = data_ml_comp['t']\n",
    "        for window in ROLLING_WINDOWS:\n",
    "            data_ml_comp[f't_mean_lag{window}'] = target_series.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "            data_ml_comp[f't_std_lag{window}'] = target_series.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        month_num = data_ml_comp.index.month\n",
    "        data_ml_comp['month_sin'] = np.sin(2 * np.pi * month_num / 12)\n",
    "        data_ml_comp['month_cos'] = np.cos(2 * np.pi * month_num / 12)\n",
    "        data_ml_comp = data_ml_comp.dropna()\n",
    "\n",
    "        # Внутри функции train_predict_ml, ПЕРЕД X_train_comp = ...\n",
    "        print(f\"    Размер data_ml_comp ПОСЛЕ dropna: {data_ml_comp.shape}\")\n",
    "        print(f\"    Колонки data_ml_comp: {data_ml_comp.columns.tolist()}\")\n",
    "        print(f\"    Ожидаемый порядок feature_order_ml: {feature_order_ml}\")\n",
    "        # Добавим проверку на пустоту еще раз\n",
    "        if data_ml_comp.empty:\n",
    "            print(f\"    ОШИБКА: data_ml_comp пуст после dropna!\")\n",
    "            return np.full(n_steps, np.nan)\n",
    "\n",
    "        # Теперь строка с возможной ошибкой:\n",
    "        X_train_comp = data_ml_comp[feature_order_ml]\n",
    "        y_train_comp_aligned = data_ml_comp['t']\n",
    "\n",
    "        X_train_comp = data_ml_comp[feature_order_ml]\n",
    "        y_train_comp_aligned = data_ml_comp['t']\n",
    "\n",
    "        if X_train_comp.empty or len(X_train_comp) < tscv_ml.get_n_splits() + 1: # Проверка для CV\n",
    "            print(f\"    Ошибка {model_name}: Недостаточно данных ({len(X_train_comp)}) для GridSearchCV.\")\n",
    "            return np.full(n_steps, np.nan)\n",
    "\n",
    "        # GridSearchCV\n",
    "        base_model = model_class(random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        gs = GridSearchCV(estimator=base_model,\n",
    "                          param_grid=model_params_grid,\n",
    "                          cv=tscv_ml, # Используем переданный tscv\n",
    "                          scoring='neg_root_mean_squared_error',\n",
    "                          n_jobs=-1,\n",
    "                          refit=True,\n",
    "                          verbose=0) # verbose=0 для краткости в цикле\n",
    "        gs.fit(X_train_comp, y_train_comp_aligned)\n",
    "        best_model = gs.best_estimator_\n",
    "        print(f\"    {model_name} обучен. Лучшие параметры: {gs.best_params_}. Лучший CV RMSE: {-gs.best_score_:.4f}\")\n",
    "\n",
    "        # Рекурсивный прогноз с лучшей моделью\n",
    "        print(f\"    Генерация рекурсивного прогноза {model_name}...\")\n",
    "        forecast = recursive_predict(best_model, y_train_comp, n_steps,\n",
    "                                     feature_calculator, feature_order_ml) # Передаем порядок\n",
    "        print(f\"    Рекурсивный прогноз {model_name} выполнен.\")\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Печатаем traceback для отладки\n",
    "        return np.full(n_steps, np.nan)\n",
    "\n",
    "def train_predict_hybrid(y_train_comp, n_steps, arima_order, xgb_params_grid,\n",
    "                         feature_calculator, feature_order_hybrid, tscv_hybrid):\n",
    "    \"\"\"Обучает гибрид ARIMA+XGB_на_ошибках (с CV для XGB) и делает рекурсивный прогноз.\"\"\"\n",
    "    print(f\"    Подготовка данных и GridSearchCV для Hybrid (XGB на ошибках)...\")\n",
    "    try:\n",
    "        # 1. Обучаем ARIMA (нужен порядок)\n",
    "        if arima_order is None: raise ValueError(\"Порядок ARIMA для гибрида не определен.\")\n",
    "        print(f\"      Обучение базовой ARIMA {arima_order}...\")\n",
    "        arima_model = StatsmodelsARIMA(y_train_comp, order=arima_order,\n",
    "                                       enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "        arima_fitted = arima_model.fittedvalues\n",
    "        first_valid_idx = arima_fitted.first_valid_index()\n",
    "        if first_valid_idx is None: raise ValueError(\"ARIMA fit failed in hybrid\")\n",
    "        y_train_aligned = y_train_comp.loc[first_valid_idx:]\n",
    "        arima_fitted_aligned = arima_fitted.loc[first_valid_idx:]\n",
    "        arima_errors = y_train_aligned - arima_fitted_aligned\n",
    "        print(f\"      Базовая ARIMA обучена, ошибки рассчитаны.\")\n",
    "\n",
    "        # 2. Готовим признаки для XGBoost\n",
    "        # ... (код генерации признаков data_ml_comp как в train_predict_ml) ...\n",
    "        data_ml_comp = pd.DataFrame({'t': y_train_comp})\n",
    "        for lag in LAGS_TO_CREATE: data_ml_comp[f't-{lag}'] = data_ml_comp['t'].shift(lag)\n",
    "        target_series = data_ml_comp['t']\n",
    "        for window in ROLLING_WINDOWS:\n",
    "            data_ml_comp[f't_mean_lag{window}'] = target_series.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "            data_ml_comp[f't_std_lag{window}'] = target_series.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        month_num = data_ml_comp.index.month\n",
    "        data_ml_comp['month_sin'] = np.sin(2 * np.pi * month_num / 12)\n",
    "        data_ml_comp['month_cos'] = np.cos(2 * np.pi * month_num / 12)\n",
    "        # Выравниваем признаки с ошибками\n",
    "        X_train_xgb_aligned = data_ml_comp.loc[arima_errors.index, feature_order_hybrid].dropna()\n",
    "        arima_errors_aligned = arima_errors.loc[X_train_xgb_aligned.index]\n",
    "\n",
    "        if X_train_xgb_aligned.empty or len(X_train_xgb_aligned) < tscv_hybrid.get_n_splits() + 1:\n",
    "             print(f\"      Ошибка Гибрид: Недостаточно данных ({len(X_train_xgb_aligned)}) для GridSearchCV XGB.\")\n",
    "             return np.full(n_steps, np.nan)\n",
    "        print(f\"      Признаки для XGB на ошибках готовы ({X_train_xgb_aligned.shape}).\")\n",
    "\n",
    "        # 3. GridSearchCV для XGBoost на ошибках\n",
    "        xgb_error_base = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        gs_xgb_err = GridSearchCV(estimator=xgb_error_base,\n",
    "                                 param_grid=xgb_params_grid, # Используем сетку для XGB\n",
    "                                 cv=tscv_hybrid,\n",
    "                                 scoring='neg_root_mean_squared_error',\n",
    "                                 n_jobs=-1, refit=True, verbose=0)\n",
    "        gs_xgb_err.fit(X_train_xgb_aligned, arima_errors_aligned)\n",
    "        best_xgb_error_model = gs_xgb_err.best_estimator_\n",
    "        print(f\"      XGB на ошибках обучен. Лучшие параметры: {gs_xgb_err.best_params_}. Лучший CV RMSE: {-gs_xgb_err.best_score_:.4f}\")\n",
    "\n",
    "        # 4. Рекурсивный прогноз\n",
    "        print(\"      Генерация рекурсивного прогноза гибрида...\")\n",
    "        # Прогноз ARIMA (делаем снова, т.к. history нужна чистая)\n",
    "        history_arima = list(y_train_comp)\n",
    "        arima_forecast_rec = []\n",
    "        for _ in range(n_steps):\n",
    "            model_rec = StatsmodelsARIMA(history_arima, order=arima_order,\n",
    "                                        enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "            next_pred_a = model_rec.forecast(steps=1)[0]\n",
    "            arima_forecast_rec.append(next_pred_a)\n",
    "            history_arima.append(next_pred_a) # Добавляем прогноз ARIMA в историю для ARIMA\n",
    "        arima_forecast_rec = np.array(arima_forecast_rec)\n",
    "        print(\"        Прогноз ARIMA для гибрида готов.\")\n",
    "\n",
    "        # Рекурсивный прогноз ошибок XGBoost (использует историю реальных данных + прогнозы ошибок)\n",
    "        # Важно: recursive_predict для ошибок должен использовать историю y_train_comp для расчета признаков!\n",
    "        xgb_error_forecast_rec = recursive_predict(best_xgb_error_model, y_train_comp, n_steps,\n",
    "                                                  feature_calculator, feature_order_hybrid)\n",
    "        print(\"        Прогноз ошибок XGB для гибрида готов.\")\n",
    "\n",
    "        # Комбинируем\n",
    "        final_forecast = arima_forecast_rec + xgb_error_forecast_rec\n",
    "        print(\"    Рекурсивный прогноз Гибрида выполнен.\")\n",
    "        return final_forecast\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка Гибрид: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return np.full(n_steps, np.nan)\n",
    "\n",
    "\n",
    "# --- 5. Функция для оценки моделей и выбора лучшей (ОБНОВЛЕННАЯ) ---\n",
    "def evaluate_component_models_cv(y_train_comp, y_test_comp, component_name, horizons, weights_rmse,\n",
    "                                 rf_param_grid, xgb_param_grid, tscv_comp):\n",
    "    \"\"\"Обучает все модели для компоненты (с CV для ML), оценивает и выбирает лучшую.\"\"\"\n",
    "    model_forecasts = {}\n",
    "    model_rmses = pd.DataFrame(columns=['model', 'horizon', 'rmse'])\n",
    "    best_params = {} # Сохраняем лучшие параметры\n",
    "\n",
    "    # ARIMA\n",
    "    print(\"  Модель: ARIMA\")\n",
    "    forecast_arima, order_arima = train_predict_arima(y_train_comp, len(y_test_comp))\n",
    "    model_forecasts['ARIMA'] = forecast_arima\n",
    "    best_params['ARIMA'] = {'order': order_arima} # Сохраняем порядок\n",
    "    if order_arima is not None:\n",
    "        for h in horizons:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_arima[:h]))\n",
    "            model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'ARIMA', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # XGBoost\n",
    "    print(\"\\n  Модель: XGBoost\")\n",
    "    forecast_xgb = train_predict_ml(xgb.XGBRegressor, xgb_param_grid, y_train_comp, len(y_test_comp),\n",
    "                                    calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp)\n",
    "    model_forecasts['XGBoost'] = forecast_xgb\n",
    "    # Не сохраняем параметры XGBoost из этой функции, т.к. best_params из CV есть внутри\n",
    "    for h in horizons:\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_xgb[:h]))\n",
    "        model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'XGBoost', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # RandomForest\n",
    "    print(\"\\n  Модель: RandomForest\")\n",
    "    forecast_rf = train_predict_ml(RandomForestRegressor, rf_param_grid, y_train_comp, len(y_test_comp),\n",
    "                                   calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp)\n",
    "    model_forecasts['RandomForest'] = forecast_rf\n",
    "    for h in horizons:\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_rf[:h]))\n",
    "        model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'RandomForest', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # Гибрид ARIMA+XGB_err_Rec\n",
    "    print(\"\\n  Модель: ARIMA+XGB_err_Rec\")\n",
    "    if order_arima is not None: # Гибрид возможен только если ARIMA обучилась\n",
    "        forecast_hybrid = train_predict_hybrid(y_train_comp, len(y_test_comp), order_arima, xgb_param_grid,\n",
    "                                              calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp)\n",
    "        model_forecasts['ARIMA+XGB_err_Rec'] = forecast_hybrid\n",
    "        for h in horizons:\n",
    "             rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_hybrid[:h]))\n",
    "             model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'ARIMA+XGB_err_Rec', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "    else:\n",
    "         print(\"    Пропуск Гибрида, т.к. ARIMA не обучилась.\")\n",
    "         model_forecasts['ARIMA+XGB_err_Rec'] = np.full(len(y_test_comp), np.nan)\n",
    "\n",
    "\n",
    "    # Выбор лучшей модели\n",
    "    print(\"\\n  Расчет взвешенного RMSE для выбора лучшей модели...\")\n",
    "    weighted_rmses = {}\n",
    "    models_evaluated = model_rmses['model'].unique()\n",
    "    for model_name in models_evaluated:\n",
    "        rmses = model_rmses[(model_rmses['model'] == model_name) & (model_rmses['horizon'].isin(weights_rmse.keys()))].set_index('horizon')['rmse']\n",
    "        common_horizons = rmses.index.intersection(weights_rmse.keys())\n",
    "        if not common_horizons.empty and not rmses.loc[common_horizons].isnull().any(): # Проверка на NaN\n",
    "            weighted_avg_rmse = (rmses.loc[common_horizons] * weights_rmse.loc[common_horizons]).sum()\n",
    "            weighted_rmses[model_name] = weighted_avg_rmse\n",
    "            print(f\"    {model_name}: Weighted RMSE = {weighted_avg_rmse:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {model_name}: Недостаточно RMSE или есть NaN для расчета взвешенного среднего.\")\n",
    "            weighted_rmses[model_name] = np.inf # Ставим бесконечность, чтобы не выбрать\n",
    "\n",
    "    if not weighted_rmses or all(np.isinf(v) for v in weighted_rmses.values()):\n",
    "        best_model_name = \"ARIMA\" # Запасной вариант - ARIMA\n",
    "        print(f\"ПРЕДУПРЕЖДЕНИЕ: Не удалось выбрать лучшую модель для {component_name}. Выбрана ARIMA по умолчанию.\")\n",
    "    else:\n",
    "        best_model_name = min(weighted_rmses, key=weighted_rmses.get)\n",
    "\n",
    "    print(f\"\\n  Лучшая модель для компоненты '{component_name}': {best_model_name}\")\n",
    "\n",
    "    return model_forecasts, best_model_name, model_rmses, best_params # Возвращаем параметры\n",
    "\n",
    "print(\"Вспомогательные функции определены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1005,
     "status": "ok",
     "timestamp": 1745845405152,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "y8RFQ-93yR7C",
    "outputId": "38e4899d-033e-4ee5-db84-1daa63f91e1f"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 10: Создание Признаков для Мета-моделей на ARIMA OOF\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_oof_comp_arima_clean: DataFrame с OOF ARIMA прогнозами (чистый)\n",
    "# train_values_aligned_meta_arima: Series с таргетом (агрегат), выровненным с OOF ARIMA\n",
    "# df_forecasts_arima_comp: DataFrame с ARIMA прогнозами компонент на тесте\n",
    "# components_to_model: list\n",
    "# create_meta_features: function (должна быть определена в Ячейке 7)\n",
    "# META_FEATURE_ORDER: list - порядок признаков для мета-моделей (определен в Ячейке 7)\n",
    "# LAGS_TO_CREATE, ROLLING_WINDOWS: list (определены в Ячейке 2 или 7)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# --- 1. Создание признаков для ОБУЧАЮЩЕЙ выборки ---\n",
    "print(\"Создание признаков для ОБУЧАЮЩЕЙ выборки (на ARIMA OOF)...\")\n",
    "try:\n",
    "    # Применяем функцию к OOF прогнозам ARIMA\n",
    "    X_train_meta_arima_eng = create_meta_features(\n",
    "        df_input=df_oof_comp_arima_clean,\n",
    "        component_names=COMPONENTS_TO_MODEL,\n",
    "        lags=LAGS_TO_CREATE,\n",
    "        windows=ROLLING_WINDOWS,\n",
    "        feature_order_meta=META_FEATURE_ORDER # Передаем ожидаемый порядок\n",
    "    )\n",
    "    # Обработка NaN после создания признаков\n",
    "    X_train_meta_arima_eng_clean = X_train_meta_arima_eng.dropna()\n",
    "\n",
    "    # Выравниваем таргет еще раз под очищенные признаки\n",
    "    y_train_meta_arima_aligned = train_values_aligned_meta_arima.loc[X_train_meta_arima_eng_clean.index]\n",
    "\n",
    "    print(f\"Размер обучающей выборки с признаками (ARIMA OOF): {X_train_meta_arima_eng_clean.shape}\")\n",
    "    print(f\"Размер выровненного таргета: {y_train_meta_arima_aligned.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при создании обучающих признаков для мета-модели: {e}\")\n",
    "    # Установим переменные в None, чтобы предотвратить ошибки далее\n",
    "    X_train_meta_arima_eng_clean = None\n",
    "    y_train_meta_arima_aligned = None\n",
    "\n",
    "# --- 2. Создание признаков для ТЕСТОВОЙ выборки ---\n",
    "# Выполняем только если обучающие признаки созданы успешно\n",
    "if X_train_meta_arima_eng_clean is not None:\n",
    "    print(\"\\nСоздание признаков для ТЕСТОВОЙ выборки (на ARIMA прогнозах теста)...\")\n",
    "    try:\n",
    "        # Определяем, сколько истории нужно из OOF\n",
    "        max_lookback_arima = max(max(LAGS_TO_CREATE, default=0), max(ROLLING_WINDOWS, default=0))\n",
    "\n",
    "        # Проверяем, достаточно ли OOF данных для истории\n",
    "        if len(df_oof_comp_arima_clean) >= max_lookback_arima:\n",
    "            history_for_test_arima = df_oof_comp_arima_clean.iloc[-max_lookback_arima:]\n",
    "        else:\n",
    "            # Если OOF не хватает, берем всю доступную OOF историю\n",
    "            print(f\"Предупреждение: Недостаточно OOF истории ({len(df_oof_comp_arima_clean)}) для lookback ({max_lookback_arima}). Используется вся доступная OOF история.\")\n",
    "            history_for_test_arima = df_oof_comp_arima_clean.copy()\n",
    "\n",
    "        # Объединяем историю OOF и прогнозы ARIMA на тесте\n",
    "        # Важно: индексы должны быть совместимы (DatetimeIndex)\n",
    "        df_for_test_features_arima = pd.concat([history_for_test_arima, df_forecasts_arima_comp])\n",
    "\n",
    "        # Генерируем признаки\n",
    "        X_test_meta_arima_eng_full = create_meta_features(\n",
    "            df_input=df_for_test_features_arima,\n",
    "            component_names=COMPONENTS_TO_MODEL,\n",
    "            lags=LAGS_TO_CREATE,\n",
    "            windows=ROLLING_WINDOWS,\n",
    "            feature_order_meta=META_FEATURE_ORDER # Передаем ожидаемый порядок\n",
    "        )\n",
    "        # Оставляем только тестовый период\n",
    "        X_test_meta_arima_eng = X_test_meta_arima_eng_full.loc[df_forecasts_arima_comp.index]\n",
    "\n",
    "        # Проверка и обработка NaN в тесте\n",
    "        if X_test_meta_arima_eng.isnull().any().any():\n",
    "            print(\"ВНИМАНИЕ: Обнаружены NaN в тестовых признаках (ARIMA OOF). Заполняем медианой трейна...\")\n",
    "            for col in X_test_meta_arima_eng.columns:\n",
    "                if X_test_meta_arima_eng[col].isnull().any():\n",
    "                    median_val = X_train_meta_arima_eng_clean[col].median()\n",
    "                    X_test_meta_arima_eng[col] = X_test_meta_arima_eng[col].fillna(median_val)\n",
    "            print(f\"Кол-во NaN после заполнения: {X_test_meta_arima_eng.isnull().sum().sum()}\")\n",
    "        else:\n",
    "            print(\"NaN в тестовых признаках (ARIMA OOF) не обнаружены.\")\n",
    "\n",
    "        print(\"\\n--- Признаки для мета-моделей на ARIMA OOF готовы ---\")\n",
    "        print(\"Созданы: X_train_meta_arima_eng_clean, y_train_meta_arima_aligned, X_test_meta_arima_eng\")\n",
    "        # Выведем размерности для контроля\n",
    "        print(f\"Размер X_train_meta_arima_eng_clean: {X_train_meta_arima_eng_clean.shape}\")\n",
    "        print(f\"Размер y_train_meta_arima_aligned: {y_train_meta_arima_aligned.shape}\")\n",
    "        print(f\"Размер X_test_meta_arima_eng: {X_test_meta_arima_eng.shape}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при создании тестовых признаков для мета-модели: {e}\")\n",
    "        X_test_meta_arima_eng = None # Устанавливаем в None в случае ошибки\n",
    "else:\n",
    "     print(\"Пропуск создания тестовых признаков из-за ошибки на этапе создания обучающих.\")\n",
    "     X_test_meta_arima_eng = None\n",
    "\n",
    "print(\"\\n--- Ячейка 10 завершена ---\")\n",
    "\n",
    "print(\"Колонки X_train_meta_arima_eng:\", X_train_meta_arima_eng.columns.tolist())\n",
    "print(\"Количество колонок:\", len(X_train_meta_arima_eng.columns))\n",
    "print(\"Размер до dropna:\", X_train_meta_arima_eng.shape)\n",
    "# Сравни этот вывод с тем, что было в старом ноутбуке (должно быть 35 признаков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124595,
     "status": "ok",
     "timestamp": 1745845585548,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "LeRxDLWTvtBq",
    "outputId": "5b510cbf-dbc3-47fb-b298-0159e5e28f45"
   },
   "outputs": [],
   "source": [
    "# print(\"\\nЯчейка 11: Обучение и Оценка ВСЕХ Мета-моделей на ARIMA OOF (с Hyperopt)\") # Изменено\n",
    "\n",
    "# --- Импорты для Hyperopt ---\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from sklearn.preprocessing import StandardScaler # Используем StandardScaler для линейных\n",
    "from sklearn.linear_model import Ridge, Lasso # RidgeCV и LassoCV убраны\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "# Убедимся, что TimeSeriesSplit определен\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# X_train_meta_arima_eng_clean: DataFrame с признаками из OOF ARIMA (трейн)\n",
    "# y_train_meta_arima_aligned: Series с таргетом (агрегат ИПЦ на трейне, выровненный с OOF)\n",
    "# X_test_meta_arima_eng: DataFrame с признаками из прогнозов ARIMA (тест)\n",
    "# test_values: Series с фактом агрегированного ИПЦ на тесте (для оценки)\n",
    "# N_CV_SPLITS: int (для TimeSeriesSplit внутри objective)\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# aggregation_rmse, all_horizon_rmse, final_forecasts - глобальные структуры для результатов\n",
    "# RANDOM_SEED\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Переменные для хранения прогнозов этой ячейки\n",
    "meta_forecasts_on_arima = {}\n",
    "MAX_EVALS_META = 50 # Общее количество итераций для каждой мета-модели (можно настроить)\n",
    "LASSO_MAX_ITER = 10000\n",
    "\n",
    "# --- Масштабирование Признаков (для Ridge/Lasso) ---\n",
    "print(\"Масштабирование мета-признаков для Ridge/Lasso...\")\n",
    "scaler_meta_arima = StandardScaler() \n",
    "X_train_meta_arima_scaled = scaler_meta_arima.fit_transform(X_train_meta_arima_eng_clean)\n",
    "X_test_meta_arima_scaled = scaler_meta_arima.transform(X_test_meta_arima_eng)\n",
    "print(\"Масштабирование выполнено.\")\n",
    "\n",
    "# Инициализируем TS Splitter для CV мета-моделей\n",
    "# Важно: tscv_meta должен быть определен и соответствовать данным X_train_meta_arima_eng_clean\n",
    "# Если N_CV_SPLITS - это просто число, то инициализируем здесь\n",
    "tscv_meta = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "\n",
    "# --- Общая функция Objective для Hyperopt (для всех мета-моделей) ---\n",
    "def objective_meta_model(params, model_type, X_train_data, y_train_data, cv_splitter, is_scaled_data=False):\n",
    "    \"\"\"\n",
    "    Универсальная целевая функция для hyperopt для мета-моделей.\n",
    "    model_type: 'ridge', 'lasso', 'rf', 'xgb'\n",
    "    X_train_data: обучающие признаки (могут быть масштабированными или нет)\n",
    "    is_scaled_data: флаг, указывающий, переданы ли уже масштабированные данные\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    \n",
    "    if model_type == 'ridge':\n",
    "        model = Ridge(alpha=params['alpha'])\n",
    "    elif model_type == 'lasso':\n",
    "        params_copy = params.copy() # Копируем, чтобы не менять оригинальный params\n",
    "        # Lasso может требовать другие параметры из space, не только alpha\n",
    "        model = Lasso(alpha=params_copy.pop('alpha', 0.01), # значение по умолчанию, если alpha не в params\n",
    "                      max_iter=params_copy.pop('max_iter', 10000), \n",
    "                      tol=params_copy.pop('tol', 0.001),\n",
    "                      random_state=RANDOM_SEED, **params_copy)\n",
    "    elif model_type == 'rf':\n",
    "        # Преобразование параметров для RF\n",
    "        rf_params = {k.replace('meta_rf_', ''): v for k, v in params.items()}\n",
    "        rf_params['n_estimators'] = int(rf_params['n_estimators'])\n",
    "        rf_params['min_samples_split'] = int(rf_params['min_samples_split'])\n",
    "        rf_params['min_samples_leaf'] = int(rf_params['min_samples_leaf'])\n",
    "        # max_depth и max_features остаются как есть (могут быть None или строкой)\n",
    "        model = RandomForestRegressor(**rf_params, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    elif model_type == 'xgb':\n",
    "        xgb_params = {k.replace('meta_xgb_', ''): v for k, v in params.items()}\n",
    "        xgb_params['n_estimators'] = int(xgb_params['n_estimators'])\n",
    "        xgb_params['max_depth'] = int(xgb_params['max_depth'])\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1, **xgb_params)\n",
    "    else:\n",
    "        raise ValueError(\"Неизвестный тип модели\")\n",
    "\n",
    "    rmses = []\n",
    "    try:\n",
    "        # X_train_data может быть DataFrame или NumPy array\n",
    "        # y_train_data должна быть Series или NumPy array\n",
    "        X_data_for_split = X_train_data\n",
    "        if isinstance(X_train_data, pd.DataFrame):\n",
    "            X_iloc_available = True\n",
    "        else: # NumPy array\n",
    "            X_iloc_available = False\n",
    "            \n",
    "        y_data_for_split = y_train_data\n",
    "        if isinstance(y_train_data, pd.Series):\n",
    "            y_iloc_available = True\n",
    "        else: # NumPy array\n",
    "            y_iloc_available = False\n",
    "\n",
    "\n",
    "        for train_idx, val_idx in cv_splitter.split(X_data_for_split):\n",
    "            if X_iloc_available:\n",
    "                X_fold_train, X_fold_val = X_data_for_split.iloc[train_idx], X_data_for_split.iloc[val_idx]\n",
    "            else:\n",
    "                X_fold_train, X_fold_val = X_data_for_split[train_idx], X_data_for_split[val_idx]\n",
    "            \n",
    "            if y_iloc_available:\n",
    "                y_fold_train, y_fold_val = y_data_for_split.iloc[train_idx], y_data_for_split.iloc[val_idx]\n",
    "            else:\n",
    "                y_fold_train, y_fold_val = y_data_for_split[train_idx], y_data_for_split[val_idx]\n",
    "\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            preds = model.predict(X_fold_val)\n",
    "\n",
    "            if not np.all(np.isfinite(preds)) or not np.all(np.isfinite(y_fold_val if isinstance(y_fold_val, np.ndarray) else y_fold_val.values)):\n",
    "                continue\n",
    "            if len(y_fold_val) == 0:\n",
    "                continue\n",
    "            rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "            rmses.append(rmse)\n",
    "        \n",
    "        if not rmses: avg_rmse = 1e10\n",
    "        else: avg_rmse = np.mean(rmses)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error in objective for {model_type} with params {params}: {e}\") # Отладка\n",
    "        avg_rmse = 1e10\n",
    "    \n",
    "    if not np.isfinite(avg_rmse): avg_rmse = 1e10\n",
    "    return {'loss': avg_rmse, 'status': STATUS_OK}\n",
    "\n",
    "# --- Настройка и запуск Hyperopt для каждой мета-модели ---\n",
    "\n",
    "meta_models_config = {\n",
    "    \"Meta-Ridge_on_ARIMA\": {\n",
    "        \"model_type\": \"ridge\",\n",
    "        \"space\": {'alpha': hp.loguniform('meta_ridge_alpha', np.log(1e-4), np.log(1e3))},\n",
    "        \"X_train\": X_train_meta_arima_scaled, # Используем масштабированные\n",
    "        \"X_test\": X_test_meta_arima_scaled,\n",
    "        \"is_scaled\": True\n",
    "    },\n",
    "    \"Meta-Lasso_on_ARIMA\": {\n",
    "        \"model_type\": \"lasso\",\n",
    "        \"space\": {\n",
    "            'alpha': hp.loguniform('meta_lasso_alpha', np.log(1e-5), np.log(1.0)),\n",
    "            'max_iter': LASSO_MAX_ITER, # Фиксированный параметр\n",
    "            'tol': 0.001                 # Фиксированный параметр\n",
    "            },\n",
    "        \"X_train\": X_train_meta_arima_scaled, # Используем масштабированные\n",
    "        \"X_test\": X_test_meta_arima_scaled,\n",
    "        \"is_scaled\": True\n",
    "    },\n",
    "    \"Meta-RF_on_ARIMA\": {\n",
    "        \"model_type\": \"rf\",\n",
    "        \"space\": {\n",
    "            'meta_rf_n_estimators': hp.quniform('meta_rf_n_estimators', 50, 200, 25),\n",
    "            'meta_rf_max_depth': hp.choice('meta_rf_max_depth', [3, 5, 7, 10, None]),\n",
    "            'meta_rf_max_features': hp.choice('meta_rf_max_features', ['sqrt', 0.5, 0.7, 0.9]),\n",
    "            'meta_rf_min_samples_split': hp.quniform('meta_rf_min_samples_split', 2, 10, 1),\n",
    "            'meta_rf_min_samples_leaf': hp.quniform('meta_rf_min_samples_leaf', 1, 8, 1)\n",
    "        },\n",
    "        \"X_train\": X_train_meta_arima_eng_clean, # НЕмасштабированные\n",
    "        \"X_test\": X_test_meta_arima_eng,\n",
    "        \"is_scaled\": False\n",
    "    },\n",
    "    \"Meta-XGB_on_ARIMA\": {\n",
    "        \"model_type\": \"xgb\",\n",
    "        \"space\": {\n",
    "            'meta_xgb_n_estimators': hp.quniform('meta_xgb_n_estimators', 50, 250, 25),\n",
    "            'meta_xgb_learning_rate': hp.loguniform('meta_xgb_learning_rate', np.log(0.01), np.log(0.2)),\n",
    "            'meta_xgb_max_depth': hp.quniform('meta_xgb_max_depth', 1, 5, 1),\n",
    "            'meta_xgb_subsample': hp.uniform('meta_xgb_subsample', 0.6, 1.0),\n",
    "            'meta_xgb_colsample_bytree': hp.uniform('meta_xgb_colsample_bytree', 0.6, 1.0),\n",
    "            'meta_xgb_gamma': hp.uniform('meta_xgb_gamma', 0.0, 0.5),\n",
    "            # Можно добавить reg_alpha и reg_lambda, если нужно\n",
    "            # 'meta_xgb_reg_alpha': hp.loguniform('meta_xgb_reg_alpha', np.log(0.001), np.log(1.0)),\n",
    "            # 'meta_xgb_reg_lambda': hp.loguniform('meta_xgb_reg_lambda', np.log(0.1), np.log(10.0))\n",
    "        },\n",
    "        \"X_train\": X_train_meta_arima_eng_clean, # НЕмасштабированные\n",
    "        \"X_test\": X_test_meta_arima_eng,\n",
    "        \"is_scaled\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Цикл по конфигурациям мета-моделей\n",
    "for model_name, config in meta_models_config.items():\n",
    "    print(f\"\\n--- Обучение и оценка с Hyperopt: {model_name} ---\")\n",
    "    \n",
    "    trials = Trials()\n",
    "    try:\n",
    "        rstate_meta = np.random.default_rng(RANDOM_SEED)\n",
    "    except AttributeError:\n",
    "        rstate_meta = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "    # Адаптируем objective для передачи нужных X_train и y_train\n",
    "    # y_train_meta_arima_aligned передается для всех мета-моделей\n",
    "    objective_fn_with_data = lambda params: objective_meta_model(\n",
    "        params, \n",
    "        config[\"model_type\"], \n",
    "        config[\"X_train\"], \n",
    "        y_train_meta_arima_aligned, # Таргет для всех мета-моделей один\n",
    "        tscv_meta, \n",
    "        config[\"is_scaled\"]\n",
    "    )\n",
    "\n",
    "    best_params_raw = fmin(\n",
    "        fn=objective_fn_with_data,\n",
    "        space=config[\"space\"],\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=MAX_EVALS_META,\n",
    "        trials=trials,\n",
    "        rstate=rstate_meta,\n",
    "        show_progressbar=True\n",
    "    )\n",
    "    \n",
    "    best_final_params = space_eval(config[\"space\"], best_params_raw)\n",
    "    # Приведение типов для финальной модели\n",
    "    if config[\"model_type\"] == 'rf' or config[\"model_type\"] == 'xgb':\n",
    "        best_final_params[f'meta_{config[\"model_type\"]}_n_estimators'] = int(best_final_params[f'meta_{config[\"model_type\"]}_n_estimators'])\n",
    "        if f'meta_{config[\"model_type\"]}_max_depth' in best_final_params and best_final_params[f'meta_{config[\"model_type\"]}_max_depth'] is not None :\n",
    "             best_final_params[f'meta_{config[\"model_type\"]}_max_depth'] = int(best_final_params[f'meta_{config[\"model_type\"]}_max_depth'])\n",
    "        if config[\"model_type\"] == 'rf':\n",
    "            best_final_params[f'meta_rf_min_samples_split'] = int(best_final_params[f'meta_rf_min_samples_split'])\n",
    "            best_final_params[f'meta_rf_min_samples_leaf'] = int(best_final_params[f'meta_rf_min_samples_leaf'])\n",
    "\n",
    "\n",
    "    best_cv_rmse = trials.best_trial['result']['loss']\n",
    "    print(f\"  Лучшие найденные параметры: {best_final_params}\")\n",
    "    print(f\"  Лучший RMSE на кросс-валидации: {best_cv_rmse:.4f}\")\n",
    "\n",
    "    # Обучение финальной мета-модели\n",
    "    final_model = None\n",
    "    clean_params_for_fit = {k.replace(f'meta_{config[\"model_type\"]}_', ''): v for k,v in best_final_params.items()}\n",
    "\n",
    "    if config[\"model_type\"] == 'ridge':\n",
    "        final_model = Ridge(**clean_params_for_fit)\n",
    "        final_model.fit(config[\"X_train\"], y_train_meta_arima_aligned)\n",
    "    elif config[\"model_type\"] == 'lasso':\n",
    "        # Убедимся, что max_iter и tol передаются, если они не в space\n",
    "        lasso_fit_params = clean_params_for_fit.copy()\n",
    "        lasso_fit_params.setdefault('max_iter', LASSO_MAX_ITER)\n",
    "        lasso_fit_params.setdefault('tol', 0.001)\n",
    "        final_model = Lasso(random_state=RANDOM_SEED, **lasso_fit_params)\n",
    "        final_model.fit(config[\"X_train\"], y_train_meta_arima_aligned)\n",
    "        n_features_selected = np.sum(final_model.coef_ != 0)\n",
    "        print(f\"  Количество отобранных признаков: {n_features_selected} из {config['X_train'].shape[1] if isinstance(config['X_train'], pd.DataFrame) else config['X_train'].shape[1]}\")\n",
    "    elif config[\"model_type\"] == 'rf':\n",
    "        final_model = RandomForestRegressor(**clean_params_for_fit, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        final_model.fit(config[\"X_train\"], y_train_meta_arima_aligned)\n",
    "    elif config[\"model_type\"] == 'xgb':\n",
    "        final_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1, **clean_params_for_fit)\n",
    "        final_model.fit(config[\"X_train\"], y_train_meta_arima_aligned)\n",
    "\n",
    "    # Прогноз и Оценка\n",
    "    if final_model:\n",
    "        y_pred = final_model.predict(config[\"X_test\"])\n",
    "        meta_forecasts_on_arima[model_name] = y_pred\n",
    "        final_forecasts[model_name] = y_pred # Сохраняем в общий словарь\n",
    "\n",
    "        overall_rmse = np.sqrt(mean_squared_error(test_values, y_pred))\n",
    "        print(f\"  Общий RMSE: {overall_rmse:.4f}\")\n",
    "        aggregation_rmse[model_name] = overall_rmse\n",
    "\n",
    "        print(\"  RMSE по горизонтам:\")\n",
    "        horizon_results_list = []\n",
    "        for h_eval in HORIZONS_TO_EVALUATE: # Переименовал h -> h_eval\n",
    "            rmse_h_val = np.sqrt(mean_squared_error(test_values[:h_eval], y_pred[:h_eval])) # Переименовал rmse_h -> rmse_h_val\n",
    "            print(f\"    1-{h_eval} мес.: {rmse_h_val:.4f}\")\n",
    "            horizon_results_list.append({'model': model_name, 'horizon': h_eval, 'rmse': rmse_h_val})\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"  Не удалось обучить финальную модель {model_name}\")\n",
    "        aggregation_rmse[model_name] = np.nan\n",
    "\n",
    "\n",
    "# --- Вывод обновленных таблиц ---\n",
    "# ... (твой код для вывода aggregation_rmse и all_horizon_rmse) ...\n",
    "\n",
    "print(\"\\n--- Ячейка 11 (Hyperopt Meta-models) завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1076044,
     "status": "ok",
     "timestamp": 1745846661591,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "UPweprzhIDi0",
    "outputId": "09b2e593-4a47-44ae-e55e-85ab4fc859c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA as StatsmodelsARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import traceback # Уже используется\n",
    "\n",
    "# НОВЫЕ ИМПОРТЫ для HYPEROPT\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope # Для целочисленных параметров hp.quniform\n",
    "import hyperopt\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМЫЕ ГЛОБАЛЬНЫЕ КОНСТАНТЫ (определите их значения) ---\n",
    "# COMPONENTS_TO_MODEL = ['C1', 'C2'] # Пример\n",
    "# LAGS_TO_CREATE = [1, 2, 3, 6]\n",
    "# ROLLING_WINDOWS = [3, 6]\n",
    "# COMP_FEATURE_ORDER = [...] # Определите на основе Ячейки 2\n",
    "# META_FEATURE_ORDER = [...] # Определите на основе Ячейки 7\n",
    "# RANDOM_SEED = 42\n",
    "# N_CV_SPLITS = 3\n",
    "# MAX_EVALS_HYPEROPT = 50 # Количество итераций для Hyperopt\n",
    "# HORIZONS_TO_EVALUATE = [1, 3, 6, 12]\n",
    "# TEST_MONTHS = 12\n",
    "# WEIGHTS_NORMALIZED_RMSE = pd.Series(...) # Определите веса\n",
    "# tscv_component_cv = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "tscv_component_cv = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "MAX_EVALS_HYPEROPT = 50 \n",
    "\n",
    "# --- Ячейка 7: Определение Вспомогательных Функций (Обновлено для Hyperopt) ---\n",
    "print(\"\\nЯчейка 7: Определение Вспомогательных Функций (Обновлено для Hyperopt)\")\n",
    "\n",
    "# --- 1. Функция для Feature Engineering (для мета-моделей) ---\n",
    "# Оставляем без изменений, так как она не зависит от GridSearchCV/Hyperopt\n",
    "def create_meta_features(df_input, component_names, lags, windows, feature_order_meta):\n",
    "    df = df_input.copy()\n",
    "    feature_df = pd.DataFrame(index=df.index)\n",
    "    for comp in component_names:\n",
    "        feature_df[f'{comp}_pred'] = df[comp]\n",
    "    for lag in lags:\n",
    "        for comp in component_names:\n",
    "            feature_df[f'{comp}_pred_lag{lag}'] = df[comp].shift(lag)\n",
    "    for window in windows:\n",
    "        for comp in component_names:\n",
    "            feature_df[f'{comp}_pred_roll_mean{window}'] = df[comp].rolling(window=window, min_periods=1).mean()\n",
    "            feature_df[f'{comp}_pred_roll_std{window}'] = df[comp].rolling(window=window, min_periods=1).std()\n",
    "    for comp in component_names:\n",
    "        feature_df[f'{comp}_pred_diff1'] = df[comp].diff(1)\n",
    "    try:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            feature_df['month_meta_sin'] = np.sin(2 * np.pi * df.index.month / 12)\n",
    "            feature_df['month_meta_cos'] = np.cos(2 * np.pi * df.index.month / 12)\n",
    "        else:\n",
    "            print(\"Предупреждение: Индекс не является DatetimeIndex, календарные признаки не добавлены.\")\n",
    "            feature_df['month_meta_sin'] = np.nan\n",
    "            feature_df['month_meta_cos'] = np.nan\n",
    "    except AttributeError:\n",
    "         print(\"Предупреждение: Не удалось извлечь месяц из индекса, календарные признаки не добавлены.\")\n",
    "         feature_df['month_meta_sin'] = np.nan\n",
    "         feature_df['month_meta_cos'] = np.nan\n",
    "\n",
    "    available_columns = feature_df.columns.tolist()\n",
    "    final_columns_order = [col for col in feature_order_meta if col in available_columns]\n",
    "    if len(final_columns_order) != len(feature_order_meta):\n",
    "        missing_cols = set(feature_order_meta) - set(available_columns)\n",
    "        print(f\"Предупреждение: Не все ожидаемые признаки найдены/созданы! Отсутствуют: {missing_cols}\")\n",
    "    feature_df = feature_df[final_columns_order]\n",
    "    return feature_df\n",
    "\n",
    "# --- 2. Функция для Feature Engineering (для моделей компонент) ---\n",
    "# Оставляем без изменений\n",
    "def calculate_features_for_step(history_series: pd.Series, feature_order_comp: list):\n",
    "    features = {}\n",
    "    n = len(history_series)\n",
    "    if n == 0: return None\n",
    "    try:\n",
    "        if not isinstance(history_series.index, pd.DatetimeIndex):\n",
    "            # print(\"Предупреждение: history_series.index не является DatetimeIndex в calculate_features_for_step.\")\n",
    "            return None # Или попытаться преобразовать, но безопаснее вернуть None\n",
    "        next_index = history_series.index[-1] + pd.DateOffset(months=1)\n",
    "    except (TypeError, IndexError): return None\n",
    "\n",
    "    for lag in LAGS_TO_CREATE:\n",
    "        features[f't-{lag}'] = history_series.iloc[-lag] if n >= lag else np.nan\n",
    "    for window in ROLLING_WINDOWS:\n",
    "        if n >= window:\n",
    "            window_data = history_series.iloc[-window:]\n",
    "            features[f't_mean_lag{window}'] = window_data.mean()\n",
    "            features[f't_std_lag{window}'] = window_data.std()\n",
    "        else:\n",
    "            features[f't_mean_lag{window}'] = np.nan\n",
    "            features[f't_std_lag{window}'] = np.nan\n",
    "    try:\n",
    "        next_month_num = next_index.month\n",
    "        features['month_sin'] = np.sin(2 * np.pi * next_month_num / 12)\n",
    "        features['month_cos'] = np.cos(2 * np.pi * next_month_num / 12)\n",
    "    except AttributeError:\n",
    "        features['month_sin'] = np.nan\n",
    "        features['month_cos'] = np.nan\n",
    "    features_series = pd.Series(features)\n",
    "    try:\n",
    "        return features_series.reindex(feature_order_comp)\n",
    "    except KeyError as e:\n",
    "        print(f\"Ошибка calculate_features_for_step: Несовпадение имен признаков! {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. Функция для рекурсивного прогноза ---\n",
    "# Оставляем без изменений\n",
    "def recursive_predict(model, initial_history_series: pd.Series, n_steps: int,\n",
    "                      feature_calculator, feature_order_func: list, scaler=None):\n",
    "    current_history = initial_history_series.copy()\n",
    "    predictions = []\n",
    "    for i in range(n_steps):\n",
    "        features_for_step = feature_calculator(current_history, feature_order_func)\n",
    "        if features_for_step is None or features_for_step.isnull().any():\n",
    "            # print(f\"  Предупреждение: Не удалось рассчитать признаки или есть NaN на шаге {i+1} для {model.__class__.__name__}. Заполняем NaN.\")\n",
    "            predictions.extend([np.nan] * (n_steps - i))\n",
    "            break\n",
    "        features_df = features_for_step.to_frame().T\n",
    "        if scaler:\n",
    "             try:\n",
    "                 features_scaled = scaler.transform(features_df)\n",
    "                 model_input = features_scaled\n",
    "             except Exception as scale_e:\n",
    "                 print(f\"  Ошибка масштабирования на шаге {i+1}: {scale_e}\")\n",
    "                 predictions.extend([np.nan] * (n_steps - i))\n",
    "                 break\n",
    "        else:\n",
    "             model_input = features_df\n",
    "        try:\n",
    "             next_pred = model.predict(model_input)[0]\n",
    "        except Exception as pred_e:\n",
    "             print(f\"  Ошибка предсказания модели {model.__class__.__name__} на шаге {i+1}: {pred_e}\")\n",
    "             predictions.extend([np.nan] * (n_steps - i))\n",
    "             break\n",
    "        predictions.append(next_pred)\n",
    "        try:\n",
    "            if not isinstance(current_history.index, pd.DatetimeIndex):\n",
    "                # print(\"Предупреждение: current_history.index не является DatetimeIndex в recursive_predict.\")\n",
    "                # Попытка преобразования или прерывание\n",
    "                current_history.index = pd.to_datetime(current_history.index, errors='coerce')\n",
    "                if current_history.index.isnull().any():\n",
    "                     raise ValueError(\"Ошибка преобразования индекса истории в DatetimeIndex во время рекурсии.\")\n",
    "\n",
    "            next_idx_val = current_history.index[-1] + pd.DateOffset(months=1)\n",
    "            new_data_point = pd.Series([next_pred], index=[next_idx_val], name='t')\n",
    "            current_history = pd.concat([current_history, new_data_point])\n",
    "        except (TypeError, IndexError, ValueError) as hist_e:\n",
    "             print(f\"  Ошибка обновления истории на шаге {i+1}: {hist_e}\")\n",
    "             predictions.extend([np.nan] * (n_steps - i))\n",
    "             break\n",
    "    return np.array(predictions)\n",
    "\n",
    "# --- 4. Функции для обучения и прогноза моделей компонент (обновленные для Hyperopt) ---\n",
    "\n",
    "def train_predict_arima(y_train_comp, n_steps):\n",
    "    \"\"\"Обучает auto_arima (с d=0) и делает рекурсивный прогноз.\"\"\"\n",
    "    print(f\"    Обучение ARIMA для ряда длиной {len(y_train_comp)}...\")\n",
    "    try:\n",
    "        model = auto_arima(y_train_comp, start_p=1, start_q=1, max_p=5, max_q=5,\n",
    "                           m=1, seasonal=False, d=0, test='adf',\n",
    "                           trace=False, error_action='ignore', suppress_warnings=True,\n",
    "                           stepwise=True, information_criterion='aic', n_jobs=1)\n",
    "        order = model.order\n",
    "        print(f\"    ARIMA обучена, порядок: {order}\")\n",
    "        history_arima = list(y_train_comp)\n",
    "        arima_forecast_rec = []\n",
    "        for _ in range(n_steps):\n",
    "            model_rec = StatsmodelsARIMA(history_arima, order=order,\n",
    "                                        enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "            next_pred = model_rec.forecast(steps=1)[0]\n",
    "            arima_forecast_rec.append(next_pred)\n",
    "            history_arima.append(next_pred)\n",
    "        print(\"    Рекурсивный прогноз ARIMA выполнен.\")\n",
    "        return np.array(arima_forecast_rec), order, {'order': order} # Возвращаем параметры\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка ARIMA: {e}\")\n",
    "        return np.full(n_steps, np.nan), None, {}\n",
    "\n",
    "def train_predict_ml(model_class, hyperopt_space, y_train_comp, n_steps,\n",
    "                     feature_calculator, feature_order_ml, tscv_ml, max_evals_hyperopt,\n",
    "                     component_name_log=\"ML_model\"): # Добавлен параметр для логгирования\n",
    "    \"\"\"Подбирает гиперпараметры с Hyperopt, обучает ML модель и делает рекурсивный прогноз.\"\"\"\n",
    "    model_name = model_class.__name__.replace('Regressor','')\n",
    "    print(f\"    [{component_name_log}] Подготовка данных и Hyperopt для {model_name}...\")\n",
    "\n",
    "    try:\n",
    "        data_ml_comp = pd.DataFrame({'t': y_train_comp})\n",
    "        for lag in LAGS_TO_CREATE: data_ml_comp[f't-{lag}'] = data_ml_comp['t'].shift(lag)\n",
    "        target_series = data_ml_comp['t']\n",
    "        for window in ROLLING_WINDOWS:\n",
    "            data_ml_comp[f't_mean_lag{window}'] = target_series.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "            data_ml_comp[f't_std_lag{window}'] = target_series.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        if isinstance(data_ml_comp.index, pd.DatetimeIndex):\n",
    "            month_num = data_ml_comp.index.month\n",
    "            data_ml_comp['month_sin'] = np.sin(2 * np.pi * month_num / 12)\n",
    "            data_ml_comp['month_cos'] = np.cos(2 * np.pi * month_num / 12)\n",
    "        else:\n",
    "            print(f\"    Предупреждение [{component_name_log} - {model_name}]: Индекс не DatetimeIndex. Календарные признаки будут NaN.\")\n",
    "            data_ml_comp['month_sin'] = np.nan\n",
    "            data_ml_comp['month_cos'] = np.nan\n",
    "        data_ml_comp = data_ml_comp.dropna()\n",
    "\n",
    "        if data_ml_comp.empty:\n",
    "            print(f\"    ОШИБКА [{component_name_log} - {model_name}]: data_ml_comp пуст после dropna!\")\n",
    "            return np.full(n_steps, np.nan), {}\n",
    "\n",
    "        X_train_comp = data_ml_comp[feature_order_ml]\n",
    "        y_train_comp_aligned = data_ml_comp['t']\n",
    "\n",
    "        min_samples_for_cv = 0\n",
    "        for train_idx, _ in tscv_ml.split(X_train_comp): # Подсчет минимального размера трейн выборки в CV\n",
    "            if len(train_idx) > min_samples_for_cv:\n",
    "                 min_samples_for_cv = len(train_idx)\n",
    "        if X_train_comp.empty or len(X_train_comp) < tscv_ml.get_n_splits() + min_samples_for_cv : # Условие для CV\n",
    "            print(f\"    Ошибка [{component_name_log} - {model_name}]: Недостаточно данных ({len(X_train_comp)}) для Hyperopt CV (нужно хотя бы {tscv_ml.get_n_splits() + min_samples_for_cv}).\")\n",
    "            return np.full(n_steps, np.nan), {}\n",
    "\n",
    "        # --- Hyperopt Objective Function ---\n",
    "        def objective(params):\n",
    "            # Преобразование типов для параметров, если hyperopt возвращает float для int\n",
    "            current_params = params.copy() # Работаем с копией\n",
    "            if 'n_estimators' in current_params: current_params['n_estimators'] = int(current_params['n_estimators'])\n",
    "            if 'max_depth' in current_params and current_params['max_depth'] is not None : current_params['max_depth'] = int(current_params['max_depth'])\n",
    "            if 'min_samples_leaf' in current_params: current_params['min_samples_leaf'] = int(current_params['min_samples_leaf'])\n",
    "            if 'min_samples_split' in current_params: current_params['min_samples_split'] = int(current_params['min_samples_split'])\n",
    "            # Для XGBoost специфичные параметры, например, num_leaves, если используется\n",
    "            if 'num_leaves' in current_params: current_params['num_leaves'] = int(current_params['num_leaves'])\n",
    "\n",
    "\n",
    "            model = model_class(**current_params, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "            cv_scores = []\n",
    "            for train_idx, val_idx in tscv_ml.split(X_train_comp):\n",
    "                X_cv_train, X_cv_val = X_train_comp.iloc[train_idx], X_train_comp.iloc[val_idx]\n",
    "                y_cv_train, y_cv_val = y_train_comp_aligned.iloc[train_idx], y_train_comp_aligned.iloc[val_idx]\n",
    "                if X_cv_train.empty or X_cv_val.empty or len(X_cv_train) < 2 or len(X_cv_val) < 1: # Добавил проверку на минимальный размер\n",
    "                    continue\n",
    "                try:\n",
    "                    model.fit(X_cv_train, y_cv_train)\n",
    "                    preds = model.predict(X_cv_val)\n",
    "                    rmse = np.sqrt(mean_squared_error(y_cv_val, preds))\n",
    "                    cv_scores.append(rmse)\n",
    "                except Exception as cv_e:\n",
    "                    # print(f\"      Предупреждение CV для {model_name} с параметрами {current_params}: {cv_e}\")\n",
    "                    cv_scores.append(np.inf) # Штрафуем за ошибку в CV\n",
    "\n",
    "            if not cv_scores or all(s == np.inf for s in cv_scores) : # Если все фолды пропущены или ошибки\n",
    "                return {'loss': np.inf, 'status': STATUS_OK}\n",
    "            avg_rmse = np.mean([s for s in cv_scores if s != np.inf]) # Среднее по успешным фолдам\n",
    "            if np.isnan(avg_rmse): avg_rmse = np.inf\n",
    "            return {'loss': avg_rmse, 'status': STATUS_OK}\n",
    "\n",
    "        trials = Trials()\n",
    "        best_params_from_fmin = fmin(fn=objective,\n",
    "                                space=hyperopt_space,\n",
    "                                algo=tpe.suggest,\n",
    "                                max_evals=max_evals_hyperopt,\n",
    "                                trials=trials,\n",
    "                                rstate=np.random.default_rng(RANDOM_SEED),\n",
    "                                verbose=0) # Установить verbose=1 или True для отладки fmin\n",
    "\n",
    "        # Преобразуем лучшие параметры в нужный формат (особенно для hp.choice)\n",
    "        # и целочисленных параметров из hp.quniform (fmin возвращает float)\n",
    "        final_best_params = {}\n",
    "        for key, value in best_params_from_fmin.items():\n",
    "            # Для hp.quniform, которые должны быть int, fmin вернет float.\n",
    "            # scope.int используется с hp.quniform для получения int.\n",
    "            # Если параметр определен как hp.choice с целочисленными опциями, он будет int.\n",
    "            # Для простоты, если параметр есть в списке \"должен быть int\", преобразуем\n",
    "            int_params_list = ['n_estimators', 'max_depth', 'min_samples_leaf', 'min_samples_split', 'num_leaves']\n",
    "            if key in int_params_list and value is not None:\n",
    "                final_best_params[key] = int(value)\n",
    "            else:\n",
    "                final_best_params[key] = value\n",
    "        \n",
    "        # Если в space есть hp.choice, fmin вернет индекс. Нужно получить само значение.\n",
    "        # Это делается автоматически, если правильно использовать Trials и best_trial.\n",
    "        # Однако, best_params_from_fmin уже содержит сами значения для простых случаев.\n",
    "        # Для сложных hp.choice, где опции - другие hp выражения, может потребоваться более сложная логика.\n",
    "        # Trials.best_trial['misc']['vals'] содержит индексы для hp.choice.\n",
    "        # Trials.best_trial['result']['params'] может содержать фактические параметры, если вы их возвращаете из objective.\n",
    "        # Проще всего положиться на то, что `best_params_from_fmin` содержит значения, а не индексы,\n",
    "        # для большинства случаев, или использовать `hyperopt.space_eval(hyperopt_space, best_params_from_fmin)`\n",
    "        \n",
    "        actual_best_params = hyperopt.space_eval(hyperopt_space, best_params_from_fmin)\n",
    "        # Убедимся, что целочисленные параметры действительно int\n",
    "        for p_name in ['n_estimators', 'max_depth', 'min_samples_leaf', 'min_samples_split', 'num_leaves']:\n",
    "            if p_name in actual_best_params and actual_best_params[p_name] is not None:\n",
    "                actual_best_params[p_name] = int(actual_best_params[p_name])\n",
    "\n",
    "\n",
    "        best_loss = trials.best_trial['result']['loss'] if trials.best_trial else np.inf\n",
    "        print(f\"    [{component_name_log} - {model_name}] Hyperopt завершен. Лучший CV RMSE: {best_loss:.4f}. Лучшие параметры: {actual_best_params}\")\n",
    "\n",
    "        final_model = model_class(**actual_best_params, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        final_model.fit(X_train_comp, y_train_comp_aligned)\n",
    "        print(f\"    [{component_name_log} - {model_name}] Обучен с лучшими параметрами.\")\n",
    "\n",
    "        print(f\"    [{component_name_log} - {model_name}] Генерация рекурсивного прогноза...\")\n",
    "        forecast = recursive_predict(final_model, y_train_comp, n_steps,\n",
    "                                     feature_calculator, feature_order_ml)\n",
    "        print(f\"    [{component_name_log} - {model_name}] Рекурсивный прогноз выполнен.\")\n",
    "        return forecast, actual_best_params\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка [{component_name_log} - {model_name}] (Hyperopt): {e}\")\n",
    "        traceback.print_exc()\n",
    "        return np.full(n_steps, np.nan), {}\n",
    "\n",
    "\n",
    "def train_predict_hybrid(y_train_comp, n_steps, arima_order, xgb_hyperopt_space,\n",
    "                         feature_calculator, feature_order_hybrid, tscv_hybrid, max_evals_hyperopt,\n",
    "                         component_name_log=\"Hybrid\"):\n",
    "    \"\"\"Обучает гибрид ARIMA+XGB_на_ошибках (с Hyperopt для XGB) и делает рекурсивный прогноз.\"\"\"\n",
    "    print(f\"    [{component_name_log}] Подготовка данных и Hyperopt для XGB на ошибках...\")\n",
    "    try:\n",
    "        if arima_order is None:\n",
    "            print(f\"      [{component_name_log}] Ошибка: Порядок ARIMA не определен.\")\n",
    "            return np.full(n_steps, np.nan), {}\n",
    "\n",
    "        print(f\"      [{component_name_log}] Обучение базовой ARIMA {arima_order}...\")\n",
    "        arima_model_fit = StatsmodelsARIMA(y_train_comp, order=arima_order,\n",
    "                                       enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "        arima_fitted = arima_model_fit.fittedvalues\n",
    "        common_idx = y_train_comp.index.intersection(arima_fitted.index)\n",
    "        if common_idx.empty :\n",
    "            raise ValueError(f\"[{component_name_log}] Нет общих индексов между y_train_comp и arima_fitted.\")\n",
    "        y_train_aligned = y_train_comp.loc[common_idx]\n",
    "        arima_fitted_aligned = arima_fitted.loc[common_idx]\n",
    "        arima_errors = y_train_aligned - arima_fitted_aligned\n",
    "        print(f\"      [{component_name_log}] Базовая ARIMA обучена, ошибки рассчитаны (длина {len(arima_errors)}).\")\n",
    "\n",
    "        # Используем y_train_comp для генерации признаков, затем выравниваем с arima_errors\n",
    "        data_ml_comp_hybrid = pd.DataFrame({'t': y_train_comp})\n",
    "        for lag in LAGS_TO_CREATE: data_ml_comp_hybrid[f't-{lag}'] = data_ml_comp_hybrid['t'].shift(lag)\n",
    "        target_series_hybrid = data_ml_comp_hybrid['t']\n",
    "        for window in ROLLING_WINDOWS:\n",
    "            data_ml_comp_hybrid[f't_mean_lag{window}'] = target_series_hybrid.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "            data_ml_comp_hybrid[f't_std_lag{window}'] = target_series_hybrid.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        if isinstance(data_ml_comp_hybrid.index, pd.DatetimeIndex):\n",
    "            month_num_hybrid = data_ml_comp_hybrid.index.month\n",
    "            data_ml_comp_hybrid['month_sin'] = np.sin(2 * np.pi * month_num_hybrid / 12)\n",
    "            data_ml_comp_hybrid['month_cos'] = np.cos(2 * np.pi * month_num_hybrid / 12)\n",
    "        else:\n",
    "            data_ml_comp_hybrid['month_sin'] = np.nan\n",
    "            data_ml_comp_hybrid['month_cos'] = np.nan\n",
    "        \n",
    "        X_train_xgb_hybrid_full = data_ml_comp_hybrid[feature_order_hybrid]\n",
    "        common_indices_xgb_err = X_train_xgb_hybrid_full.index.intersection(arima_errors.index)\n",
    "        X_train_xgb_aligned = X_train_xgb_hybrid_full.loc[common_indices_xgb_err].dropna()\n",
    "        arima_errors_aligned = arima_errors.loc[X_train_xgb_aligned.index]\n",
    "\n",
    "        min_samples_for_cv_hybrid = 0\n",
    "        for train_idx_h, _ in tscv_hybrid.split(X_train_xgb_aligned):\n",
    "            if len(train_idx_h) > min_samples_for_cv_hybrid:\n",
    "                 min_samples_for_cv_hybrid = len(train_idx_h)\n",
    "\n",
    "        if X_train_xgb_aligned.empty or len(X_train_xgb_aligned) < tscv_hybrid.get_n_splits() + min_samples_for_cv_hybrid:\n",
    "             print(f\"      [{component_name_log}] Ошибка: Недостаточно данных ({len(X_train_xgb_aligned)}) для Hyperopt XGB (нужно {tscv_hybrid.get_n_splits() + min_samples_for_cv_hybrid}).\")\n",
    "             return np.full(n_steps, np.nan), {}\n",
    "        print(f\"      [{component_name_log}] Признаки для XGB на ошибках готовы ({X_train_xgb_aligned.shape}). Цель (ошибки) готова ({arima_errors_aligned.shape})\")\n",
    "\n",
    "        # --- Hyperopt Objective Function для XGBoost на ошибках ---\n",
    "        def objective_xgb_error(params):\n",
    "            current_params = params.copy()\n",
    "            if 'n_estimators' in current_params: current_params['n_estimators'] = int(current_params['n_estimators'])\n",
    "            if 'max_depth' in current_params and current_params['max_depth'] is not None: current_params['max_depth'] = int(current_params['max_depth'])\n",
    "            if 'num_leaves' in current_params: current_params['num_leaves'] = int(current_params['num_leaves'])\n",
    "\n",
    "            model_xgb_err = xgb.XGBRegressor(**current_params, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "            cv_scores_err = []\n",
    "            for train_idx, val_idx in tscv_hybrid.split(X_train_xgb_aligned):\n",
    "                X_cv_train_err, X_cv_val_err = X_train_xgb_aligned.iloc[train_idx], X_train_xgb_aligned.iloc[val_idx]\n",
    "                y_cv_train_err, y_cv_val_err = arima_errors_aligned.iloc[train_idx], arima_errors_aligned.iloc[val_idx]\n",
    "                if X_cv_train_err.empty or X_cv_val_err.empty or len(X_cv_train_err) < 2 or len(X_cv_val_err) < 1:\n",
    "                    continue\n",
    "                try:\n",
    "                    model_xgb_err.fit(X_cv_train_err, y_cv_train_err)\n",
    "                    preds_err = model_xgb_err.predict(X_cv_val_err)\n",
    "                    rmse_err = np.sqrt(mean_squared_error(y_cv_val_err, preds_err))\n",
    "                    cv_scores_err.append(rmse_err)\n",
    "                except Exception:\n",
    "                    cv_scores_err.append(np.inf)\n",
    "            if not cv_scores_err or all(s == np.inf for s in cv_scores_err):\n",
    "                return {'loss': np.inf, 'status': STATUS_OK}\n",
    "            avg_rmse_err = np.mean([s for s in cv_scores_err if s != np.inf])\n",
    "            if np.isnan(avg_rmse_err): avg_rmse_err = np.inf\n",
    "            return {'loss': avg_rmse_err, 'status': STATUS_OK}\n",
    "\n",
    "        trials_xgb_err = Trials()\n",
    "        best_params_xgb_err_fmin = fmin(fn=objective_xgb_error,\n",
    "                                   space=xgb_hyperopt_space, # Используем то же пространство, что и для обычного XGB\n",
    "                                   algo=tpe.suggest,\n",
    "                                   max_evals=max_evals_hyperopt,\n",
    "                                   trials=trials_xgb_err,\n",
    "                                   rstate=np.random.default_rng(RANDOM_SEED),\n",
    "                                   verbose=0)\n",
    "\n",
    "        actual_best_params_xgb_err = hyperopt.space_eval(xgb_hyperopt_space, best_params_xgb_err_fmin)\n",
    "        for p_name in ['n_estimators', 'max_depth', 'num_leaves']:\n",
    "             if p_name in actual_best_params_xgb_err and actual_best_params_xgb_err[p_name] is not None:\n",
    "                actual_best_params_xgb_err[p_name] = int(actual_best_params_xgb_err[p_name])\n",
    "\n",
    "        best_loss_xgb_err = trials_xgb_err.best_trial['result']['loss'] if trials_xgb_err.best_trial else np.inf\n",
    "        print(f\"      [{component_name_log}] Hyperopt для XGB на ошибках завершен. Лучший CV RMSE: {best_loss_xgb_err:.4f}. Параметры: {actual_best_params_xgb_err}\")\n",
    "\n",
    "        final_xgb_error_model = xgb.XGBRegressor(**actual_best_params_xgb_err, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        final_xgb_error_model.fit(X_train_xgb_aligned, arima_errors_aligned)\n",
    "        print(f\"      [{component_name_log}] XGB на ошибках обучен с лучшими параметрами.\")\n",
    "\n",
    "        print(f\"      [{component_name_log}] Генерация рекурсивного прогноза гибрида...\")\n",
    "        history_arima = list(y_train_comp)\n",
    "        arima_forecast_rec = []\n",
    "        for _ in range(n_steps): # Рекурсивный прогноз ARIMA\n",
    "            model_rec_fit = StatsmodelsARIMA(history_arima, order=arima_order,\n",
    "                                        enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "            next_pred_a = model_rec_fit.forecast(steps=1)[0]\n",
    "            arima_forecast_rec.append(next_pred_a)\n",
    "            history_arima.append(next_pred_a)\n",
    "        arima_forecast_rec = np.array(arima_forecast_rec)\n",
    "        print(f\"        [{component_name_log}] Прогноз ARIMA для гибрида готов.\")\n",
    "\n",
    "        # Рекурсивный прогноз ошибок XGBoost (использует историю y_train_comp для расчета признаков!)\n",
    "        xgb_error_forecast_rec = recursive_predict(final_xgb_error_model, y_train_comp, n_steps,\n",
    "                                                  feature_calculator, feature_order_hybrid)\n",
    "        print(f\"        [{component_name_log}] Прогноз ошибок XGB для гибрида готов.\")\n",
    "\n",
    "        final_forecast = arima_forecast_rec + xgb_error_forecast_rec\n",
    "        print(f\"    [{component_name_log}] Рекурсивный прогноз Гибрида выполнен.\")\n",
    "        return final_forecast, {'arima_order': arima_order, 'xgb_error_params': actual_best_params_xgb_err}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Ошибка [{component_name_log}] (Hyperopt): {e}\")\n",
    "        traceback.print_exc()\n",
    "        return np.full(n_steps, np.nan), {}\n",
    "\n",
    "\n",
    "# --- 5. Функция для оценки моделей и выбора лучшей (ОБНОВЛЕННАЯ для Hyperopt) ---\n",
    "def evaluate_component_models_cv(y_train_comp, y_test_comp, component_name, horizons, weights_rmse,\n",
    "                                 rf_hyperopt_space, xgb_hyperopt_space,\n",
    "                                 tscv_comp, max_evals_hyperopt):\n",
    "    model_forecasts = {}\n",
    "    model_rmses = pd.DataFrame(columns=['model', 'horizon', 'rmse'])\n",
    "    component_all_model_best_params = {} # Сохраняем лучшие параметры для КАЖДОЙ модели внутри этой компоненты\n",
    "\n",
    "    # ARIMA\n",
    "    print(f\"  [{component_name}] Модель: ARIMA\")\n",
    "    forecast_arima, order_arima, arima_params = train_predict_arima(y_train_comp, len(y_test_comp))\n",
    "    model_forecasts['ARIMA'] = forecast_arima\n",
    "    component_all_model_best_params['ARIMA'] = arima_params\n",
    "    if order_arima is not None and not np.all(np.isnan(forecast_arima)): # Проверка на все NaN\n",
    "        for h in horizons:\n",
    "            if len(y_test_comp[:h]) == len(forecast_arima[:h]):\n",
    "                 rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_arima[:h]))\n",
    "                 model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'ARIMA', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # XGBoost\n",
    "    print(f\"\\n  [{component_name}] Модель: XGBoost\")\n",
    "    forecast_xgb, xgb_best_p = train_predict_ml(xgb.XGBRegressor, xgb_hyperopt_space, y_train_comp, len(y_test_comp),\n",
    "                                        calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp, max_evals_hyperopt,\n",
    "                                        component_name_log=component_name) # Передаем имя компоненты для логов\n",
    "    model_forecasts['XGBoost'] = forecast_xgb\n",
    "    component_all_model_best_params['XGBoost'] = xgb_best_p\n",
    "    if not np.all(np.isnan(forecast_xgb)):\n",
    "        for h in horizons:\n",
    "            if len(y_test_comp[:h]) == len(forecast_xgb[:h]):\n",
    "                rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_xgb[:h]))\n",
    "                model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'XGBoost', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # RandomForest\n",
    "    print(f\"\\n  [{component_name}] Модель: RandomForest\")\n",
    "    forecast_rf, rf_best_p = train_predict_ml(RandomForestRegressor, rf_hyperopt_space, y_train_comp, len(y_test_comp),\n",
    "                                       calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp, max_evals_hyperopt,\n",
    "                                       component_name_log=component_name)\n",
    "    model_forecasts['RandomForest'] = forecast_rf\n",
    "    component_all_model_best_params['RandomForest'] = rf_best_p\n",
    "    if not np.all(np.isnan(forecast_rf)):\n",
    "        for h in horizons:\n",
    "            if len(y_test_comp[:h]) == len(forecast_rf[:h]):\n",
    "                rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_rf[:h]))\n",
    "                model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'RandomForest', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "\n",
    "    # Гибрид ARIMA+XGB_err_Rec\n",
    "    print(f\"\\n  [{component_name}] Модель: ARIMA+XGB_err_Rec\")\n",
    "    if order_arima is not None:\n",
    "        forecast_hybrid, hybrid_best_p = train_predict_hybrid(y_train_comp, len(y_test_comp), order_arima, xgb_hyperopt_space,\n",
    "                                                  calculate_features_for_step, COMP_FEATURE_ORDER, tscv_comp, max_evals_hyperopt,\n",
    "                                                  component_name_log=component_name)\n",
    "        model_forecasts['ARIMA+XGB_err_Rec'] = forecast_hybrid\n",
    "        component_all_model_best_params['ARIMA+XGB_err_Rec'] = hybrid_best_p\n",
    "        if not np.all(np.isnan(forecast_hybrid)):\n",
    "            for h in horizons:\n",
    "                if len(y_test_comp[:h]) == len(forecast_hybrid[:h]):\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test_comp[:h], forecast_hybrid[:h]))\n",
    "                    model_rmses = pd.concat([model_rmses, pd.DataFrame([{'model':'ARIMA+XGB_err_Rec', 'horizon':h, 'rmse':rmse}])], ignore_index=True)\n",
    "    else:\n",
    "         print(f\"    [{component_name}] Пропуск Гибрида, т.к. ARIMA не обучилась.\")\n",
    "         model_forecasts['ARIMA+XGB_err_Rec'] = np.full(len(y_test_comp), np.nan)\n",
    "         component_all_model_best_params['ARIMA+XGB_err_Rec'] = {}\n",
    "\n",
    "    print(f\"\\n  [{component_name}] Расчет взвешенного RMSE для выбора лучшей модели...\")\n",
    "    weighted_rmses = {}\n",
    "    models_evaluated = model_rmses['model'].unique()\n",
    "    for model_name_eval in models_evaluated:\n",
    "        if model_name_eval not in model_forecasts or np.all(np.isnan(model_forecasts[model_name_eval])):\n",
    "            print(f\"    [{component_name} - {model_name_eval}] Пропуск из-за отсутствия или NaN прогнозов.\")\n",
    "            weighted_rmses[model_name_eval] = np.inf\n",
    "            continue\n",
    "        rmses_s = model_rmses[(model_rmses['model'] == model_name_eval) & (model_rmses['horizon'].isin(weights_rmse.index))].set_index('horizon')['rmse'].dropna()\n",
    "        common_h = rmses_s.index.intersection(weights_rmse.index)\n",
    "        if not common_h.empty:\n",
    "            valid_weights = weights_rmse.loc[common_h].dropna()\n",
    "            final_common_h = rmses_s.loc[common_h].index.intersection(valid_weights.index) # Пересечение после dropna весов\n",
    "            if not final_common_h.empty:\n",
    "                weighted_avg_rmse = (rmses_s.loc[final_common_h] * valid_weights.loc[final_common_h]).sum() / valid_weights.loc[final_common_h].sum()\n",
    "                if np.isnan(weighted_avg_rmse): weighted_avg_rmse = np.inf\n",
    "                weighted_rmses[model_name_eval] = weighted_avg_rmse\n",
    "                print(f\"    [{component_name} - {model_name_eval}]: Weighted RMSE = {weighted_avg_rmse:.4f} (на {len(final_common_h)} горизонтах)\")\n",
    "            else:\n",
    "                weighted_rmses[model_name_eval] = np.inf\n",
    "        else:\n",
    "            print(f\"    [{component_name} - {model_name_eval}]: Недостаточно RMSE или весов для расчета.\")\n",
    "            weighted_rmses[model_name_eval] = np.inf\n",
    "\n",
    "    best_model_name = \"ARIMA\" # Запасной вариант\n",
    "    if not weighted_rmses or all(np.isinf(v) for v in weighted_rmses.values()):\n",
    "        print(f\"  ПРЕДУПРЕЖДЕНИЕ [{component_name}]: Не удалось выбрать лучшую модель. Выбрана ARIMA по умолчанию.\")\n",
    "    else:\n",
    "        valid_weighted_rmses = {k: v for k, v in weighted_rmses.items() if not (model_forecasts.get(k) is None or np.all(np.isnan(model_forecasts.get(k))))}\n",
    "        if not valid_weighted_rmses:\n",
    "             print(f\"  ПРЕДУПРЕЖДЕНИЕ [{component_name}]: Нет моделей с валидными прогнозами. Выбрана ARIMA по умолчанию.\")\n",
    "        else:\n",
    "            best_model_name = min(valid_weighted_rmses, key=valid_weighted_rmses.get)\n",
    "    print(f\"\\n  Лучшая модель для компоненты '{component_name}': {best_model_name}\")\n",
    "    return model_forecasts, best_model_name, model_rmses, component_all_model_best_params\n",
    "\n",
    "print(\"Вспомогательные функции (обновленные для Hyperopt) определены.\")\n",
    "# --- КОНЕЦ ОБНОВЛЕННОЙ ЯЧЕЙКИ 7 ---\n",
    "\n",
    "\n",
    "# --- Ячейка 10: Создание Признаков для Мета-моделей на ARIMA OOF ---\n",
    "# Логика этой ячейки не меняется, т.к. она использует create_meta_features,\n",
    "# которая не зависит от GridSearchCV или Hyperopt.\n",
    "# Убедитесь, что все необходимые переменные (df_oof_comp_arima_clean,\n",
    "# train_values_aligned_meta_arima, df_forecasts_arima_comp, COMPONENTS_TO_MODEL,\n",
    "# LAGS_TO_CREATE, ROLLING_WINDOWS, META_FEATURE_ORDER) определены корректно.\n",
    "print(\"\\nЯчейка 10: Создание Признаков для Мета-моделей на ARIMA OOF\")\n",
    "# ... (ваш код Ячейки 10 без изменений) ...\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_oof_comp_arima_clean: DataFrame с OOF ARIMA прогнозами (чистый)\n",
    "# train_values_aligned_meta_arima: Series с таргетом (агрегат), выровненным с OOF ARIMA\n",
    "# df_forecasts_arima_comp: DataFrame с ARIMA прогнозами компонент на тесте\n",
    "# components_to_model: list\n",
    "# create_meta_features: function (должна быть определена в Ячейке 7)\n",
    "# META_FEATURE_ORDER: list - порядок признаков для мета-моделей (определен в Ячейке 7)\n",
    "# LAGS_TO_CREATE, ROLLING_WINDOWS: list (определены в Ячейке 2 или 7)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# --- 1. Создание признаков для ОБУЧАЮЩЕЙ выборки ---\n",
    "# print(\"Создание признаков для ОБУЧАЮЩЕЙ выборки (на ARIMA OOF)...\")\n",
    "# try:\n",
    "#     X_train_meta_arima_eng = create_meta_features(\n",
    "#         df_input=df_oof_comp_arima_clean,\n",
    "#         component_names=COMPONENTS_TO_MODEL,\n",
    "#         lags=LAGS_TO_CREATE,\n",
    "#         windows=ROLLING_WINDOWS,\n",
    "#         feature_order_meta=META_FEATURE_ORDER\n",
    "#     )\n",
    "#     X_train_meta_arima_eng_clean = X_train_meta_arima_eng.dropna()\n",
    "#     y_train_meta_arima_aligned = train_values_aligned_meta_arima.loc[X_train_meta_arima_eng_clean.index]\n",
    "#     print(f\"Размер обучающей выборки с признаками (ARIMA OOF): {X_train_meta_arima_eng_clean.shape}\")\n",
    "#     print(f\"Размер выровненного таргета: {y_train_meta_arima_aligned.shape}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Ошибка при создании обучающих признаков для мета-модели: {e}\")\n",
    "#     X_train_meta_arima_eng_clean = None\n",
    "#     y_train_meta_arima_aligned = None\n",
    "\n",
    "# # --- 2. Создание признаков для ТЕСТОВОЙ выборки ---\n",
    "# if X_train_meta_arima_eng_clean is not None:\n",
    "#     print(\"\\nСоздание признаков для ТЕСТОВОЙ выборки (на ARIMA прогнозах теста)...\")\n",
    "#     try:\n",
    "#         max_lookback_arima = max(max(LAGS_TO_CREATE, default=0), max(ROLLING_WINDOWS, default=0))\n",
    "#         if len(df_oof_comp_arima_clean) >= max_lookback_arima:\n",
    "#             history_for_test_arima = df_oof_comp_arima_clean.iloc[-max_lookback_arima:]\n",
    "#         else:\n",
    "#             print(f\"Предупреждение: Недостаточно OOF истории ({len(df_oof_comp_arima_clean)}) для lookback ({max_lookback_arima}). Используется вся доступная OOF история.\")\n",
    "#             history_for_test_arima = df_oof_comp_arima_clean.copy()\n",
    "#         df_for_test_features_arima = pd.concat([history_for_test_arima, df_forecasts_arima_comp])\n",
    "#         X_test_meta_arima_eng_full = create_meta_features(\n",
    "#             df_input=df_for_test_features_arima,\n",
    "#             component_names=COMPONENTS_TO_MODEL,\n",
    "#             lags=LAGS_TO_CREATE,\n",
    "#             windows=ROLLING_WINDOWS,\n",
    "#             feature_order_meta=META_FEATURE_ORDER\n",
    "#         )\n",
    "#         X_test_meta_arima_eng = X_test_meta_arima_eng_full.loc[df_forecasts_arima_comp.index]\n",
    "#         if X_test_meta_arima_eng.isnull().any().any():\n",
    "#             print(\"ВНИМАНИЕ: Обнаружены NaN в тестовых признаках (ARIMA OOF). Заполняем медианой трейна...\")\n",
    "#             for col in X_test_meta_arima_eng.columns:\n",
    "#                 if X_test_meta_arima_eng[col].isnull().any():\n",
    "#                     median_val = X_train_meta_arima_eng_clean[col].median()\n",
    "#                     X_test_meta_arima_eng[col] = X_test_meta_arima_eng[col].fillna(median_val)\n",
    "#             print(f\"Кол-во NaN после заполнения: {X_test_meta_arima_eng.isnull().sum().sum()}\")\n",
    "#         else:\n",
    "#             print(\"NaN в тестовых признаках (ARIMA OOF) не обнаружены.\")\n",
    "#         print(\"\\n--- Признаки для мета-моделей на ARIMA OOF готовы ---\")\n",
    "#         print(f\"Размер X_train_meta_arima_eng_clean: {X_train_meta_arima_eng_clean.shape}\")\n",
    "#         print(f\"Размер y_train_meta_arima_aligned: {y_train_meta_arima_aligned.shape}\")\n",
    "#         print(f\"Размер X_test_meta_arima_eng: {X_test_meta_arima_eng.shape}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Ошибка при создании тестовых признаков для мета-модели: {e}\")\n",
    "#         X_test_meta_arima_eng = None\n",
    "# else:\n",
    "#      print(\"Пропуск создания тестовых признаков из-за ошибки на этапе создания обучающих.\")\n",
    "#      X_test_meta_arima_eng = None\n",
    "# print(\"\\n--- Ячейка 10 завершена ---\")\n",
    "# print(\"Колонки X_train_meta_arima_eng:\", X_train_meta_arima_eng.columns.tolist() if X_train_meta_arima_eng is not None else \"N/A\")\n",
    "# print(\"Количество колонок:\", len(X_train_meta_arima_eng.columns) if X_train_meta_arima_eng is not None else \"N/A\")\n",
    "# print(\"Размер до dropna:\", X_train_meta_arima_eng.shape if X_train_meta_arima_eng is not None else \"N/A\")\n",
    "\n",
    "# --- КОНЕЦ ЯЧЕЙКИ 10 ---\n",
    "\n",
    "\n",
    "# --- Ячейка 14: Запуск Оценки Моделей для Каждой Компоненты (с Hyperopt для ML) ---\n",
    "print(\"\\n--- Раздел 4: Bottom-up Подход - Выбор Лучшей Модели для Компонент ---\")\n",
    "print(\"\\nЯчейка 14: Запуск Оценки Моделей для Каждой Компоненты (с Hyperopt для ML)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ГЛОБАЛЬНЫХ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data, test_comp_data: DataFrames\n",
    "# COMPONENTS_TO_MODEL: list\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# WEIGHTS_NORMALIZED_RMSE: pd.Series\n",
    "# N_CV_SPLITS: int\n",
    "# MAX_EVALS_HYPEROPT: int\n",
    "# COMP_FEATURE_ORDER: list\n",
    "# LAGS_TO_CREATE, ROLLING_WINDOWS\n",
    "# RANDOM_SEED\n",
    "# TEST_MONTHS\n",
    "# tscv_component_cv: TimeSeriesSplit объект\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ --\n",
    "\n",
    "# Определяем \"обоснованные\" пространства поиска для Hyperopt\n",
    "# hp.quniform(label, low, high, q) -> round(uniform(low, high) / q) * q\n",
    "# Для строго целочисленных значений используем scope.int()\n",
    "rf_hyperopt_space_comp = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators_rf', 50, 250, 25)),\n",
    "    'max_depth': hp.choice('max_depth_rf', [None, scope.int(hp.quniform('max_depth_val_rf', 3, 15, 1))]),\n",
    "    'max_features': hp.choice('max_features_rf', ['sqrt', 'log2', hp.uniform('max_features_float_rf', 0.4, 0.9)]),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf_rf', 1, 5, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split_rf', 2, 10, 1))\n",
    "}\n",
    "\n",
    "xgb_hyperopt_space_comp = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators_xgb', 50, 350, 25)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth_xgb', 2, 8, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate_xgb', np.log(0.005), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample_xgb', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree_xgb', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma_xgb', 0, 0.7),\n",
    "    'reg_alpha': hp.uniform('reg_alpha_xgb', 0, 0.5), # L1\n",
    "    'reg_lambda': hp.uniform('reg_lambda_xgb', 0, 0.5), # L2\n",
    "    # 'num_leaves': scope.int(hp.quniform('num_leaves_xgb', 10, 100, 5)) # Если используется LightGBM или XGB с tree_method='hist'\n",
    "}\n",
    "\n",
    "# Инициализируем структуры для хранения результатов\n",
    "component_best_models = {} # {компонента: имя_лучшей_модели}\n",
    "component_best_forecasts_dict = {} # {компонента: pd.Series прогноза}\n",
    "all_component_model_rmses_list = [] # список DataFrame'ов RMSE для всех моделей всех компонент\n",
    "all_components_best_params = {} # {компонента: {модель: параметры}}\n",
    "\n",
    "print(f\"Запуск оценки {len(COMPONENTS_TO_MODEL)} компонент...\")\n",
    "print(f\"Используется TimeSeriesSplit с {N_CV_SPLITS} сплитами для Hyperopt ML моделей (max_evals={MAX_EVALS_HYPEROPT}).\")\n",
    "\n",
    "for component_name in COMPONENTS_TO_MODEL:\n",
    "    print(f\"\\n===== Обработка компоненты: {component_name} =====\")\n",
    "    y_train_c = train_comp_data[component_name].copy()\n",
    "    if not isinstance(y_train_c.index, pd.DatetimeIndex):\n",
    "        y_train_c.index = pd.to_datetime(y_train_c.index)\n",
    "    y_train_c = y_train_c.rename('t').dropna()\n",
    "\n",
    "    y_test_c = test_comp_data[component_name].copy()\n",
    "    if not isinstance(y_test_c.index, pd.DatetimeIndex):\n",
    "        y_test_c.index = pd.to_datetime(y_test_c.index)\n",
    "    y_test_c = y_test_c.rename('t')\n",
    "\n",
    "    if len(y_train_c) < 20: # Минимальный размер обучающей выборки\n",
    "        print(f\"  ПРЕДУПРЕЖДЕНИЕ: Слишком мало данных ({len(y_train_c)}) для компоненты {component_name}. Пропуск.\")\n",
    "        component_best_models[component_name] = 'Skipped'\n",
    "        component_best_forecasts_dict[component_name] = pd.Series(np.full(len(y_test_c), np.nan), index=y_test_c.index)\n",
    "        all_components_best_params[component_name] = {} # Нет параметров для пропущенной компоненты\n",
    "        continue\n",
    "\n",
    "    model_forecasts_comp, best_model_name_comp, component_rmses_df, comp_model_p = evaluate_component_models_cv(\n",
    "        y_train_c,\n",
    "        y_test_c,\n",
    "        component_name,\n",
    "        HORIZONS_TO_EVALUATE,\n",
    "        WEIGHTS_NORMALIZED_RMSE,\n",
    "        rf_hyperopt_space_comp,\n",
    "        xgb_hyperopt_space_comp,\n",
    "        tscv_component_cv, # Передаем объект TimeSeriesSplit\n",
    "        MAX_EVALS_HYPEROPT\n",
    "    )\n",
    "\n",
    "    component_best_models[component_name] = best_model_name_comp\n",
    "    all_components_best_params[component_name] = comp_model_p # Сохраняем параметры всех моделей для этой компоненты\n",
    "\n",
    "    if best_model_name_comp and best_model_name_comp in model_forecasts_comp and model_forecasts_comp[best_model_name_comp] is not None:\n",
    "        best_fc = model_forecasts_comp[best_model_name_comp]\n",
    "        # Приводим к pd.Series с правильным индексом и длиной\n",
    "        final_fc_series = pd.Series(np.nan, index=y_test_c.index)\n",
    "        common_len = min(len(best_fc), len(y_test_c))\n",
    "        final_fc_series.iloc[:common_len] = best_fc[:common_len]\n",
    "        component_best_forecasts_dict[component_name] = final_fc_series\n",
    "\n",
    "        if len(best_fc) != len(y_test_c):\n",
    "            print(f\"  Предупреждение для {component_name} ({best_model_name_comp}): Длина прогноза ({len(best_fc)}) не совпадает с тестом ({len(y_test_c)}). Скорректировано.\")\n",
    "    else:\n",
    "        print(f\"  ПРЕДУПРЕЖДЕНИЕ для {component_name}: Не удалось получить прогноз лучшей модели ({best_model_name_comp}) или он NaN. Заполняем NaN.\")\n",
    "        component_best_forecasts_dict[component_name] = pd.Series(np.full(len(y_test_c), np.nan), index=y_test_c.index)\n",
    "\n",
    "    if not component_rmses_df.empty:\n",
    "        component_rmses_df['component'] = component_name\n",
    "        all_component_model_rmses_list.append(component_rmses_df)\n",
    "\n",
    "print(\"\\n--- Выбор лучших моделей для компонент завершен ---\")\n",
    "\n",
    "df_best_component_forecasts = pd.DataFrame(component_best_forecasts_dict)\n",
    "if not df_best_component_forecasts.empty and not isinstance(df_best_component_forecasts.index, pd.DatetimeIndex):\n",
    "    # Попытка восстановить DatetimeIndex, если он был утерян (например, если dict был пуст и создался с RangeIndex)\n",
    "    if not test_comp_data.index.empty:\n",
    "         df_best_component_forecasts.index = test_comp_data.index[:len(df_best_component_forecasts)]\n",
    "\n",
    "\n",
    "if all_component_model_rmses_list:\n",
    "    df_all_component_rmses = pd.concat(all_component_model_rmses_list, ignore_index=True)\n",
    "else:\n",
    "    df_all_component_rmses = pd.DataFrame(columns=['model', 'horizon', 'rmse', 'component'])\n",
    "\n",
    "print(\"\\n--- Лучшие параметры, найденные Hyperopt для каждой компоненты и модели: ---\")\n",
    "for comp_name_p, models_p_dict in all_components_best_params.items():\n",
    "    print(f\"  Компонента: {comp_name_p}\")\n",
    "    if models_p_dict:\n",
    "        for model_n, params_m in models_p_dict.items():\n",
    "            print(f\"    Модель: {model_n}, Параметры: {params_m}\")\n",
    "    else:\n",
    "        print(\"    Параметры не найдены (компонент мог быть пропущен или для него не нашлись параметры).\")\n",
    "\n",
    "print(\"\\n--- Ячейка 14 завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1745846661609,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "xZpT1zCRIF2h",
    "outputId": "c15ed065-02c7-4a54-e654-0bcfbe0a497e"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 15: Анализ Выбора Моделей Компонент\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# component_best_models: dict - Словарь с именами лучших моделей для компонент\n",
    "# df_all_component_rmses: pd.DataFrame - Таблица с RMSE всех моделей для всех компонент\n",
    "# df_best_component_forecasts: pd.DataFrame - Прогнозы лучших моделей на тесте\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# 1. Вывод списка лучших моделей\n",
    "print(\"--- Лучшие модели, выбранные для каждой компоненты ---\")\n",
    "if 'component_best_models' in locals() and component_best_models:\n",
    "    for component, model_name in component_best_models.items():\n",
    "        print(f\"  - {component}: {model_name}\")\n",
    "else:\n",
    "    print(\"Словарь лучших моделей пуст или не определен.\")\n",
    "\n",
    "# 2. Вывод сводной таблицы RMSE всех моделей по компонентам\n",
    "print(\"\\n--- Сводная таблица RMSE моделей по компонентам и горизонтам ---\")\n",
    "if 'df_all_component_rmses' in locals() and not df_all_component_rmses.empty:\n",
    "    try:\n",
    "        # Создаем сводную таблицу: строки - компонента и горизонт, колонки - модели\n",
    "        pivot_comp_rmse = df_all_component_rmses.pivot_table(\n",
    "            index=['component', 'horizon'],\n",
    "            columns='model',\n",
    "            values='rmse'\n",
    "        )\n",
    "        # Определяем порядок колонок (моделей) для вывода\n",
    "        model_order = ['ARIMA', 'RandomForest', 'XGBoost', 'ARIMA+XGB_err_Rec'] # Примерный порядок\n",
    "        # Убираем модели, которых нет в таблице\n",
    "        model_order_present = [m for m in model_order if m in pivot_comp_rmse.columns]\n",
    "        # Добавляем остальные модели, если они есть\n",
    "        remaining_models = [m for m in pivot_comp_rmse.columns if m not in model_order_present]\n",
    "        final_model_order = model_order_present + remaining_models\n",
    "\n",
    "        print(pivot_comp_rmse[final_model_order].round(4)) # Выводим с заданным порядком\n",
    "    except KeyError as e:\n",
    "        print(f\"Ошибка при создании сводной таблицы (возможно, отсутствует колонка): {e}\")\n",
    "        print(\"Вывод таблицы как есть:\")\n",
    "        try:\n",
    "            print(df_all_component_rmses.pivot_table(index=['component', 'horizon'], columns='model', values='rmse').round(4))\n",
    "        except Exception as e_inner:\n",
    "            print(f\"Не удалось вывести таблицу: {e_inner}\")\n",
    "            print(\"Данные RMSE:\")\n",
    "            print(df_all_component_rmses) # Печатаем сырые данные для отладки\n",
    "    except Exception as e:\n",
    "        print(f\"Не удалось создать или вывести сводную таблицу RMSE: {e}\")\n",
    "        print(\"Данные RMSE:\")\n",
    "        print(df_all_component_rmses)\n",
    "else:\n",
    "    print(\"Нет данных по RMSE моделей компонент для отображения.\")\n",
    "\n",
    "# 3. Вывод начала таблицы с лучшими прогнозами\n",
    "print(\"\\n--- Лучшие прогнозы для агрегации (первые 5 строк) ---\")\n",
    "if 'df_best_component_forecasts' in locals() and not df_best_component_forecasts.empty:\n",
    "    print(df_best_component_forecasts.head())\n",
    "    print(\"\\nПроверка на NaN в лучших прогнозах:\")\n",
    "    print(df_best_component_forecasts.isnull().sum())\n",
    "else:\n",
    "    print(\"DataFrame с лучшими прогнозами пуст или не определен.\")\n",
    "\n",
    "print(\"\\n--- Ячейка 15 завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1745848575857,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "WTnnZ16EQpw1",
    "outputId": "dc83482c-4cd9-4101-d7db-264fe11be39f"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Раздел 5: Агрегация Лучших Прогнозов Компонент ---\")\n",
    "print(\"\\nЯчейка 16: Агрегация Лучших Прогнозов Простыми Весами\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_best_component_forecasts: DataFrame [ИндексТеста x Компоненты] - прогнозы ЛУЧШИХ моделей (из Ячейки 14)\n",
    "# weights_norm: DataFrame [Год x Компонент] - нормализованные веса (из Ячейки 4)\n",
    "# train_values: Series - Обучающая выборка агрегата (из Ячейки 5)\n",
    "# test_values: Series - Тестовая выборка агрегата (из Ячейки 5)\n",
    "# COMPONENTS_TO_MODEL: list\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# aggregation_rmse: dict (инициализирован в Ячейке 2)\n",
    "# all_horizon_rmse: pd.DataFrame (инициализирован в Ячейке 2)\n",
    "# final_forecasts: dict (инициализирован в Ячейке 2)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Проверка на NaN в лучших прогнозах (важно!)\n",
    "if 'df_best_component_forecasts' not in locals() or df_best_component_forecasts is None:\n",
    "     print(\"ОШИБКА: DataFrame 'df_best_component_forecasts' не найден. Запустите Ячейку 14.\")\n",
    "     # Останавливаем выполнение или обрабатываем ошибку\n",
    "     raise NameError(\"df_best_component_forecasts не определен\")\n",
    "elif df_best_component_forecasts.isnull().any().any():\n",
    "    print(\"ВНИМАНИЕ: Обнаружены NaN в 'df_best_component_forecasts'. Результаты агрегации могут быть некорректными.\")\n",
    "    print(df_best_component_forecasts.isnull().sum())\n",
    "    # Можно добавить стратегию обработки NaN, если необходимо\n",
    "    # df_best_component_forecasts = df_best_component_forecasts.fillna(method='ffill').fillna(method='bfill') # Пример\n",
    "\n",
    "# --- Метод 1: Веса последнего года обучения ---\n",
    "model_name_best_lastw = \"BottomUp-BestComp-LastW\" # Используем это имя\n",
    "print(f\"\\n--- Расчет: {model_name_best_lastw} ---\")\n",
    "try:\n",
    "    last_train_date_agg = train_values.index.max()\n",
    "    last_train_year_agg = last_train_date_agg.year\n",
    "\n",
    "    if last_train_year_agg in weights_norm.index:\n",
    "        last_train_weights_agg = weights_norm.loc[last_train_year_agg]\n",
    "        print(f\"Используются веса за {last_train_year_agg} год:\\n\", last_train_weights_agg[COMPONENTS_TO_MODEL])\n",
    "\n",
    "        # Агрегация (убедимся, что порядок колонок совпадает)\n",
    "        forecast_best_lastw = (df_best_component_forecasts[COMPONENTS_TO_MODEL] * last_train_weights_agg[COMPONENTS_TO_MODEL]).sum(axis=1)\n",
    "        final_forecasts[model_name_best_lastw] = forecast_best_lastw.values\n",
    "\n",
    "        # Оценка\n",
    "        eval_len = min(len(forecast_best_lastw), len(test_values))\n",
    "        overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], forecast_best_lastw[:eval_len]))\n",
    "        print(f\"Общий RMSE: {overall_rmse:.4f}\")\n",
    "        aggregation_rmse[model_name_best_lastw] = overall_rmse\n",
    "\n",
    "        print(\"RMSE по горизонтам:\")\n",
    "        horizon_results_list = []\n",
    "        for h in HORIZONS_TO_EVALUATE:\n",
    "            if h <= eval_len:\n",
    "                rmse_h = np.sqrt(mean_squared_error(test_values[:h], forecast_best_lastw[:h]))\n",
    "                # print(f\"  1-{h} мес.: {rmse_h:.4f}\") # Убрал вывод для краткости\n",
    "                horizon_results_list.append({'model': model_name_best_lastw, 'horizon': h, 'rmse': rmse_h})\n",
    "        if horizon_results_list:\n",
    "             df_horizon = pd.DataFrame(horizon_results_list)\n",
    "             all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_best_lastw]\n",
    "             all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "        print(\"RMSE по горизонтам рассчитаны.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"ОШИБКА: Веса за {last_train_year_agg} год не найдены!\")\n",
    "        aggregation_rmse[model_name_best_lastw] = np.nan\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при расчете {model_name_best_lastw}: {e}\")\n",
    "    aggregation_rmse[model_name_best_lastw] = np.nan\n",
    "\n",
    "\n",
    "# --- Метод 2: Средние веса ---\n",
    "model_name_best_avgw = \"BottomUp-BestComp-AvgW\" # Используем это имя\n",
    "print(f\"\\n--- Расчет: {model_name_best_avgw} ---\")\n",
    "try:\n",
    "    avg_weights_agg = weights_norm.mean()\n",
    "    print(\"Используемые средние веса:\\n\", avg_weights_agg[COMPONENTS_TO_MODEL])\n",
    "\n",
    "    # Агрегация\n",
    "    forecast_best_avgw = (df_best_component_forecasts[COMPONENTS_TO_MODEL] * avg_weights_agg[COMPONENTS_TO_MODEL]).sum(axis=1)\n",
    "    final_forecasts[model_name_best_avgw] = forecast_best_avgw.values\n",
    "\n",
    "    # Оценка\n",
    "    eval_len = min(len(forecast_best_avgw), len(test_values))\n",
    "    overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], forecast_best_avgw[:eval_len]))\n",
    "    print(f\"Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name_best_avgw] = overall_rmse\n",
    "\n",
    "    print(\"RMSE по горизонтам:\")\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "         if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(test_values[:h], forecast_best_avgw[:h]))\n",
    "            # print(f\"  1-{h} мес.: {rmse_h:.4f}\") # Убрал вывод\n",
    "            horizon_results_list.append({'model': model_name_best_avgw, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_best_avgw]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "    print(\"RMSE по горизонтам рассчитаны.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при расчете {model_name_best_avgw}: {e}\")\n",
    "    aggregation_rmse[model_name_best_avgw] = np.nan\n",
    "\n",
    "\n",
    "# --- Вывод обновленных таблиц ---\n",
    "print(\"\\n--- Обновленное сравнение RMSE (Общий RMSE) ---\")\n",
    "if 'aggregation_rmse' in locals() and aggregation_rmse:\n",
    "    # Добавляем проверку, что ключи существуют перед сортировкой\n",
    "    valid_keys = [k for k in aggregation_rmse if pd.notna(aggregation_rmse[k])]\n",
    "    if valid_keys:\n",
    "        sorted_rmse = {k: aggregation_rmse[k] for k in sorted(valid_keys, key=aggregation_rmse.get)}\n",
    "        for model, rmse in sorted_rmse.items(): print(f\"  {model}: {rmse:.4f}\")\n",
    "    else: print(\"Нет валидных RMSE для сортировки.\")\n",
    "else: print(\"Словарь aggregation_rmse пуст.\")\n",
    "\n",
    "print(\"\\nОбновленная таблица RMSE по горизонтам:\")\n",
    "if 'all_horizon_rmse' in locals() and not all_horizon_rmse.empty:\n",
    "    try:\n",
    "        pivot_rmse_final = all_horizon_rmse.pivot(index='horizon', columns='model', values='rmse')\n",
    "        # Получаем порядок из обновленного aggregation_rmse\n",
    "        if 'sorted_rmse' in locals():\n",
    "             ordered_columns_keys = list(sorted_rmse.keys())\n",
    "             ordered_columns_final = [col for col in ordered_columns_keys if col in pivot_rmse_final.columns]\n",
    "             remaining_cols_final = [col for col in pivot_rmse_final.columns if col not in ordered_columns_final]\n",
    "             pivot_rmse_final = pivot_rmse_final[ordered_columns_final + remaining_cols_final]\n",
    "        print(pivot_rmse_final.round(4))\n",
    "    except Exception as e: print(f\"Не удалось создать сводную таблицу: {e}\")\n",
    "else: print(\"Нет данных.\")\n",
    "\n",
    "print(\"\\n--- Ячейка 16 завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386149,
     "status": "ok",
     "timestamp": 1745848090582,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "NclqhadRSkGy",
    "outputId": "57d1010e-d061-47b5-b540-95e3fd92315a"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 17: Генерация OOF-прогнозов от Лучших Моделей (с Hyperopt)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# train_comp_data: DataFrame\n",
    "# components_to_model: list\n",
    "# component_best_models: Словарь {компонента: имя_лучшей_модели} (из Ячейки 14/15)\n",
    "# N_OOF_SPLITS: int\n",
    "# N_CV_SPLITS: int (для внутреннего CV в Hyperopt)\n",
    "# MAX_EVALS_HYPEROPT: int (для Hyperopt)\n",
    "# Функции: calculate_features_for_step, recursive_predict (из Ячейки 7)\n",
    "# Функции: train_predict_arima, train_predict_ml, train_predict_hybrid (из Ячейки 7, обновленные для Hyperopt)\n",
    "# Пространства поиска: rf_hyperopt_space_comp, xgb_hyperopt_space_comp (из Ячейки 14)\n",
    "# Порядки: arima_orders_dict (сформированный словарь {компонента: порядок_ARIMA})\n",
    "# COMP_FEATURE_ORDER: list\n",
    "# Глобальные переменные, используемые функциями (RANDOM_SEED, LAGS_TO_CREATE, ROLLING_WINDOWS)\n",
    "# train_values: pd.Series (агрегированный таргет для финального выравнивания)\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# --- Инициализация DataFrame для OOF прогнозов лучших моделей ---\n",
    "df_oof_best_comp = pd.DataFrame(index=train_comp_data.index,\n",
    "                                columns=COMPONENTS_TO_MODEL, # Используем COMPONENTS_TO_MODEL\n",
    "                                dtype=float)\n",
    "\n",
    "print(f\"Генерация OOF-прогнозов ЛУЧШИХ моделей (согласно component_best_models)...\")\n",
    "print(f\"Используется TimeSeriesSplit с {N_OOF_SPLITS} фолдами для OOF.\")\n",
    "print(f\"Для ML моделей внутри каждого OOF фолда будет запущен Hyperopt (max_evals={MAX_EVALS_HYPEROPT}) с {N_CV_SPLITS} внутренними CV фолдами.\")\n",
    "# print(f\"Выбранные лучшие модели: {component_best_models}\") # Раскомментируйте, если нужно детально посмотреть\n",
    "\n",
    "# Инициализируем TimeSeriesSplit для OOF\n",
    "tscv_oof_best = TimeSeriesSplit(n_splits=N_OOF_SPLITS)\n",
    "# Инициализируем TimeSeriesSplit для CV внутри ML моделей (Hyperopt)\n",
    "tscv_inner_cv_oof = TimeSeriesSplit(n_splits=N_CV_SPLITS) # Переименовал во избежание путаницы с tscv_component_cv из Ячейки 14\n",
    "\n",
    "# --- Цикл по компонентам ---\n",
    "for i, component_name in enumerate(COMPONENTS_TO_MODEL): # Используем COMPONENTS_TO_MODEL\n",
    "    print(f\"\\n({i+1}/{len(COMPONENTS_TO_MODEL)}) OOF для компоненты: {component_name}\")\n",
    "\n",
    "    # Убедимся, что индекс является DatetimeIndex перед работой\n",
    "    component_series_train_original = train_comp_data[component_name].copy()\n",
    "    if not isinstance(component_series_train_original.index, pd.DatetimeIndex):\n",
    "        component_series_train_original.index = pd.to_datetime(component_series_train_original.index)\n",
    "    component_series_train = component_series_train_original.rename('t').dropna()\n",
    "\n",
    "    best_model_type = component_best_models.get(component_name)\n",
    "\n",
    "    if not best_model_type or best_model_type == 'Skipped': # Проверка на пропущенные модели\n",
    "        print(f\"  ПРЕДУПРЕЖДЕНИЕ: Лучшая модель для {component_name} не определена или пропущена ({best_model_type}). Пропуск OOF.\")\n",
    "        df_oof_best_comp[component_name] = np.nan # Заполняем NaN, если модель была пропущена\n",
    "        continue\n",
    "\n",
    "    print(f\"  Лучшая модель для OOF: {best_model_type}\")\n",
    "\n",
    "    oof_predictions_component = pd.Series(index=component_series_train.index, dtype=float)\n",
    "    current_arima_order = arima_orders_dict.get(component_name)\n",
    "\n",
    "    split_count = 0\n",
    "    for train_idx, val_idx in tscv_oof_best.split(component_series_train):\n",
    "        split_count += 1\n",
    "        train_data_split = component_series_train.iloc[train_idx]\n",
    "        # val_data_split = component_series_train.iloc[val_idx] # Не используется напрямую, только индексы\n",
    "        val_indices = component_series_train.index[val_idx]\n",
    "        n_steps_val = len(val_idx)\n",
    "\n",
    "        if len(train_data_split) < 20: # Минимальный размер обучающей выборки на фолде\n",
    "             print(f\"    Фолд {split_count}/{N_OOF_SPLITS}: Пропуск (мало данных: {len(train_data_split)}) для {best_model_type}\")\n",
    "             # oof_predictions_component.loc[val_indices] = np.nan # Уже инициализировано NaN по умолчанию\n",
    "             continue\n",
    "        else:\n",
    "             print(f\"    Фолд {split_count}/{N_OOF_SPLITS}: Обучение {best_model_type} на {len(train_data_split)}, прогноз на {n_steps_val} шагов.\")\n",
    "\n",
    "        forecast_val_fold = np.full(n_steps_val, np.nan) # Инициализация прогноза для текущего фолда\n",
    "\n",
    "        try:\n",
    "            if best_model_type == 'ARIMA':\n",
    "                forecast_val_fold, _, _ = train_predict_arima(train_data_split, n_steps_val) # train_predict_arima возвращает (forecast, order, params)\n",
    "\n",
    "            elif best_model_type == 'RandomForest':\n",
    "                forecast_val_fold, _ = train_predict_ml( # train_predict_ml возвращает (forecast, params)\n",
    "                    model_class=RandomForestRegressor,\n",
    "                    hyperopt_space=rf_hyperopt_space_comp, # Пространство Hyperopt для RF\n",
    "                    y_train_comp=train_data_split,\n",
    "                    n_steps=n_steps_val,\n",
    "                    feature_calculator=calculate_features_for_step,\n",
    "                    feature_order_ml=COMP_FEATURE_ORDER,\n",
    "                    tscv_ml=tscv_inner_cv_oof, # TS Splitter для внутреннего CV Hyperopt\n",
    "                    max_evals_hyperopt=MAX_EVALS_HYPEROPT,\n",
    "                    component_name_log=f\"{component_name}_OOF_Fold{split_count}\"\n",
    "                )\n",
    "\n",
    "            elif best_model_type == 'XGBoost':\n",
    "                 forecast_val_fold, _ = train_predict_ml( # train_predict_ml возвращает (forecast, params)\n",
    "                    model_class=xgb.XGBRegressor,\n",
    "                    hyperopt_space=xgb_hyperopt_space_comp, # Пространство Hyperopt для XGB\n",
    "                    y_train_comp=train_data_split,\n",
    "                    n_steps=n_steps_val,\n",
    "                    feature_calculator=calculate_features_for_step,\n",
    "                    feature_order_ml=COMP_FEATURE_ORDER,\n",
    "                    tscv_ml=tscv_inner_cv_oof,\n",
    "                    max_evals_hyperopt=MAX_EVALS_HYPEROPT,\n",
    "                    component_name_log=f\"{component_name}_OOF_Fold{split_count}\"\n",
    "                )\n",
    "\n",
    "            elif best_model_type == 'ARIMA+XGB_err_Rec':\n",
    "                if current_arima_order:\n",
    "                    forecast_val_fold, _ = train_predict_hybrid( # train_predict_hybrid возвращает (forecast, params)\n",
    "                        y_train_comp=train_data_split,\n",
    "                        n_steps=n_steps_val,\n",
    "                        arima_order=current_arima_order,\n",
    "                        xgb_hyperopt_space=xgb_hyperopt_space_comp, # Пространство Hyperopt для XGB в гибриде\n",
    "                        feature_calculator=calculate_features_for_step,\n",
    "                        feature_order_hybrid=COMP_FEATURE_ORDER,\n",
    "                        tscv_hybrid=tscv_inner_cv_oof,\n",
    "                        max_evals_hyperopt=MAX_EVALS_HYPEROPT,\n",
    "                        component_name_log=f\"{component_name}_OOF_Fold{split_count}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"      Предупреждение: Порядок ARIMA для гибрида ({component_name}) не найден в arima_orders_dict на фолде {split_count}. Прогноз будет NaN.\")\n",
    "                    # forecast_val_fold остается np.full(n_steps_val, np.nan)\n",
    "\n",
    "            else:\n",
    "                print(f\"    Неизвестный тип модели: {best_model_type} для компоненты {component_name}. Прогноз будет NaN.\")\n",
    "                # forecast_val_fold остается np.full(n_steps_val, np.nan)\n",
    "\n",
    "            # Сохраняем прогноз фолда\n",
    "            if forecast_val_fold is not None and not np.all(np.isnan(forecast_val_fold)): # Проверяем, что прогноз не полностью NaN\n",
    "                 # Обрезаем или дополняем прогноз до нужной длины n_steps_val, если необходимо\n",
    "                actual_len_forecast = len(forecast_val_fold)\n",
    "                if actual_len_forecast >= n_steps_val:\n",
    "                    oof_predictions_component.loc[val_indices] = forecast_val_fold[:n_steps_val]\n",
    "                else: # Прогноз короче, чем нужно\n",
    "                    # print(f\"      Предупреждение: Прогноз на фолде {split_count} для {best_model_type} ({component_name}) короче ({actual_len_forecast}), чем ожидалось ({n_steps_val}). Дополняем NaN.\")\n",
    "                    padded_val = np.full(n_steps_val, np.nan)\n",
    "                    padded_val[:actual_len_forecast] = forecast_val_fold\n",
    "                    oof_predictions_component.loc[val_indices] = padded_val\n",
    "            # else:\n",
    "                # print(f\"      Предупреждение: Прогноз на фолде {split_count} для {best_model_type} ({component_name}) пуст или полностью NaN.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ОШИБКА на фолде {split_count} для {component_name} ({best_model_type}): {e}\")\n",
    "            traceback.print_exc()\n",
    "            # oof_predictions_component.loc[val_indices] остается NaN для этого фолда\n",
    "\n",
    "    # Сохраняем OOF-прогнозы для всей компоненты\n",
    "    # Нужно убедиться, что oof_predictions_component имеет тот же индекс, что и колонка в df_oof_best_comp\n",
    "    df_oof_best_comp.loc[oof_predictions_component.index, component_name] = oof_predictions_component\n",
    "    print(f\"  OOF-прогнозы для {component_name} ({best_model_type}) сгенерированы. Не NaN: {oof_predictions_component.notna().sum()}/{len(oof_predictions_component)}\")\n",
    "\n",
    "print(\"\\n--- Генерация OOF-прогнозов лучших моделей завершена ---\")\n",
    "\n",
    "# --- Постобработка и проверка OOF ---\n",
    "print(\"\\nКоличество НЕ-NaN OOF-прогнозов (лучшие модели) по компонентам:\")\n",
    "print(df_oof_best_comp.notna().sum(axis=0)) # Сумма по колонкам\n",
    "print(f\"Всего НЕ-NaN OOF-прогнозов: {df_oof_best_comp.notna().sum().sum()}\")\n",
    "\n",
    "# Удаляем строки, где ВСЕ компоненты имеют NaN OOF прогнозы\n",
    "# Это важно, так как если одна компонента дала NaN, а другие нет, строку не нужно удалять\n",
    "df_oof_best_comp_clean = df_oof_best_comp.dropna(how='all')\n",
    "\n",
    "\n",
    "if df_oof_best_comp_clean.empty:\n",
    "     print(\"\\nОШИБКА: После удаления строк, где все прогнозы NaN, не осталось OOF-прогнозов лучших моделей.\")\n",
    "     # Дальнейший код обучения мета-модели выполнять не стоит\n",
    "     train_values_aligned_meta = pd.Series(dtype=float) # Пустая серия\n",
    "else:\n",
    "    # Выравниваем таргет для МЕТА-модели (используем train_values из Раздела 1)\n",
    "    # Убедимся, что train_values также имеет DatetimeIndex\n",
    "    if not isinstance(train_values.index, pd.DatetimeIndex):\n",
    "        train_values.index = pd.to_datetime(train_values.index)\n",
    "\n",
    "    common_index_meta = df_oof_best_comp_clean.index.intersection(train_values.index)\n",
    "    df_oof_best_comp_clean = df_oof_best_comp_clean.loc[common_index_meta]\n",
    "    train_values_aligned_meta = train_values.loc[common_index_meta]\n",
    "\n",
    "    print(f\"\\nРазмер OOF DataFrame лучших моделей после очистки и выравнивания: {df_oof_best_comp_clean.shape}\")\n",
    "    print(f\"Размер выровненного таргета для мета-модели: {train_values_aligned_meta.shape}\")\n",
    "\n",
    "    if not df_oof_best_comp_clean.empty:\n",
    "        print(\"\\nПервые 5 строк OOF-прогнозов (лучшие модели, очищенные):\")\n",
    "        print(df_oof_best_comp_clean.head())\n",
    "    else:\n",
    "        print(\"\\nOOF DataFrame лучших моделей пуст после очистки и выравнивания.\")\n",
    "\n",
    "\n",
    "# Теперь df_oof_best_comp_clean и train_values_aligned_meta готовы для Ячейки 18 (Признаки) и 19 (Meta-RF).\n",
    "print(\"\\n--- Ячейка 17 завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1745848091086,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "1TXQAQpkUkAt",
    "outputId": "468a8e4b-b672-4e41-b8c2-6c9975827515"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 18: Создание Признаков для Мета-модели на Лучших OOF\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# df_oof_best_comp_clean: DataFrame с OOF лучших моделей (чистый)\n",
    "# train_values_aligned_meta: Series с таргетом (агрегат), выровненным с ЭТИМИ OOF\n",
    "# df_best_component_forecasts: DataFrame с прогнозами лучших моделей на тесте\n",
    "# components_to_model: list\n",
    "# create_meta_features: function (из Ячейки 7)\n",
    "# META_FEATURE_ORDER: list - порядок признаков для мета-моделей (из Ячейки 7, 35 признаков)\n",
    "# LAGS_TO_CREATE, ROLLING_WINDOWS: list\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# --- 1. Создание признаков для ОБУЧАЮЩЕЙ выборки ---\n",
    "print(\"Создание признаков для ОБУЧАЮЩЕЙ выборки (на OOF лучших моделей)...\")\n",
    "try:\n",
    "    # Применяем функцию к OOF прогнозам лучших моделей\n",
    "    X_train_meta_best_eng = create_meta_features(\n",
    "        df_input=df_oof_best_comp_clean, # <--- Используем OOF лучших\n",
    "        component_names=COMPONENTS_TO_MODEL,\n",
    "        lags=LAGS_TO_CREATE,\n",
    "        windows=ROLLING_WINDOWS,\n",
    "        feature_order_meta=META_FEATURE_ORDER # Ожидаем 35 признаков\n",
    "    )\n",
    "    # Обработка NaN после создания признаков\n",
    "    X_train_meta_best_eng_clean = X_train_meta_best_eng.dropna()\n",
    "\n",
    "    # Выравниваем таргет под очищенные признаки\n",
    "    # Важно использовать train_values_aligned_meta, который был выровнен с df_oof_best_comp_clean\n",
    "    y_train_meta_best_aligned = train_values_aligned_meta.loc[X_train_meta_best_eng_clean.index]\n",
    "\n",
    "    print(f\"Размер обучающей выборки с признаками (OOF лучших): {X_train_meta_best_eng_clean.shape}\")\n",
    "    print(f\"Размер выровненного таргета: {y_train_meta_best_aligned.shape}\")\n",
    "    # Добавим проверку количества признаков\n",
    "    if X_train_meta_best_eng_clean.shape[1] != len(META_FEATURE_ORDER):\n",
    "         print(f\"ПРЕДУПРЕЖДЕНИЕ: Количество признаков ({X_train_meta_best_eng_clean.shape[1]}) не совпадает с ожидаемым ({len(META_FEATURE_ORDER)})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при создании обучающих признаков для мета-модели: {e}\")\n",
    "    X_train_meta_best_eng_clean = None\n",
    "    y_train_meta_best_aligned = None\n",
    "\n",
    "# --- 2. Создание признаков для ТЕСТОВОЙ выборки ---\n",
    "if X_train_meta_best_eng_clean is not None:\n",
    "    print(\"\\nСоздание признаков для ТЕСТОВОЙ выборки (на прогнозах лучших моделей)...\")\n",
    "    try:\n",
    "        max_lookback_best = max(max(LAGS_TO_CREATE, default=0), max(ROLLING_WINDOWS, default=0))\n",
    "        # Используем историю из OOF лучших моделей\n",
    "        if len(df_oof_best_comp_clean) >= max_lookback_best:\n",
    "            history_for_test_best = df_oof_best_comp_clean.iloc[-max_lookback_best:]\n",
    "        else:\n",
    "            print(f\"Предупреждение: Недостаточно OOF истории ({len(df_oof_best_comp_clean)}) для lookback ({max_lookback_best}).\")\n",
    "            history_for_test_best = df_oof_best_comp_clean.copy()\n",
    "\n",
    "        # Объединяем историю OOF лучших и прогнозы лучших на тесте\n",
    "        df_for_test_features_best = pd.concat([history_for_test_best, df_best_component_forecasts])\n",
    "        # Генерируем признаки\n",
    "        X_test_meta_best_eng_full = create_meta_features(\n",
    "            df_input=df_for_test_features_best,\n",
    "            component_names=COMPONENTS_TO_MODEL,\n",
    "            lags=LAGS_TO_CREATE,\n",
    "            windows=ROLLING_WINDOWS,\n",
    "            feature_order_meta=META_FEATURE_ORDER # Ожидаем 35 признаков\n",
    "        )\n",
    "        # Оставляем только тестовый период\n",
    "        X_test_meta_best_eng = X_test_meta_best_eng_full.loc[df_best_component_forecasts.index]\n",
    "\n",
    "        # Проверка и обработка NaN в тесте\n",
    "        if X_test_meta_best_eng.isnull().any().any():\n",
    "            print(\"ВНИМАНИЕ: Обнаружены NaN в тестовых признаках (BestComp OOF). Заполняем медианой трейна...\")\n",
    "            for col in X_test_meta_best_eng.columns:\n",
    "                if X_test_meta_best_eng[col].isnull().any():\n",
    "                    median_val = X_train_meta_best_eng_clean[col].median()\n",
    "                    X_test_meta_best_eng[col] = X_test_meta_best_eng[col].fillna(median_val)\n",
    "            print(f\"Кол-во NaN после заполнения: {X_test_meta_best_eng.isnull().sum().sum()}\")\n",
    "        else:\n",
    "            print(\"NaN в тестовых признаках (BestComp OOF) не обнаружены.\")\n",
    "\n",
    "        print(\"\\n--- Признаки для мета-модели на лучших OOF готовы ---\")\n",
    "        print(\"Созданы: X_train_meta_best_eng_clean, y_train_meta_best_aligned, X_test_meta_best_eng\")\n",
    "        print(f\"Размер X_train_meta_best_eng_clean: {X_train_meta_best_eng_clean.shape}\")\n",
    "        print(f\"Размер y_train_meta_best_aligned: {y_train_meta_best_aligned.shape}\")\n",
    "        print(f\"Размер X_test_meta_best_eng: {X_test_meta_best_eng.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при создании тестовых признаков для мета-модели: {e}\")\n",
    "        X_test_meta_best_eng = None\n",
    "else:\n",
    "     print(\"Пропуск создания тестовых признаков из-за ошибки на этапе создания обучающих.\")\n",
    "     X_test_meta_best_eng = None\n",
    "\n",
    "print(\"\\n--- Ячейка 18 завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119267,
     "status": "ok",
     "timestamp": 1745848210531,
     "user": {
      "displayName": "Юрий Зиняков",
      "userId": "03392600535050190679"
     },
     "user_tz": -180
    },
    "id": "A-iA3UKLVBLz",
    "outputId": "b58d6784-3d09-47b3-fac2-9a73f160dbb1"
   },
   "outputs": [],
   "source": [
    "print(\"\\nЯчейка 19: Обучение и Оценка ВСЕХ Мета-моделей на Лучших OOF\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# X_train_meta_best_eng_clean: DataFrame с 35 признаками из OOF лучших моделей (трейн)\n",
    "# y_train_meta_best_aligned: Series с таргетом (агрегат)\n",
    "# X_test_meta_best_eng: DataFrame с 35 признаками из прогнозов лучших моделей (тест)\n",
    "# test_values: Series с фактом на тесте\n",
    "# N_CV_SPLITS: int\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# aggregation_rmse, all_horizon_rmse, final_forecasts\n",
    "# RANDOM_SEED\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "# Переменные для хранения прогнозов этой ячейки\n",
    "meta_forecasts_on_best = {}\n",
    "\n",
    "# --- Масштабирование Признаков (для Ridge/Lasso) ---\n",
    "print(\"Масштабирование признаков для Ridge/Lasso...\")\n",
    "scaler_meta_best = StandardScaler() # Новый scaler для этих данных\n",
    "X_train_meta_best_scaled = scaler_meta_best.fit_transform(X_train_meta_best_eng_clean)\n",
    "X_test_meta_best_scaled = scaler_meta_best.transform(X_test_meta_best_eng)\n",
    "print(\"Масштабирование выполнено.\")\n",
    "\n",
    "\n",
    "# Инициализируем TS Splitter для CV\n",
    "tscv_meta_best = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "# --- Meta-Ridge на Лучших OOF ---\n",
    "model_name = \"Meta-Ridge_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name} ---\")\n",
    "try:\n",
    "    alphas = np.logspace(-6, 6, 13)\n",
    "    ridge_model = RidgeCV(alphas=alphas, store_cv_values=False, cv=None)\n",
    "    ridge_model.fit(X_train_meta_best_scaled, y_train_meta_best_aligned) # На масштабированных\n",
    "    print(f\"  Лучший alpha: {ridge_model.alpha_:.6f}\")\n",
    "    y_pred = ridge_model.predict(X_test_meta_best_scaled) # На масштабированных\n",
    "    meta_forecasts_on_best[model_name] = y_pred\n",
    "    final_forecasts[model_name] = y_pred\n",
    "    # --- Оценка (Общий RMSE) ---\n",
    "    eval_len = min(len(y_pred), len(test_values))\n",
    "    overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], y_pred[:eval_len]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name] = overall_rmse\n",
    "    # --- Оценка (По горизонтам) ---\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(test_values[:h], y_pred[:h]))\n",
    "            horizon_results_list.append({'model': model_name, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "except Exception as e: print(f\"  Ошибка {model_name}: {e}\"); aggregation_rmse[model_name] = np.nan\n",
    "\n",
    "\n",
    "# --- Meta-Lasso на Лучших OOF ---\n",
    "model_name = \"Meta-Lasso_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name} ---\")\n",
    "try:\n",
    "    lasso_model = LassoCV(cv=tscv_meta_best, random_state=RANDOM_SEED, max_iter=10000, n_jobs=-1, verbose=0)\n",
    "    lasso_model.fit(X_train_meta_best_scaled, y_train_meta_best_aligned) # На масштабированных\n",
    "    print(f\"  Лучший alpha: {lasso_model.alpha_:.6f}\")\n",
    "    n_feat = np.sum(lasso_model.coef_ != 0); print(f\"  Признаков отобрано: {n_feat}\")\n",
    "    y_pred = lasso_model.predict(X_test_meta_best_scaled) # На масштабированных\n",
    "    meta_forecasts_on_best[model_name] = y_pred\n",
    "    final_forecasts[model_name] = y_pred\n",
    "    # --- Оценка ---\n",
    "    eval_len = min(len(y_pred), len(test_values))\n",
    "    overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], y_pred[:eval_len]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name] = overall_rmse\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(test_values[:h], y_pred[:h]))\n",
    "            horizon_results_list.append({'model': model_name, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "except Exception as e: print(f\"  Ошибка {model_name}: {e}\"); aggregation_rmse[model_name] = np.nan\n",
    "\n",
    "\n",
    "# --- Meta-RandomForest на Лучших OOF ---\n",
    "model_name = \"Meta-RF_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name} ---\")\n",
    "try:\n",
    "    param_grid_rf = { # Используем ту же сетку, что и для RF_on_ARIMA\n",
    "        'n_estimators': [100, 200], 'max_depth': [5, 10, None],\n",
    "        'max_features': ['sqrt', 0.7], 'min_samples_leaf': [1, 3]\n",
    "    }\n",
    "    rf_model = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    gs_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, scoring='neg_root_mean_squared_error',\n",
    "                         cv=tscv_meta_best, n_jobs=-1, verbose=1, refit=True)\n",
    "    print(\"Подбор параметров для Meta-RF (на лучших OOF)...\")\n",
    "    gs_rf.fit(X_train_meta_best_eng_clean, y_train_meta_best_aligned) # На НЕмасштабированных\n",
    "    print(f\"  Лучшие параметры: {gs_rf.best_params_}\")\n",
    "    print(f\"  Лучший CV RMSE: {-gs_rf.best_score_:.4f}\")\n",
    "    best_rf = gs_rf.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test_meta_best_eng) # На НЕмасштабированных\n",
    "    meta_forecasts_on_best[model_name] = y_pred\n",
    "    final_forecasts[model_name] = y_pred\n",
    "    # --- Оценка ---\n",
    "    eval_len = min(len(y_pred), len(test_values))\n",
    "    overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], y_pred[:eval_len]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name] = overall_rmse\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(test_values[:h], y_pred[:h]))\n",
    "            horizon_results_list.append({'model': model_name, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "except Exception as e: print(f\"  Ошибка {model_name}: {e}\"); aggregation_rmse[model_name] = np.nan\n",
    "\n",
    "\n",
    "# --- Meta-XGBoost на Лучших OOF ---\n",
    "model_name = \"Meta-XGB_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name} ---\")\n",
    "try:\n",
    "    param_grid_xgb = { # Используем ту же сетку, что и для XGB_on_ARIMA\n",
    "        'n_estimators': [50, 100], 'max_depth': [2, 3],\n",
    "        'learning_rate': [0.1, 0.15], 'subsample': [0.8], 'colsample_bytree': [0.8]\n",
    "    }\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    gs_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, scoring='neg_root_mean_squared_error',\n",
    "                          cv=tscv_meta_best, n_jobs=-1, verbose=1, refit=True)\n",
    "    print(\"Подбор параметров для Meta-XGB (на лучших OOF)...\")\n",
    "    gs_xgb.fit(X_train_meta_best_eng_clean, y_train_meta_best_aligned) # На НЕмасштабированных\n",
    "    print(f\"  Лучшие параметры: {gs_xgb.best_params_}\")\n",
    "    print(f\"  Лучший CV RMSE: {-gs_xgb.best_score_:.4f}\")\n",
    "    best_xgb = gs_xgb.best_estimator_\n",
    "    y_pred = best_xgb.predict(X_test_meta_best_eng) # На НЕмасштабированных\n",
    "    meta_forecasts_on_best[model_name] = y_pred\n",
    "    final_forecasts[model_name] = y_pred\n",
    "    # --- Оценка ---\n",
    "    eval_len = min(len(y_pred), len(test_values))\n",
    "    overall_rmse = np.sqrt(mean_squared_error(test_values[:eval_len], y_pred[:eval_len]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse:.4f}\")\n",
    "    aggregation_rmse[model_name] = overall_rmse\n",
    "    horizon_results_list = []\n",
    "    for h in HORIZONS_TO_EVALUATE:\n",
    "        if h <= eval_len:\n",
    "            rmse_h = np.sqrt(mean_squared_error(test_values[:h], y_pred[:h]))\n",
    "            horizon_results_list.append({'model': model_name, 'horizon': h, 'rmse': rmse_h})\n",
    "    if horizon_results_list:\n",
    "        df_horizon = pd.DataFrame(horizon_results_list)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon], ignore_index=True)\n",
    "except Exception as e: print(f\"  Ошибка {model_name}: {e}\"); aggregation_rmse[model_name] = np.nan\n",
    "\n",
    "\n",
    "# --- Вывод Финальных Таблиц ---\n",
    "print(\"\\n--- Финальное сравнение RMSE (Общий RMSE) ---\")\n",
    "if 'aggregation_rmse' in locals() and aggregation_rmse:\n",
    "    valid_keys = [k for k in aggregation_rmse if pd.notna(aggregation_rmse[k])]\n",
    "    if valid_keys:\n",
    "        sorted_rmse_final = {k: aggregation_rmse[k] for k in sorted(valid_keys, key=aggregation_rmse.get)}\n",
    "        # Убедимся, что все модели есть в словаре перед выводом\n",
    "        print(\"Модели в словаре:\", list(aggregation_rmse.keys()))\n",
    "        for model, rmse in sorted_rmse_final.items(): print(f\"  {model}: {rmse:.4f}\")\n",
    "    else: print(\"Нет валидных RMSE для сортировки.\")\n",
    "else: print(\"Словарь aggregation_rmse пуст.\")\n",
    "\n",
    "print(\"\\nФинальная таблица RMSE по горизонтам:\")\n",
    "if 'all_horizon_rmse' in locals() and not all_horizon_rmse.empty:\n",
    "    try:\n",
    "        pivot_rmse_final_final = all_horizon_rmse.pivot(index='horizon', columns='model', values='rmse')\n",
    "        if 'sorted_rmse_final' in locals():\n",
    "             ordered_columns_keys_final = list(sorted_rmse_final.keys())\n",
    "             ordered_columns_final_final = [col for col in ordered_columns_keys_final if col in pivot_rmse_final_final.columns]\n",
    "             remaining_cols_final_final = [col for col in pivot_rmse_final_final.columns if col not in ordered_columns_final_final]\n",
    "             pivot_rmse_final_final = pivot_rmse_final_final[ordered_columns_final_final + remaining_cols_final_final]\n",
    "        print(pivot_rmse_final_final.round(4))\n",
    "    except Exception as e: print(f\"Не удалось создать сводную таблицу: {e}\")\n",
    "else: print(\"Нет данных.\")\n",
    "\n",
    "print(\"\\n--- Ячейка 19 (Все мета-модели на лучших OOF) завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем даты из Baseline ARIMA в качестве общего индекса\n",
    "dates = final_forecasts['Baseline ARIMA'].index\n",
    "df = pd.DataFrame(final_forecasts, index=dates)\n",
    "\n",
    "# Сохраняем в Excel\n",
    "df.to_excel('Прогноз-43-компоненты-февраль23.xlsx', index=True, index_label='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV # GridSearchCV остается для сравнения, если нужно, но мы его заменяем\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import traceback\n",
    "\n",
    "# Импорты для Hyperopt (если еще не были сделаны в этом ноутбуке/скрипте глобально)\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope # Для scope.int\n",
    "\n",
    "print(\"\\nЯчейка 19: Обучение и Оценка ВСЕХ Мета-моделей на Лучших OOF (с Hyperopt для RF и XGB)\")\n",
    "\n",
    "# --- ПРЕДПОЛАГАЕМ НАЛИЧИЕ ПЕРЕМЕННЫХ ---\n",
    "# X_train_meta_best_eng_clean: DataFrame (трейн признаки мета-модели)\n",
    "# y_train_meta_best_aligned: Series (трейн таргет мета-модели)\n",
    "# X_test_meta_best_eng: DataFrame (тест признаки мета-модели)\n",
    "# test_values: Series (факт на тесте для оценки)\n",
    "# N_CV_SPLITS: int (для TimeSeriesSplit)\n",
    "# HORIZONS_TO_EVALUATE: list\n",
    "# RANDOM_SEED: int\n",
    "# MAX_EVALS_HYPEROPT_META: int (новое, количество итераций Hyperopt для мета-моделей, например, 30)\n",
    "\n",
    "# Структуры для хранения результатов (предполагаем, что они инициализированы ранее)\n",
    "# aggregation_rmse = {}\n",
    "# all_horizon_rmse = pd.DataFrame(columns=['model', 'horizon', 'rmse'])\n",
    "# final_forecasts = {}\n",
    "# --- КОНЕЦ ПРЕДПОЛОЖЕНИЙ ---\n",
    "\n",
    "MAX_EVALS_HYPEROPT_META=50\n",
    "\n",
    "# Инициализация структур, если они не существуют\n",
    "if 'aggregation_rmse' not in locals(): aggregation_rmse = {}\n",
    "if 'all_horizon_rmse' not in locals(): all_horizon_rmse = pd.DataFrame(columns=['model', 'horizon', 'rmse'])\n",
    "if 'final_forecasts' not in locals(): final_forecasts = {}\n",
    "if 'MAX_EVALS_HYPEROPT_META' not in locals(): MAX_EVALS_HYPEROPT_META = 30 # Значение по умолчанию\n",
    "\n",
    "# Переменные для хранения прогнозов этой ячейки\n",
    "meta_forecasts_on_best = {}\n",
    "\n",
    "# --- Масштабирование Признаков (для Ridge/Lasso) ---\n",
    "print(\"Масштабирование признаков для Ridge/Lasso...\")\n",
    "scaler_meta_best = StandardScaler()\n",
    "# Убедимся, что нет NaN перед масштабированием, иначе fit/transform может выдать ошибку или предупреждение\n",
    "if X_train_meta_best_eng_clean.isnull().any().any():\n",
    "    print(\"ПРЕДУПРЕЖДЕНИЕ: Обнаружены NaN в X_train_meta_best_eng_clean перед масштабированием. Это может вызвать проблемы.\")\n",
    "if X_test_meta_best_eng.isnull().any().any():\n",
    "    print(\"ПРЕДУПРЕЖДЕНИЕ: Обнаружены NaN в X_test_meta_best_eng перед масштабированием. Это может вызвать проблемы.\")\n",
    "\n",
    "X_train_meta_best_scaled = scaler_meta_best.fit_transform(X_train_meta_best_eng_clean)\n",
    "X_test_meta_best_scaled = scaler_meta_best.transform(X_test_meta_best_eng)\n",
    "print(\"Масштабирование выполнено.\")\n",
    "\n",
    "\n",
    "# Инициализируем TS Splitter для CV мета-моделей\n",
    "tscv_meta_best = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "# --- Meta-Ridge на Лучших OOF (без изменений, использует RidgeCV) ---\n",
    "model_name_ridge = \"Meta-Ridge_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name_ridge} ---\")\n",
    "try:\n",
    "    alphas_ridge = np.logspace(-6, 6, 20) # Можно увеличить количество альф\n",
    "    # RidgeCV с TimeSeriesSplit требует, чтобы cv был итератором\n",
    "    # RidgeCV сам делает кросс-валидацию, если cv=None (LOO) или указан int (KFold)\n",
    "    # Для TimeSeriesSplit лучше использовать GridSearchCV или реализовать цикл вручную,\n",
    "    # но RidgeCV довольно устойчив. Оставим как есть для простоты, но для строгости\n",
    "    # можно было бы обернуть в цикл с tscv_meta_best.\n",
    "    ridge_model_meta = RidgeCV(alphas=alphas_ridge, store_cv_values=False, cv=None) # cv=None для Generalized Cross-Validation\n",
    "    ridge_model_meta.fit(X_train_meta_best_scaled, y_train_meta_best_aligned)\n",
    "    print(f\"  Лучший alpha: {ridge_model_meta.alpha_:.6f}\")\n",
    "    y_pred_ridge = ridge_model_meta.predict(X_test_meta_best_scaled)\n",
    "    meta_forecasts_on_best[model_name_ridge] = y_pred_ridge\n",
    "    final_forecasts[model_name_ridge] = y_pred_ridge\n",
    "    eval_len_ridge = min(len(y_pred_ridge), len(test_values))\n",
    "    overall_rmse_ridge = np.sqrt(mean_squared_error(test_values[:eval_len_ridge], y_pred_ridge[:eval_len_ridge]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse_ridge:.4f}\")\n",
    "    aggregation_rmse[model_name_ridge] = overall_rmse_ridge\n",
    "    horizon_results_list_ridge = []\n",
    "    for h_r in HORIZONS_TO_EVALUATE:\n",
    "        if h_r <= eval_len_ridge:\n",
    "            rmse_h_r = np.sqrt(mean_squared_error(test_values[:h_r], y_pred_ridge[:h_r]))\n",
    "            horizon_results_list_ridge.append({'model': model_name_ridge, 'horizon': h_r, 'rmse': rmse_h_r})\n",
    "    if horizon_results_list_ridge:\n",
    "        df_horizon_ridge = pd.DataFrame(horizon_results_list_ridge)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_ridge] # Удаляем старые записи\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon_ridge], ignore_index=True)\n",
    "except Exception as e_r:\n",
    "    print(f\"  Ошибка {model_name_ridge}: {e_r}\")\n",
    "    aggregation_rmse[model_name_ridge] = np.nan\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Meta-Lasso на Лучших OOF (без изменений, использует LassoCV) ---\n",
    "model_name_lasso = \"Meta-Lasso_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name_lasso} ---\")\n",
    "try:\n",
    "    # LassoCV хорошо работает с объектом TimeSeriesSplit\n",
    "    lasso_model_meta = LassoCV(cv=tscv_meta_best, random_state=RANDOM_SEED, max_iter=20000, n_jobs=-1, verbose=0, n_alphas=100)\n",
    "    lasso_model_meta.fit(X_train_meta_best_scaled, y_train_meta_best_aligned)\n",
    "    print(f\"  Лучший alpha: {lasso_model_meta.alpha_:.6f}\")\n",
    "    n_feat_lasso = np.sum(lasso_model_meta.coef_ != 0)\n",
    "    print(f\"  Признаков отобрано: {n_feat_lasso}\")\n",
    "    y_pred_lasso = lasso_model_meta.predict(X_test_meta_best_scaled)\n",
    "    meta_forecasts_on_best[model_name_lasso] = y_pred_lasso\n",
    "    final_forecasts[model_name_lasso] = y_pred_lasso\n",
    "    eval_len_lasso = min(len(y_pred_lasso), len(test_values))\n",
    "    overall_rmse_lasso = np.sqrt(mean_squared_error(test_values[:eval_len_lasso], y_pred_lasso[:eval_len_lasso]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse_lasso:.4f}\")\n",
    "    aggregation_rmse[model_name_lasso] = overall_rmse_lasso\n",
    "    horizon_results_list_lasso = []\n",
    "    for h_l in HORIZONS_TO_EVALUATE:\n",
    "        if h_l <= eval_len_lasso:\n",
    "            rmse_h_l = np.sqrt(mean_squared_error(test_values[:h_l], y_pred_lasso[:h_l]))\n",
    "            horizon_results_list_lasso.append({'model': model_name_lasso, 'horizon': h_l, 'rmse': rmse_h_l})\n",
    "    if horizon_results_list_lasso:\n",
    "        df_horizon_lasso = pd.DataFrame(horizon_results_list_lasso)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_lasso] # Удаляем старые записи\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon_lasso], ignore_index=True)\n",
    "except Exception as e_l:\n",
    "    print(f\"  Ошибка {model_name_lasso}: {e_l}\")\n",
    "    aggregation_rmse[model_name_lasso] = np.nan\n",
    "    traceback.print_exc()\n",
    "\n",
    "# --- Определяем пространства поиска Hyperopt для мета-моделей ---\n",
    "# Можно взять за основу пространства из Ячейки 14 или адаптировать их\n",
    "rf_hyperopt_space_meta = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators_rf_meta', 50, 250, 25)),\n",
    "    'max_depth': hp.choice('max_depth_rf_meta', [None, scope.int(hp.quniform('max_depth_val_rf_meta', 3, 12, 1))]), # Немного уменьшил глубину для мета\n",
    "    'max_features': hp.choice('max_features_rf_meta', ['sqrt', 'log2', hp.uniform('max_features_float_rf_meta', 0.3, 0.8)]),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf_rf_meta', 1, 5, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split_rf_meta', 2, 10, 1))\n",
    "}\n",
    "\n",
    "xgb_hyperopt_space_meta = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators_xgb_meta', 50, 300, 25)), # Чуть больше диапазон\n",
    "    'max_depth': scope.int(hp.quniform('max_depth_xgb_meta', 2, 6, 1)), # Обычно мета-модели не требуют большой глубины\n",
    "    'learning_rate': hp.loguniform('learning_rate_xgb_meta', np.log(0.01), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample_xgb_meta', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree_xgb_meta', 0.6, 1.0),\n",
    "    'gamma': hp.uniform('gamma_xgb_meta', 0, 0.5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha_xgb_meta', 0, 0.3),\n",
    "    'reg_lambda': hp.uniform('reg_lambda_xgb_meta', 0, 0.3),\n",
    "}\n",
    "\n",
    "# --- Meta-RandomForest на Лучших OOF с Hyperopt ---\n",
    "model_name_rf = \"Meta-RF_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name_rf} (с Hyperopt) ---\")\n",
    "try:\n",
    "    def objective_rf_meta(params):\n",
    "        current_params = params.copy()\n",
    "        # Преобразование целочисленных параметров\n",
    "        for p_name in ['n_estimators', 'max_depth', 'min_samples_leaf', 'min_samples_split']:\n",
    "            if p_name in current_params and current_params[p_name] is not None:\n",
    "                current_params[p_name] = int(current_params[p_name])\n",
    "\n",
    "        model = RandomForestRegressor(**current_params, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv_meta_best.split(X_train_meta_best_eng_clean):\n",
    "            X_cv_train, X_cv_val = X_train_meta_best_eng_clean.iloc[train_idx], X_train_meta_best_eng_clean.iloc[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_meta_best_aligned.iloc[train_idx], y_train_meta_best_aligned.iloc[val_idx]\n",
    "            if X_cv_train.empty or X_cv_val.empty: continue\n",
    "            try:\n",
    "                model.fit(X_cv_train, y_cv_train)\n",
    "                preds = model.predict(X_cv_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_cv_val, preds))\n",
    "                cv_scores.append(rmse)\n",
    "            except Exception: cv_scores.append(np.inf)\n",
    "        if not cv_scores or all(s == np.inf for s in cv_scores): return {'loss': np.inf, 'status': STATUS_OK}\n",
    "        avg_rmse = np.mean([s for s in cv_scores if s != np.inf])\n",
    "        return {'loss': avg_rmse if not np.isnan(avg_rmse) else np.inf, 'status': STATUS_OK}\n",
    "\n",
    "    trials_rf_meta = Trials()\n",
    "    print(f\"  Подбор параметров для {model_name_rf} с Hyperopt (max_evals={MAX_EVALS_HYPEROPT_META})...\")\n",
    "    best_params_rf_fmin = fmin(fn=objective_rf_meta,\n",
    "                             space=rf_hyperopt_space_meta,\n",
    "                             algo=tpe.suggest,\n",
    "                             max_evals=MAX_EVALS_HYPEROPT_META,\n",
    "                             trials=trials_rf_meta,\n",
    "                             rstate=np.random.default_rng(RANDOM_SEED),\n",
    "                             verbose=0) # verbose=1 для отладки\n",
    "\n",
    "    actual_best_params_rf_meta = hyperopt.space_eval(rf_hyperopt_space_meta, best_params_rf_fmin)\n",
    "    # Повторное преобразование целочисленных параметров после space_eval\n",
    "    for p_name in ['n_estimators', 'max_depth', 'min_samples_leaf', 'min_samples_split']:\n",
    "            if p_name in actual_best_params_rf_meta and actual_best_params_rf_meta[p_name] is not None:\n",
    "                actual_best_params_rf_meta[p_name] = int(actual_best_params_rf_meta[p_name])\n",
    "\n",
    "    best_loss_rf_meta = trials_rf_meta.best_trial['result']['loss'] if trials_rf_meta.best_trial else np.inf\n",
    "    print(f\"  Лучшие параметры: {actual_best_params_rf_meta}\")\n",
    "    print(f\"  Лучший CV RMSE (Hyperopt): {best_loss_rf_meta:.4f}\")\n",
    "\n",
    "    final_rf_meta_model = RandomForestRegressor(**actual_best_params_rf_meta, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    final_rf_meta_model.fit(X_train_meta_best_eng_clean, y_train_meta_best_aligned) # На НЕмасштабированных\n",
    "\n",
    "    y_pred_rf = final_rf_meta_model.predict(X_test_meta_best_eng) # На НЕмасштабированных\n",
    "    meta_forecasts_on_best[model_name_rf] = y_pred_rf\n",
    "    final_forecasts[model_name_rf] = y_pred_rf\n",
    "\n",
    "    eval_len_rf = min(len(y_pred_rf), len(test_values))\n",
    "    overall_rmse_rf = np.sqrt(mean_squared_error(test_values[:eval_len_rf], y_pred_rf[:eval_len_rf]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse_rf:.4f}\")\n",
    "    aggregation_rmse[model_name_rf] = overall_rmse_rf\n",
    "    horizon_results_list_rf = []\n",
    "    for h_rf in HORIZONS_TO_EVALUATE:\n",
    "        if h_rf <= eval_len_rf:\n",
    "            rmse_h_rf = np.sqrt(mean_squared_error(test_values[:h_rf], y_pred_rf[:h_rf]))\n",
    "            horizon_results_list_rf.append({'model': model_name_rf, 'horizon': h_rf, 'rmse': rmse_h_rf})\n",
    "    if horizon_results_list_rf:\n",
    "        df_horizon_rf = pd.DataFrame(horizon_results_list_rf)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_rf]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon_rf], ignore_index=True)\n",
    "except Exception as e_rf:\n",
    "    print(f\"  Ошибка {model_name_rf}: {e_rf}\")\n",
    "    aggregation_rmse[model_name_rf] = np.nan\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Meta-XGBoost на Лучших OOF с Hyperopt ---\n",
    "model_name_xgb = \"Meta-XGB_on_BestComp\"\n",
    "print(f\"\\n--- Обучение и оценка: {model_name_xgb} (с Hyperopt) ---\")\n",
    "try:\n",
    "    def objective_xgb_meta(params):\n",
    "        current_params = params.copy()\n",
    "        # Преобразование целочисленных параметров\n",
    "        for p_name in ['n_estimators', 'max_depth']: # Добавьте другие, если есть\n",
    "            if p_name in current_params and current_params[p_name] is not None:\n",
    "                current_params[p_name] = int(current_params[p_name])\n",
    "\n",
    "        model = xgb.XGBRegressor(**current_params, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv_meta_best.split(X_train_meta_best_eng_clean):\n",
    "            X_cv_train, X_cv_val = X_train_meta_best_eng_clean.iloc[train_idx], X_train_meta_best_eng_clean.iloc[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_meta_best_aligned.iloc[train_idx], y_train_meta_best_aligned.iloc[val_idx]\n",
    "            if X_cv_train.empty or X_cv_val.empty: continue\n",
    "            try:\n",
    "                model.fit(X_cv_train, y_cv_train)\n",
    "                preds = model.predict(X_cv_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_cv_val, preds))\n",
    "                cv_scores.append(rmse)\n",
    "            except Exception: cv_scores.append(np.inf)\n",
    "        if not cv_scores or all(s == np.inf for s in cv_scores): return {'loss': np.inf, 'status': STATUS_OK}\n",
    "        avg_rmse = np.mean([s for s in cv_scores if s != np.inf])\n",
    "        return {'loss': avg_rmse if not np.isnan(avg_rmse) else np.inf, 'status': STATUS_OK}\n",
    "\n",
    "    trials_xgb_meta = Trials()\n",
    "    print(f\"  Подбор параметров для {model_name_xgb} с Hyperopt (max_evals={MAX_EVALS_HYPEROPT_META})...\")\n",
    "    best_params_xgb_fmin = fmin(fn=objective_xgb_meta,\n",
    "                              space=xgb_hyperopt_space_meta,\n",
    "                              algo=tpe.suggest,\n",
    "                              max_evals=MAX_EVALS_HYPEROPT_META,\n",
    "                              trials=trials_xgb_meta,\n",
    "                              rstate=np.random.default_rng(RANDOM_SEED),\n",
    "                              verbose=0)\n",
    "\n",
    "    actual_best_params_xgb_meta = hyperopt.space_eval(xgb_hyperopt_space_meta, best_params_xgb_fmin)\n",
    "    # Повторное преобразование целочисленных параметров\n",
    "    for p_name in ['n_estimators', 'max_depth']:\n",
    "            if p_name in actual_best_params_xgb_meta and actual_best_params_xgb_meta[p_name] is not None:\n",
    "                actual_best_params_xgb_meta[p_name] = int(actual_best_params_xgb_meta[p_name])\n",
    "\n",
    "    best_loss_xgb_meta = trials_xgb_meta.best_trial['result']['loss'] if trials_xgb_meta.best_trial else np.inf\n",
    "    print(f\"  Лучшие параметры: {actual_best_params_xgb_meta}\")\n",
    "    print(f\"  Лучший CV RMSE (Hyperopt): {best_loss_xgb_meta:.4f}\")\n",
    "\n",
    "    final_xgb_meta_model = xgb.XGBRegressor(**actual_best_params_xgb_meta, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    final_xgb_meta_model.fit(X_train_meta_best_eng_clean, y_train_meta_best_aligned) # На НЕмасштабированных\n",
    "\n",
    "    y_pred_xgb = final_xgb_meta_model.predict(X_test_meta_best_eng) # На НЕмасштабированных\n",
    "    meta_forecasts_on_best[model_name_xgb] = y_pred_xgb\n",
    "    final_forecasts[model_name_xgb] = y_pred_xgb\n",
    "\n",
    "    eval_len_xgb = min(len(y_pred_xgb), len(test_values))\n",
    "    overall_rmse_xgb = np.sqrt(mean_squared_error(test_values[:eval_len_xgb], y_pred_xgb[:eval_len_xgb]))\n",
    "    print(f\"  Общий RMSE: {overall_rmse_xgb:.4f}\")\n",
    "    aggregation_rmse[model_name_xgb] = overall_rmse_xgb\n",
    "    horizon_results_list_xgb = []\n",
    "    for h_xgb in HORIZONS_TO_EVALUATE:\n",
    "        if h_xgb <= eval_len_xgb:\n",
    "            rmse_h_xgb = np.sqrt(mean_squared_error(test_values[:h_xgb], y_pred_xgb[:h_xgb]))\n",
    "            horizon_results_list_xgb.append({'model': model_name_xgb, 'horizon': h_xgb, 'rmse': rmse_h_xgb})\n",
    "    if horizon_results_list_xgb:\n",
    "        df_horizon_xgb = pd.DataFrame(horizon_results_list_xgb)\n",
    "        all_horizon_rmse = all_horizon_rmse[all_horizon_rmse['model'] != model_name_xgb]\n",
    "        all_horizon_rmse = pd.concat([all_horizon_rmse, df_horizon_xgb], ignore_index=True)\n",
    "except Exception as e_xgb:\n",
    "    print(f\"  Ошибка {model_name_xgb}: {e_xgb}\")\n",
    "    aggregation_rmse[model_name_xgb] = np.nan\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Вывод Финальных Таблиц ---\n",
    "print(\"\\n--- Финальное сравнение RMSE (Общий RMSE) ---\")\n",
    "if 'aggregation_rmse' in locals() and aggregation_rmse:\n",
    "    # Фильтруем NaN значения перед сортировкой\n",
    "    valid_rmse_items = {k: v for k, v in aggregation_rmse.items() if pd.notna(v)}\n",
    "    if valid_rmse_items:\n",
    "        sorted_rmse_final = dict(sorted(valid_rmse_items.items(), key=lambda item: item[1]))\n",
    "        print(\"Модели в словаре aggregation_rmse:\", list(aggregation_rmse.keys()))\n",
    "        for model, rmse_val in sorted_rmse_final.items():\n",
    "            print(f\"  {model}: {rmse_val:.4f}\")\n",
    "    else:\n",
    "        print(\"Нет валидных RMSE для сортировки в aggregation_rmse.\")\n",
    "else:\n",
    "    print(\"Словарь aggregation_rmse не определен или пуст.\")\n",
    "\n",
    "print(\"\\nФинальная таблица RMSE по горизонтам:\")\n",
    "if 'all_horizon_rmse' in locals() and not all_horizon_rmse.empty:\n",
    "    try:\n",
    "        # Удаляем дубликаты, если модель оценивалась несколько раз (например, при перезапусках)\n",
    "        all_horizon_rmse_unique = all_horizon_rmse.drop_duplicates(subset=['model', 'horizon'], keep='last')\n",
    "        pivot_rmse_final_final = all_horizon_rmse_unique.pivot(index='horizon', columns='model', values='rmse')\n",
    "        # Сортируем колонки по общему RMSE из sorted_rmse_final, если он существует\n",
    "        if 'sorted_rmse_final' in locals() and sorted_rmse_final:\n",
    "             ordered_columns_keys_final = list(sorted_rmse_final.keys())\n",
    "             # Включаем только те колонки, которые есть в pivot_rmse_final_final\n",
    "             available_ordered_columns = [col for col in ordered_columns_keys_final if col in pivot_rmse_final_final.columns]\n",
    "             remaining_cols_final_final = [col for col in pivot_rmse_final_final.columns if col not in available_ordered_columns]\n",
    "             pivot_rmse_final_final = pivot_rmse_final_final[available_ordered_columns + remaining_cols_final_final]\n",
    "        print(pivot_rmse_final_final.round(4))\n",
    "    except Exception as e_pivot:\n",
    "        print(f\"Не удалось создать сводную таблицу RMSE по горизонтам: {e_pivot}\")\n",
    "        print(\"Данные в all_horizon_rmse:\")\n",
    "        print(all_horizon_rmse)\n",
    "else:\n",
    "    print(\"Нет данных в all_horizon_rmse для отображения таблицы по горизонтам.\")\n",
    "\n",
    "print(\"\\n--- Ячейка 19 (Все мета-модели на лучших OOF с Hyperopt) завершена ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем даты из Baseline ARIMA в качестве общего индекса\n",
    "dates = final_forecasts['Baseline ARIMA'].index\n",
    "df = pd.DataFrame(final_forecasts, index=dates)\n",
    "\n",
    "# Сохраняем в Excel\n",
    "df.to_excel('Прогноз-43-компоненты-февраль23.xlsx', index=True, index_label='Date')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+tX8QfFThxbc9KcG3npWr",
   "mount_file_id": "1FvO53W_3z1NeaknkpmnG0n4isDoWj0OZ",
   "provenance": [
    {
     "file_id": "1q-PD0RZWFsLS48PXWam2klSwIcU3J79a",
     "timestamp": 1745610169654
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
